{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from sec_edgar_api import EdgarClient\n",
    "# from sec_edgar_downloader import Downloader\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import glob\n",
    "import logging\n",
    "from typing import Literal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read HTML File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_html(file: str):\n",
    "    \"\"\"Reads html file and returns BeautifulSoup object\"\"\"\n",
    "    with open(f'{file}', 'r') as f:\n",
    "        contents = f.read()\n",
    "    soup = BeautifulSoup(contents, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find target table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_target_table(target_text, soup, search_type=Literal['loose','strict']):\n",
    "    \"\"\"\n",
    "    target_text: regex pattern\n",
    "    \n",
    "    soup: BeautifulSoup object\n",
    "    \n",
    "    search_type: 'loose' or 'strict'\n",
    "        - loose: search for target_text in the soup\n",
    "        - strict: search for target_text in the soup, then find the next table following the text\n",
    "\n",
    "    Returns a list of BeautifulSoup objects containing the target table(s) in html format.\n",
    "    \"\"\"\n",
    "    target_tables = None\n",
    "    \n",
    "    target_element = soup.find_all('div') # find_all returns a list\n",
    "    if search_type == 'loose':\n",
    "        target_tables = [i for i in target_element if target_text.search(i.text.lower())]\n",
    "    elif search_type == 'strict':\n",
    "        target_tables = [j.find_next('table') for j in [i for i in target_element if target_text.search(i.text)]]\n",
    "    \n",
    "    return target_tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Table from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_from_html(table_html):\n",
    "    \"\"\"\n",
    "    table_html: BeautifulSoup object\n",
    "    \n",
    "    Returns a dataframe of the table.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = [] # row x col [[1,2,3,4,5], [1,2,3,4,5], [1,2,3,4,5]] = 3 rows, 5 columns\n",
    "    # Loop over each row\n",
    "    for row in table_html.find_all('tr')[:]: #\n",
    "        # print(f'row: {row}')\n",
    "        cols = []\n",
    "        # Loop over each cell in the row\n",
    "        for cell in row.find_all(['td', 'th'])[:]:\n",
    "            # print(cell)\n",
    "            # Get the colspan and rowspan attributes, defaulting to 1 if they don't exist\n",
    "            colspan = int(cell.get('colspan', 1))\n",
    "            rowspan = int(cell.get('rowspan', 1))\n",
    "\n",
    "            # print(f'cell: {cell.text.strip()}\\ncolspan: {colspan}\\nrowspan: {rowspan}')\n",
    "            # If the cell spans multiple rows or columns, add copies of it to the cols list\n",
    "            for i in range(rowspan):\n",
    "                for j in range(colspan):\n",
    "                    cols.append(cell.text.strip())\n",
    "\n",
    "\n",
    "        # Add the cols list to the data list\n",
    "        data.append(cols)\n",
    "        # print(data)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.replace('', np.nan, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    \"\"\"\n",
    "    df: dataframe\n",
    "    \n",
    "    Process the dataframe to remove unnecessary rows and columns.\n",
    "    \n",
    "    Steps:\n",
    "    1. Find header row and use it to set column names\n",
    "    2. Remove non-header rows and columns where all values are NaN\n",
    "    3. Replace empty strings with NaN\n",
    "    4. Forward fill NaN values along columns\n",
    "    5. Remove rows where all values are NaN\n",
    "    6. Reset index after dropping rows\n",
    "    7. if column contains '$' then remove it\n",
    "    8. if row only contains same element then store the index and value in a dictionary\n",
    "    9. Use the dictionary to assign the value as first level of multiindex for the rows in between the rows to keep\n",
    "    10. replace any cell that contains : with empty string\n",
    "    11. Combine two columns to create a multiindex for the rows\n",
    "\n",
    "    Returns a cleaned dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    first_row = df.iloc[:, 0].notnull().idxmax()\n",
    "    # Use first three rows for the header\n",
    "    header = df.iloc[0:first_row].fillna('').agg(' '.join).str.strip()\n",
    "    df.columns = header\n",
    "\n",
    "    # Remove the rows used for header and reset index\n",
    "    df = df.iloc[first_row:].reset_index(drop=True)\n",
    "\n",
    "    # Remove columns where all values are NaN\n",
    "    df = df.dropna(how='all', axis=1)\n",
    "\n",
    "    # Replace empty strings with NaN\n",
    "    df.replace(\"\", np.nan, inplace=True)\n",
    "\n",
    "    # Forward fill NaN values along columns\n",
    "    df.fillna(method='ffill', axis=1, inplace=True)\n",
    "\n",
    "    # Remove rows where all values are NaN\n",
    "    df = df.dropna(how='all', axis=0)\n",
    "\n",
    "    # Reset index after dropping rows\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # if column contains '$' then remove it\n",
    "    col_to_keep = [i for i,x in enumerate(df.columns) if '$' not in df.iloc[:, i].values and '%' not in df.iloc[:, i].values and 'change' not in x.lower()]\n",
    "    df = df.iloc[:, col_to_keep]\n",
    "    df = df.loc[:, ~df.columns.duplicated(keep='first')]\n",
    "\n",
    "    # if row only contains same element then store the index and value in a dictionary\n",
    "    row_to_keep = {}\n",
    "    for i in range(len(df)):\n",
    "        if len(set(df.iloc[i, :])) == 1:\n",
    "            row_to_keep[i] = df.iloc[i, 0]\n",
    "\n",
    "    # Use the dictionary to assign the value as first level of multiindex for the rows in between the rows to keep\n",
    "    df['Category'] = df.iloc[:, 0]\n",
    "    for i in range(len(df)):\n",
    "        if i in row_to_keep.keys():\n",
    "            df.iloc[i, -1] = row_to_keep[i]\n",
    "        else:\n",
    "            df.iloc[i, -1] = df.iloc[i-1, -1]\n",
    "\n",
    "\n",
    "    row_to_keep = []\n",
    "    for i in range(len(df)):\n",
    "        if len(set(df.iloc[i, :])) != 1:\n",
    "            row_to_keep.append(i)\n",
    "\n",
    "    df = df.iloc[row_to_keep, :]\n",
    "    multiindex = df.iloc[:,[-1,0]]\n",
    "\n",
    "    # replace any cell that contains : with empty string\n",
    "    df = df.replace(to_replace=':', value='', regex=True)\n",
    "\n",
    "    # Combine two columns to create a multiindex for the rows\n",
    "    df.index = pd.MultiIndex.from_arrays([df.iloc[:,-1].values, df.iloc[:,0].values])\n",
    "    df = df.iloc[:, 1:-1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the logging settings\n",
    "logging.basicConfig(level=logging.DEBUG, filename='loop_logs.log', filemode='w', format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_read = html_files[6]\n",
    "file_name = file_to_read.split(\"\\\\\")[-2]\n",
    "print(f'Reading html files... {file_name}')\n",
    "html_soup = read_html(file_to_read)\n",
    "target_table = find_target_table(re.compile(r'.*(Products and services|Net sales|Sales Data|net sales by operating segment).*'), html_soup, search_type='strict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_table_from_html(target_table[4])\n",
    "df = clean_df(df)\n",
    "df = df.stack(level=0).reset_index(level=2).reset_index()\n",
    "df.columns = ['Category', 'Segment', 'Date_info', 'Value']\n",
    "df.to_dict('records')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.DataFrame()\n",
    "number_of_files = len(html_files)\n",
    "for file in html_files[:]:\n",
    "    logging.info(file)\n",
    "    file_name = file.split(\"\\\\\")[-2]\n",
    "    \n",
    "    try:\n",
    "        html_soup = read_html(file)\n",
    "        logging.info(f'Step 1: {file_name}: Read html successfully.')\n",
    "    \n",
    "        target_table = find_target_table(re.compile(r'.*(Sales Data|net sales by operating segment|net sales|products and services).*'), html_soup, search_type='strict')\n",
    "        logging.info(f'Step 2: {file_name}: Found target table successfully.')\n",
    "\n",
    "        assert target_table is not None, f'{file_name}: Target table not found.'\n",
    "        df = extract_table_from_html(target_table).iloc[:, :]\n",
    "        logging.info(f'Step 3: {file_name}: Extracted table successfully.')\n",
    "\n",
    "        df = clean_df(df)\n",
    "        logging.info(f'Step 4: {file_name}: Cleaned table successfully.')\n",
    "\n",
    "        df = df.stack(level=0).reset_index(level=2).reset_index()\n",
    "        logging.info(f'Step 5: {file_name}: Stacked table successfully.')\n",
    "    \n",
    "        df.columns = ['Category', 'Segment', 'Date_info', 'Value']\n",
    "        logging.info(f'Step 6: {file_name}: Renamed columns successfully.')\n",
    "    \n",
    "        all_df = pd.concat([all_df, df], axis=0, ignore_index=True)\n",
    "        logging.info(f'Step 7: {file_name}: Concatenated table successfully.')\n",
    "    except Exception as e:\n",
    "        logging.info(f'Step 7: {file_name}: Concatenated table unsuccessfully.')\n",
    "        logging.error(e)\n",
    "    \n",
    "    number_of_files -= 1\n",
    "    logging.info(f'Number of files left: {number_of_files}')\n",
    "    break\n",
    "all_df['Date'] = all_df['Date_info'].str.extract(r\"([A-Za-z]+\\s\\d{1,2},\\s\\d{4})\")\n",
    "\n",
    "# Extract three months ended text from date column\n",
    "all_df['Time Leading To'] = all_df['Date_info'].str.extract(r\"([A-Za-z]+\\s[A-Za-z]+\\s[A-Za-z]+)\")\n",
    "\n",
    "all_df.drop(columns=['Date_info'], inplace=True)\n",
    "\n",
    "# reorder columns\n",
    "all_df = all_df[['Category', 'Segment', 'Time Leading To', 'Date', 'Value']]\n",
    "all_df.to_csv('all_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_soup = read_html(html_files[6])\n",
    "target_text = re.compile(r'.*(sales data|net sales|products).*')\n",
    "target_element = html_soup.find_all('div')\n",
    "# all_text = [i for i in target_element if target_text.search(i.text.lower())]\n",
    "all_text = [j.find_next('table') for j in [i for i in target_element if target_text.search(i.text)]]\n",
    "len(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in all_text[:22]:\n",
    "    table = extract_table_from_html(i)\n",
    "    if len(table) != 0:\n",
    "        actual = table\n",
    "df = clean_df(actual)\n",
    "df = df.stack(level=0).reset_index(level=2).reset_index()\n",
    "\n",
    "df.columns = ['Category', 'Segment', 'Date_info', 'Value']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_dict(orient='records')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data_table_02 = pd.read_html(r\"sec-edgar-filings\\0000320193\\10-Q\\0000912057-02-004945\\filing-details.html\")\n",
    "sales_data_table_23 = pd.read_html(r\"sec-edgar-filings\\0000320193\\10-Q\\0000320193-23-000006\\filing-details.html\")\n",
    "sales_data_table_15 = pd.read_html(r\"sec-edgar-filings\\0000320193\\10-Q\\0001193125-15-259935\\filing-details.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data_table_15[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sales_data_table_15[38]\n",
    "# Use first three rows for the header\n",
    "header = df.iloc[0:3].fillna('').agg(' '.join).str.strip()\n",
    "df.columns = header\n",
    "\n",
    "# Remove the rows used for header and reset index\n",
    "df = df.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "# Remove columns where all values are NaN\n",
    "df = df.dropna(how='all', axis=1)\n",
    "\n",
    "# Replace empty strings with NaN\n",
    "df.replace(\"\", np.nan, inplace=True)\n",
    "\n",
    "# Forward fill NaN values along columns\n",
    "df.fillna(method='ffill', axis=1, inplace=True)\n",
    "\n",
    "# Remove rows where all values are NaN\n",
    "df = df.dropna(how='all', axis=0)\n",
    "\n",
    "# Reset index after dropping rows\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# if column contains '$' then remove it\n",
    "col_to_keep = [i for i,x in enumerate(df.columns) if '$' not in df.iloc[:, i].values]\n",
    "df = df.iloc[:, col_to_keep]\n",
    "df = df.loc[:, ~df.columns.duplicated(keep='first')]\n",
    "\n",
    "# if row only contains same element then store the index and value in a dictionary\n",
    "row_to_keep = {}\n",
    "for i in range(len(df)):\n",
    "    if len(set(df.iloc[i, :])) == 1:\n",
    "        row_to_keep[i] = df.iloc[i, 0]\n",
    "\n",
    "# Use the dictionary to assign the value as first level of multiindex for the rows in between the rows to keep\n",
    "df['Category'] = df.iloc[:, 0]\n",
    "for i in range(len(df)):\n",
    "    if i in row_to_keep.keys():\n",
    "        df.iloc[i, -1] = row_to_keep[i]\n",
    "    else:\n",
    "        df.iloc[i, -1] = df.iloc[i-1, -1]\n",
    "\n",
    "\n",
    "row_to_keep = []\n",
    "for i in range(len(df)):\n",
    "    if len(set(df.iloc[i, :])) != 1:\n",
    "        row_to_keep.append(i)\n",
    "\n",
    "df = df.iloc[row_to_keep, :]\n",
    "multiindex = df.iloc[:,[-1,0]]\n",
    "\n",
    "# replace any cell that contains : with empty string\n",
    "df = df.replace(to_replace=':', value='', regex=True)\n",
    "\n",
    "# Combine two columns to create a multiindex for the rows\n",
    "df.index = pd.MultiIndex.from_arrays([df.iloc[:,-1].values, df.iloc[:,0].values])\n",
    "df = df.iloc[:, 1:-1]\n",
    "\n",
    "# unpivot the dataframe based on the third column\n",
    "df = df.stack(level=0).reset_index(level=2).reset_index()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find a way to convert data in the tables to a key-value pair in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert df to dictionary and label with AAPL\n",
    "df = df.to_dict(orient='index')\n",
    "df = {'AAPL': df}\n",
    "df # shares in thousands, eps in per share, and others in millions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test SEC API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import trange\n",
    "import logging\n",
    "import re\n",
    "\n",
    "class SECData:\n",
    "    \"\"\"Class to retrieve data from SEC Edgar database.\n",
    "\n",
    "    Args:\n",
    "        requester_name (str): Name of the requester\n",
    "        requester_email (str): Email of the requester\n",
    "        taxonomy (str): us-gaap, ifrs-full, dei, or srt\n",
    "\n",
    "    Raises:\n",
    "        Exception: If taxonomy is not one of the following: us-gaap, ifrs-full, dei, or srt\n",
    "\n",
    "    Attributes:\n",
    "        BASE_API_URL (str): Base url for SEC Edgar database\n",
    "        US_GAAP_TAXONOMY_URL (str): URL for us-gaap taxonomy\n",
    "        ALLOWED_TAXONOMIES (list): List of allowed taxonomies\n",
    "        headers (dict): Headers to be used for API calls\n",
    "        cik (DataFrame): DataFrame containing CIK and ticker\n",
    "        tags (list): List of tags in us-gaap taxonomy\n",
    "        taxonomy (str): us-gaap, ifrs-full, dei, or srt\n",
    "\n",
    "    Methods:\n",
    "        get_cik_list: Retrieves the full list of CIK available from SEC database.\n",
    "        get_ticker_cik: Get a specific ticker's CIK number. \n",
    "        get_usgaap_tags: Get the list of tags in us-gaap taxonomy.\n",
    "        get_submissions: Retrieves the list of submissions for a specific CIK.\n",
    "        get_company_concept: Retrieves the XBRL disclosures from a single company (CIK) \n",
    "            and concept (a taxonomy and tag) into a single JSON file.\n",
    "        get_company_facts: Retrieves the XBRL disclosures from a single company (CIK) \n",
    "            into a single JSON file.\n",
    "        get_frames: Retrieves one fact for each reporting entity that is last filed that most closely fits the calendrical period requested.\n",
    "    \"\"\"\n",
    "\n",
    "    BASE_API_URL = \"https://data.sec.gov/\"\n",
    "    BASE_SEC_URL = \"https://www.sec.gov/\"\n",
    "    BASE_DIRECTORY_URL = \"https://www.sec.gov/Archives/edgar/data/\"\n",
    "    US_GAAP_TAXONOMY_URL = \"https://xbrl.fasb.org/us-gaap/2023/elts/us-gaap-2023.xsd\"\n",
    "    ALLOWED_TAXONOMIES = ['us-gaap', 'ifrs-full', 'dei', 'srt']\n",
    "    INDEX_EXTENSION = ['-index.html', '-index-headers.html']\n",
    "    FILE_EXTENSIONS = ['.xsd', '.htm', '_cal.xml',\n",
    "                       '_def.xml', '_lab.xml', '_pre.xml', '_htm.xml', '.xml']\n",
    "\n",
    "    def __init__(self, requester_company: str = 'Financial API', requester_name: str = 'API Caller', requester_email: str = 'apicaller@gmail.com', taxonomy: str = 'us-gaap',):\n",
    "        # Initialize logger\n",
    "        self.scrape_logger = logging.getLogger('sec_scraper')\n",
    "        self.scrape_logger.setLevel(logging.DEBUG)\n",
    "\n",
    "        # Check if the self.scrape_logger already has handlers to avoid duplicate logging.\n",
    "        if not self.scrape_logger.hasHandlers():\n",
    "            # Create a file handler\n",
    "            file_handler = logging.FileHandler(\n",
    "                r'C:\\Users\\lianz\\Python\\finance-dashboard\\utils\\sec-scraper\\logs\\sec_logs.log')\n",
    "            file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "            # Create a stream handler\n",
    "            stream_handler = logging.StreamHandler()\n",
    "            stream_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "            # Create a logging format\n",
    "            formatter = logging.Formatter(\n",
    "                '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            file_handler.setFormatter(formatter)\n",
    "            stream_handler.setFormatter(formatter)\n",
    "\n",
    "            # Add the handlers to the self.scrape_logger\n",
    "            self.scrape_logger.addHandler(file_handler)\n",
    "            self.scrape_logger.addHandler(stream_handler)\n",
    "\n",
    "        self.requester_company = requester_company\n",
    "        self.requester_name = requester_name\n",
    "        self.requester_email = requester_email\n",
    "        self.sec_headers = {\"User-Agent\": f\"{requester_company} {requester_name} {requester_email}\",\n",
    "                            \"Accept-Encoding\": \"gzip, deflate\",\n",
    "                            \"Host\": \"www.sec.gov\"}\n",
    "        self.sec_data_headers = {\"User-Agent\": f\"{requester_company} {requester_name} {requester_email}\",\n",
    "                                 \"Accept-Encoding\": \"gzip, deflate\",\n",
    "                                 \"Host\": \"data.sec.gov\"}\n",
    "        self.cik = self.get_cik_list()\n",
    "        self.tags = self.get_usgaap_tags()\n",
    "        if taxonomy not in self.ALLOWED_TAXONOMIES:\n",
    "            raise ValueError(\n",
    "                f\"Taxonomy {taxonomy} is not supported. Please use one of the following taxonomies: {self.ALLOWED_TAXONOMIES}\")\n",
    "        self.taxonomy = taxonomy\n",
    "\n",
    "\n",
    "    @sleep_and_retry\n",
    "    @limits(calls=10, period=1)\n",
    "    def rate_limited_request(self, url: str, headers: dict):\n",
    "        \"\"\"Rate limited request to SEC Edgar database.\n",
    "\n",
    "        Args:\n",
    "            url (str): URL to retrieve data from\n",
    "            headers (dict): Headers to be used for API calls\n",
    "\n",
    "        Returns:\n",
    "            response: Response from API call\n",
    "        \"\"\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            self.scrape_logger.error(f'''Request failed at URL: {url}''')\n",
    "        else:\n",
    "            self.scrape_logger.info(f'''Request successful at URL: {url}''')\n",
    "        return response\n",
    "\n",
    "    def get_cik_list(self):\n",
    "        \"\"\"Retrieves the full list of CIK available from SEC database.\n",
    "\n",
    "        Raises:\n",
    "            Exception: On failure to retrieve CIK list\n",
    "\n",
    "        Returns:\n",
    "            cik_df: DataFrame containing CIK and ticker\n",
    "        \"\"\"\n",
    "        url = r\"https://www.sec.gov/files/company_tickers.json\"\n",
    "        cik_raw = self.rate_limited_request(url, self.sec_headers)\n",
    "        cik_json = cik_raw.json()\n",
    "        cik_df = pd.DataFrame.from_dict(cik_json).T\n",
    "        return cik_df\n",
    "\n",
    "    def get_ticker_cik(self, ticker: str,):\n",
    "        \"\"\"Get a specific ticker's CIK number. \n",
    "        CIK########## is the entity's 10-digit Central Index Key (CIK).\n",
    "\n",
    "        Args:\n",
    "            ticker (str): public ticker symbol of the company\n",
    "\n",
    "        Returns:\n",
    "            cik: CIK number of the company excluding the leading 'CIK'\n",
    "        \"\"\"\n",
    "        ticker_cik = self.cik.query(f\"ticker == '{ticker}'\")['cik_str']\n",
    "        cik = f\"{ticker_cik.iloc[0]:010d}\"\n",
    "        return cik\n",
    "\n",
    "    def get_usgaap_tags(self, xsd_url: str = US_GAAP_TAXONOMY_URL):\n",
    "        \"\"\"Get the list of tags (elements) in us-gaap taxonomy or provide a different xsd_url to get tags from a different taxonomy.\n",
    "\n",
    "        Returns:\n",
    "            list of tags\n",
    "        \"\"\"\n",
    "        response = self.rate_limited_request(xsd_url, headers=self.sec_headers)\n",
    "        xsd_content = response.text\n",
    "        root = ET.fromstring(xsd_content)\n",
    "\n",
    "        return [element.attrib['name'] for element in root.findall(\".//{http://www.w3.org/2001/XMLSchema}element\")]\n",
    "\n",
    "    def get_submissions(self, cik: str = None, submission_file: str = None) -> dict:\n",
    "        if cik is not None:\n",
    "            url = f\"{self.BASE_API_URL}submissions/CIK{cik}.json\"\n",
    "        elif submission_file is not None:\n",
    "            url = f\"{self.BASE_API_URL}submissions/{submission_file}\"\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Please provide either a CIK number or a submission file.\")\n",
    "        \n",
    "        response = self.rate_limited_request(\n",
    "            url, headers=self.sec_data_headers)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\n",
    "                f\"Failed to retrieve submissions. Status code: {response.status_code}\")\n",
    "        data = json.loads(response.text)\n",
    "        return data\n",
    "\n",
    "    def get_company_concept(self, cik: str, tag: str, taxonomy: str = 'us-gaap',):\n",
    "        \"\"\"The company-concept API returns all the XBRL disclosures from a single company (CIK) \n",
    "        and concept (a taxonomy and tag) into a single JSON file, with a separate array of facts \n",
    "        for each units on measure that the company has chosen to disclose \n",
    "        (e.g. net profits reported in U.S. dollars and in Canadian dollars).\n",
    "\n",
    "        Args:\n",
    "            cik (str): CIK number of the company. Get the list using self.cik\n",
    "            taxonomy (str): us-gaap, ifrs-full, dei, or srt\n",
    "            tag (str): taxonomy tag (e.g. Revenue, AccountsPayableCurrent). See full list from https://xbrl.fasb.org/us-gaap/2023/elts/us-gaap-2023.xsd\n",
    "\n",
    "        Raises:\n",
    "            Exception: On failure to retrieve company concept either due to invalid CIK, taxonomy, or tag\n",
    "\n",
    "        Returns:\n",
    "            data: JSON file containing all the XBRL disclosures from a single company (CIK)\n",
    "        \"\"\"\n",
    "        url = f\"{self.BASE_API_URL}api/xbrl/companyconcept/CIK{cik}/{taxonomy}/{tag}.json\"\n",
    "        response = self.rate_limited_request(\n",
    "            url, headers=self.sec_data_headers)\n",
    "        data = json.loads(response.text)\n",
    "        return data\n",
    "\n",
    "    def get_company_facts(self, cik):\n",
    "        url = f\"{self.BASE_API_URL}api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "        response = self.rate_limited_request(\n",
    "            url, headers=self.sec_data_headers)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\n",
    "                f\"Failed to retrieve company facts for CIK {cik}. Status code: {response.status_code}\")\n",
    "        data = json.loads(response.text)\n",
    "        return data\n",
    "\n",
    "    def get_frames(self, taxonomy, tag, unit, period):\n",
    "        \"\"\"The xbrl/frames API aggregates one fact for each reporting entity that is last filed that most closely fits the calendrical period requested. \n",
    "        This API supports for annual, quarterly and instantaneous data: https://data.sec.gov/api/xbrl/frames/us-gaap/AccountsPayableCurrent/USD/CY2019Q1I.json\n",
    "\n",
    "        Args:\n",
    "            taxonomy (str): us-gaap, ifrs-full, dei, or srt\n",
    "            tag (str): taxonomy tag (e.g. Revenue, AccountsPayableCurrent). See full list from https://xbrl.fasb.org/us-gaap/2023/elts/us-gaap-2023.xsd\n",
    "            unit (str): USD, USD-per-shares, etc.\n",
    "            period (str): CY#### for annual data (duration 365 days +/- 30 days), CY####Q# for quarterly data (duration 91 days +/- 30 days), CY####Q#I for instantaneous data\n",
    "\n",
    "        Raises:\n",
    "            Exception: (placeholder)\n",
    "\n",
    "        Returns:\n",
    "            data: json formatted response\n",
    "        \"\"\"\n",
    "        url = f\"{self.BASE_API_URL}api/xbrl/frames/{taxonomy}/{tag}/{unit}/{period}.json\"\n",
    "        response = self.rate_limited_request(\n",
    "            url, headers=self.sec_data_headers)\n",
    "        data = json.loads(response.text)\n",
    "        return data\n",
    "\n",
    "    def get_data_as_dataframe(self, cik: str,):\n",
    "        \"\"\"Retrieves the XBRL disclosures from a single company (CIK) and returns it as a pandas dataframe.\n",
    "\n",
    "        Args:\n",
    "            cik (str): CIK number of the company. Get the list using self.cik\n",
    "\n",
    "        Returns:\n",
    "            df: pandas dataframe containing the XBRL disclosures from a single company (CIK)\n",
    "        \"\"\"\n",
    "        data = self.get_company_facts(cik)\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for tag in data['facts'][self.taxonomy]:\n",
    "            facts = data['facts']['us-gaap'][tag]['units']\n",
    "            unit_key = list(facts.keys())[0]\n",
    "            temp_df = pd.DataFrame(facts[unit_key])\n",
    "            temp_df['label'] = tag\n",
    "            df = pd.concat([df, temp_df], axis=0, ignore_index=True)\n",
    "        df = df.astype({'val': 'float64',\n",
    "                        'end': 'datetime64[ns]',\n",
    "                        'start': 'datetime64[ns]',\n",
    "                        'filed': 'datetime64[ns]'})\n",
    "        df['Months Ended'] = (df['end'] - df['start']\n",
    "                              ).dt.days.div(30.4375).round(0)\n",
    "        return df\n",
    "\n",
    "    def get_index(self, cik: str = None,) -> dict:\n",
    "        \"\"\"Each CIK directory and all child subdirectories contain three files to assist in \n",
    "        automated crawling of these directories. \n",
    "        These are not visible through directory browsing.\n",
    "            - index.html (the web browser would normally receive these)\n",
    "            - index.xml (a XML structured version of the same content)\n",
    "            - index.json (a JSON structured vision of the same content)\n",
    "\n",
    "        Args:\n",
    "            cik (str): CIK number of the company. Get the list using self.cik\n",
    "\n",
    "        Returns:\n",
    "            json: pandas dataframe containing the XBRL disclosures from a single company (CIK)\n",
    "        \"\"\"\n",
    "        if cik is not None:\n",
    "            url = self.BASE_DIRECTORY_URL + cik + '/' + 'index.json'\n",
    "\n",
    "        else:\n",
    "            url = self.BASE_DIRECTORY_URL + self.cik + '/' + 'index.json'\n",
    "\n",
    "        response = self.rate_limited_request(url, headers=self.sec_headers)\n",
    "        return response.json()\n",
    "\n",
    "\n",
    "class TickerData(SECData):\n",
    "    \"\"\"Inherited from SECData class. Retrieves data from SEC Edgar database based on ticker.\n",
    "    url is constructed based on the following: https://www.sec.gov/Archives/edgar/data/{cik}/{ascension_number}/{file_name}\n",
    "    cik is the CIK number of the company = access via get_ticker_cik\n",
    "    ascension_number is the accessionNumber column of filings_df\n",
    "    file name for xml is always '{ticker}-{reportDate}.{extension}\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ticker: str, requester_company: str = 'Financial API', requester_name: str = 'API Caller', requester_email: str = 'apicaller@gmail.com', taxonomy: str = 'us-gaap',):\n",
    "        super().__init__(requester_company, requester_name, requester_email, taxonomy)\n",
    "        self.ticker = ticker.upper()\n",
    "        self.cik = self.get_ticker_cik(self.ticker)\n",
    "        self._submissions = None\n",
    "        self._filings = None\n",
    "        self.forms = self.filings['form'].unique()\n",
    "        self._index = self.get_index(self.cik)\n",
    "        self._filing_folder_urls = None\n",
    "        self._filing_urls = None\n",
    "\n",
    "    @property\n",
    "    def submissions(self,) -> dict:\n",
    "        if self._submissions is None:\n",
    "            self._submissions = self.get_submissions(self.cik)\n",
    "        return self._submissions\n",
    "\n",
    "    @property\n",
    "    def filings(self,) -> pd.DataFrame:\n",
    "        if self._filings is None:\n",
    "            self._filings = self.get_filings()\n",
    "        return self._filings\n",
    "\n",
    "    @property\n",
    "    def latest_filing(self,) -> pd.DataFrame:\n",
    "        return self.filings.iloc[0, :].to_dict()\n",
    "    \n",
    "    @property\n",
    "    def latest_10Q(self,) -> pd.DataFrame:\n",
    "        return self.filings.query(\"form == '10-Q'\").iloc[0, :].to_dict()\n",
    "    \n",
    "    @property\n",
    "    def latest_10K(self,) -> pd.DataFrame:\n",
    "        return self.filings.query(\"form == '10-K'\").iloc[0, :].to_dict()\n",
    "    \n",
    "    @property \n",
    "    def latest_8K(self,) -> pd.DataFrame:\n",
    "        return self.filings.query(\"form == '8-K'\").iloc[0, :].to_dict()\n",
    "\n",
    "    @property\n",
    "    def filing_folder_urls(self,) -> list:\n",
    "        if self._filing_folder_urls is None:\n",
    "            self._filing_folder_urls = self._get_filing_folder_urls()\n",
    "        return self._filing_folder_urls\n",
    "\n",
    "    @property\n",
    "    def filing_urls(self,) -> list:\n",
    "        if self._filing_urls is None:\n",
    "            self._filing_urls = self.filings['file_url'].tolist()\n",
    "\n",
    "        return self._filing_urls\n",
    "\n",
    "    def _get_filing_folder_urls(self,) -> list:\n",
    "        \"\"\"Get filing folder urls from index dict.\n",
    "\n",
    "        Args:\n",
    "            index (dict): index dict from get_index method\n",
    "\n",
    "        Returns:s\n",
    "            filing_folder_urls (list): list of filing folder urls\n",
    "        \"\"\"\n",
    "        filing_folder_urls = [self.BASE_SEC_URL + self._index['directory']['name'] + '/' + folder['name']\n",
    "                              for folder in self._index['directory']['item'] if folder['type'] == 'folder.gif']\n",
    "        return filing_folder_urls\n",
    "\n",
    "\n",
    "    def _get_filing_urls(self,) -> list:\n",
    "        \"\"\"(DEPRECATED)\n",
    "        ---The filing urls are implemented in the get_filings method.---\n",
    "        \n",
    "        Get filing urls from filing folder urls.\n",
    "\n",
    "        Args:\n",
    "            filing_folder_urls (list): list of filing folder urls\n",
    "\n",
    "        Returns:\n",
    "            filing_urls (list): list of filing urls to .txt files\n",
    "        \"\"\"\n",
    "        filing_urls = []\n",
    "        with trange(len(self.filing_folder_urls), desc=f'Instantiating filing urls for {self.ticker}...') as t:\n",
    "            for i in t:\n",
    "                self.scrape_logger.info(t)\n",
    "                try:\n",
    "                    soup = self.get_file_data(self.filing_folder_urls[i])\n",
    "                    for link in soup.find_all('a'):\n",
    "                        if link.get('href').endswith('.txt'):\n",
    "                            filing_urls.append(\n",
    "                                self.BASE_SEC_URL + link.get('href'))\n",
    "                except Exception as e:\n",
    "                    self.scrape_logger.error(\n",
    "                        f'Failed to instantiate filing urls for {self.ticker}...')\n",
    "                    self.scrape_logger.error(e)\n",
    "                    t.write(\n",
    "                        f'Failed to instantiate filing urls for {self.ticker}...')\n",
    "                    continue\n",
    "        return filing_urls\n",
    "    \n",
    "\n",
    "    def get_filings(self,) -> dict:\n",
    "        \"\"\"Get filings and urls to .txt from submissions dict.\n",
    "\n",
    "        Args:\n",
    "            submissions (dict): submissions dict from get_submissions method\n",
    "\n",
    "        Returns:\n",
    "            filings (dict): dictionary containing filings\n",
    "        \"\"\"\n",
    "        self.scrape_logger.info(f'Retrieving filings for {self.ticker}...')\n",
    "        filings = self.submissions['filings']['recent']\n",
    "\n",
    "        if len(self.submissions['filings']) > 1:\n",
    "            self.scrape_logger.info(\n",
    "                f'Additional filings found for {self.ticker}...')\n",
    "            for file in self.submissions['filings']['files']:\n",
    "                additional_filing = self.get_submissions(submission_file=file['name'])\n",
    "                filings = {key: filings[key] + additional_filing[key] for key in filings.keys()}\n",
    "\n",
    "        filings = pd.DataFrame(filings)\n",
    "\n",
    "        # Convert reportDate, filingDate, acceptanceDateTime columns to datetime\n",
    "        filings['reportDate'] = pd.to_datetime(filings['reportDate'])\n",
    "        filings['filingDate'] = pd.to_datetime(filings['filingDate'])\n",
    "        filings['acceptanceDateTime'] = pd.to_datetime(\n",
    "            filings['acceptanceDateTime'])\n",
    "        \n",
    "        # get folder url for each row\n",
    "        filings['folder_url'] = self.BASE_DIRECTORY_URL + \\\n",
    "            self.cik + '/' + filings['accessionNumber'].str.replace('-', '')\n",
    "\n",
    "        # get file url for each row\n",
    "        filings['file_url'] = filings['folder_url'] + '/' + filings['accessionNumber'] + '.txt'\n",
    "        \n",
    "        return filings\n",
    "    \n",
    "\n",
    "    def get_file_data(self, file_url: str) -> BeautifulSoup:\n",
    "        \"\"\"Get file data from file url which can be retrieved by calling self.get_file_url method.\n",
    "\n",
    "        Args:\n",
    "            file_url (str): File url to retrieve data from on the SEC website\n",
    "\n",
    "        Returns:\n",
    "            data: File data as a BeautifulSoup object\n",
    "        \"\"\"\n",
    "        data = self.rate_limited_request(\n",
    "            url=file_url, headers=self.sec_headers)\n",
    "        try:\n",
    "            soup = BeautifulSoup(data.content, \"lxml\")\n",
    "            self.scrape_logger.info(\n",
    "                f'Parsed file data from {file_url} successfully.')\n",
    "            return soup\n",
    "\n",
    "        except Exception as e:\n",
    "            self.scrape_logger.error(\n",
    "                f'Failed to parse file data from {file_url}. Error: {e}')\n",
    "            raise Exception(\n",
    "                f'Failed to parse file data from {file_url}. Error: {e}')\n",
    "\n",
    "\n",
    "    def search_tags(self, soup: BeautifulSoup, pattern: str) -> BeautifulSoup:\n",
    "        \"\"\"Search for tags in BeautifulSoup object.\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): BeautifulSoup object\n",
    "            pattern (str): regex pattern to search for\n",
    "\n",
    "        Returns:\n",
    "            soup: BeautifulSoup object\n",
    "        \"\"\"\n",
    "        return soup.find_all(re.compile(pattern))\n",
    "\n",
    "\n",
    "    def search_context(self, soup: BeautifulSoup) -> pd.DataFrame:\n",
    "        \"\"\"Search for context in company .txt filing. \n",
    "        Context provides information about the entity, segment, and time period for facts in the filing.\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): BeautifulSoup object\n",
    "\n",
    "        Returns:\n",
    "            df: DataFrame containing context information with columns \n",
    "            {\n",
    "                'contextId': str,\n",
    "                'entity': str,\n",
    "                'segment': str,\n",
    "                'startDate': 'datetime64[ns]',\n",
    "                'endDate': 'datetime64[ns]',\n",
    "                'instant': 'datetime64[ns]'\n",
    "            }\n",
    "        \"\"\"\n",
    "        contexts = self.search_tags(soup, '^context$')\n",
    "        dict_list = []\n",
    "        columns = {'contextId': str, 'entity': str, 'segment': str,\n",
    "                   'startDate': 'datetime64[ns]', 'endDate': 'datetime64[ns]', 'instant': 'datetime64[ns]'}\n",
    "        for tag in contexts:\n",
    "            temp_dict = {}\n",
    "            temp_dict['contextId'] = tag.attrs.get('id')\n",
    "            temp_dict['entity'] = tag.find(\"entity\").text.split()[\n",
    "                0] if tag.find(\"entity\") is not None else None\n",
    "            temp_dict['segment'] = tag.find(\"segment\").text.strip(\n",
    "            ) if tag.find(\"segment\") is not None else None\n",
    "            temp_dict['startDate'] = tag.find(\"startdate\").text if tag.find(\n",
    "                \"startdate\") is not None else None\n",
    "            temp_dict['endDate'] = tag.find(\"enddate\").text if tag.find(\n",
    "                \"enddate\") is not None else None\n",
    "            temp_dict['instant'] = tag.find(\"instant\").text if tag.find(\n",
    "                \"instant\") is not None else None\n",
    "            dict_list.append(temp_dict)\n",
    "\n",
    "        df = pd.DataFrame(dict_list, columns=columns.keys()).astype(columns)\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def search_linklabels(self, soup: BeautifulSoup) -> pd.DataFrame:\n",
    "        \"\"\"Search for link labels in company .txt filing. \n",
    "        Link labels provide information about the relationship between facts and their corresponding concepts.\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): BeautifulSoup object\n",
    "\n",
    "        Returns:\n",
    "            df: DataFrame containing link label information with columns \n",
    "            {\n",
    "                'linkLabelId': str,\n",
    "                'xlinkLabel': str,\n",
    "                'xlinkRole': str,\n",
    "                'xlinkType': str,\n",
    "                'xlmnsXml': str,\n",
    "                'xmlLang': str,\n",
    "                'label': str\n",
    "            }\n",
    "        \"\"\"\n",
    "        links = self.search_tags(soup, '^link:label$')\n",
    "        dict_list = []\n",
    "        columns = {'linkLabelId': str, 'xlinkLabel': str, 'xlinkRole': str,\n",
    "                    'xlinkType': str, 'xlmnsXml': str, 'xmlLang': str, 'label': str}\n",
    "\n",
    "        for tag in links:\n",
    "            temp_dict = {}\n",
    "            temp_dict['linkLabelId'] = tag.attrs.get('id')\n",
    "            temp_dict['xlinkLabel'] = tag.attrs.get('xlink:label')\n",
    "            temp_dict['xlinkRole'] = tag.attrs.get('xlink:role')\n",
    "            temp_dict['xlinkType'] = tag.attrs.get('xlink:type')\n",
    "            temp_dict['xlmnsXml'] = tag.attrs.get('xmlns:xml')\n",
    "            temp_dict['xmlLang'] = tag.attrs.get('xml:lang')\n",
    "            temp_dict['label'] = tag.text if tag.text is not None else None\n",
    "            dict_list.append(temp_dict)\n",
    "\n",
    "        df = pd.DataFrame(dict_list, columns=columns.keys()).astype(columns)\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def search_facts(self, soup: BeautifulSoup) -> pd.DataFrame:\n",
    "        \"\"\"Search for facts in company .txt filing. \n",
    "        Facts provide the actual data values for the XBRL disclosures.\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): BeautifulSoup object\n",
    "\n",
    "        Returns:\n",
    "            df: DataFrame containing fact information with columns \n",
    "            {\n",
    "                'factName': str,\n",
    "                'contextRef': str,\n",
    "                'decimals': int,\n",
    "                'factId': str,\n",
    "                'unitRef': str,\n",
    "                'value': str\n",
    "            }\n",
    "        \"\"\"\n",
    "        facts = self.search_tags(soup, '^us-gaap:')\n",
    "        dict_list = []\n",
    "        columns = {'factName': str, 'contextRef': str, 'decimals': int, 'factId': str,\n",
    "                   'unitRef': str, 'value': str}\n",
    "\n",
    "        for tag in facts:\n",
    "            temp_dict = {}\n",
    "            temp_dict['factName'] = tag.name\n",
    "            temp_dict['contextRef'] = tag.attrs.get('contextref')\n",
    "            temp_dict['decimals'] = tag.attrs.get('decimals')\n",
    "            temp_dict['factId'] = tag.attrs.get('id')\n",
    "            temp_dict['unitRef'] = tag.attrs.get('unitref')\n",
    "            temp_dict['value'] = tag.text\n",
    "            dict_list.append(temp_dict)\n",
    "\n",
    "        df = pd.DataFrame(dict_list, columns=columns.keys())\n",
    "        df['factNameMerge'] = df['factName'].str.replace(':', '_')\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "    def get_metalinks(self, metalinks_url: str) -> pd.DataFrame:\n",
    "        \"\"\"Get metalinks from metalinks url.\n",
    "\n",
    "        Args:\n",
    "            metalinks_url (str): metalinks url to retrieve data from\n",
    "\n",
    "        Returns:\n",
    "            df: DataFrame containing metalinks information with columns \n",
    "            {\n",
    "                'labelKey': str,\n",
    "                'localName': str,\n",
    "                'labelName': int,\n",
    "                'terseLabel': str,\n",
    "                'documentation': str,\n",
    "            }\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.rate_limited_request(\n",
    "                url=metalinks_url, headers=self.sec_headers).json()\n",
    "        except Exception as e:\n",
    "            self.scrape_logger.error(\n",
    "                f'Failed to retrieve metalinks from {metalinks_url}. Error: {e}')\n",
    "            raise Exception(\n",
    "                f'Failed to retrieve metalinks from {metalinks_url}. Error: {e}')\n",
    "        metalinks_instance = response['instance']\n",
    "        instance_key = list(metalinks_instance.keys())[0]\n",
    "        dict_list = []\n",
    "        for i in metalinks_instance[instance_key]['tag']:\n",
    "            dict_list.append(dict(labelKey=i.lower(), \n",
    "                localName=metalinks_instance[instance_key]['tag'][i].get('localname'),\n",
    "                labelName=metalinks_instance[instance_key]['tag'][i].get('lang').get('en-us').get('role').get('label'),\n",
    "                terseLabel=metalinks_instance[instance_key]['tag'][i].get('lang').get('en-us').get('role').get('terseLabel'),\n",
    "                documentation=metalinks_instance[instance_key]['tag'][i].get('lang').get('en-us').get('role').get('documentation'),))\n",
    "            \n",
    "        df = pd.DataFrame.from_dict(dict_list)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        class_name = type(self).__name__\n",
    "        main_attrs = ['ticker', 'cik', 'submissions', 'filings']\n",
    "        available_methods = [method_name for method_name in dir(self) if callable(getattr(self, method_name)) and not method_name.startswith(\"_\")]\n",
    "        return f\"\"\"{class_name}({self.ticker})\n",
    "    CIK: {self.cik}\n",
    "    Latest filing: {self.latest_filing['filingDate'].strftime('%Y-%m-%d')} for Form {self.latest_filing['form']}. Access via: {self.latest_filing['folder_url']}\n",
    "    Latest 10-Q: {self.latest_10Q['filingDate'].strftime('%Y-%m-%d')}. Access via: {self.latest_10Q['folder_url']}\n",
    "    Latest 10-K: {self.latest_10K['filingDate'].strftime('%Y-%m-%d')}. Access via: {self.latest_10K['folder_url']}\n",
    "\n",
    "Available methods:\n",
    "    {', '.join(available_methods)}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec = SECData()\n",
    "aapl = TickerData(requester_company='Financial Docs', requester_name='John Doe', requester_email='financial@gmail.com', taxonomy='us-gaap', ticker='aapl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get tags for filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.sec.gov/Archives/edgar/data/0000320193/000032019322000108/0000320193-22-000108.txt'"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find reporting time frame of reportDate\n",
    "soup = aapl.get_file_data(aapl.latest_10K['file_url'])\n",
    "aapl.latest_10K['file_url']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = aapl.search_context(soup)\n",
    "facts = aapl.search_facts(soup)\n",
    "labels = aapl.search_linklabels(soup)\n",
    "metalinks = aapl.get_metalinks(aapl.latest_10K['folder_url'] + '/MetaLinks.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "facts_w_context = facts.merge(right=context, left_on='contextRef', right_on='contextId', how='left').merge(right=metalinks, left_on='factNameMerge', right_on='labelKey', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>factName</th>\n",
       "      <th>contextRef</th>\n",
       "      <th>decimals</th>\n",
       "      <th>factId</th>\n",
       "      <th>unitRef</th>\n",
       "      <th>value</th>\n",
       "      <th>factNameMerge</th>\n",
       "      <th>contextId</th>\n",
       "      <th>entity</th>\n",
       "      <th>segment</th>\n",
       "      <th>startDate</th>\n",
       "      <th>endDate</th>\n",
       "      <th>instant</th>\n",
       "      <th>labelKey</th>\n",
       "      <th>localName</th>\n",
       "      <th>labelName</th>\n",
       "      <th>terseLabel</th>\n",
       "      <th>documentation</th>\n",
       "      <th>metalinksURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>us-gaap:revenueremainingperformanceobligatione...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-09-25</td>\n",
       "      <td>us-gaap_revenueremainingperformanceobligatione...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>us-gaap:revenueremainingperformanceobligatione...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-10-01</td>\n",
       "      <td>us-gaap_revenueremainingperformanceobligatione...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>us-gaap:revenueremainingperformanceobligatione...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-09-29</td>\n",
       "      <td>us-gaap_revenueremainingperformanceobligatione...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>us-gaap:revenueremainingperformanceobligatione...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2025-09-28</td>\n",
       "      <td>us-gaap_revenueremainingperformanceobligatione...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>us-gaap:revenueremainingperformanceobligatione...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-09-25</td>\n",
       "      <td>us-gaap_revenueremainingperformanceobligatione...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>us-gaap:noncurrentassets</td>\n",
       "      <td>i9e3c16d46ab346b49d8f31a4458e6ddb_I20210925</td>\n",
       "      <td>-6</td>\n",
       "      <td>id3VybDovL2RvY3MudjEvZG9jOmVmNWVmYjdhNzI4ZDQyO...</td>\n",
       "      <td>usd</td>\n",
       "      <td>3716000000</td>\n",
       "      <td>us-gaap_noncurrentassets</td>\n",
       "      <td>i9e3c16d46ab346b49d8f31a4458e6ddb_I20210925</td>\n",
       "      <td>0000320193</td>\n",
       "      <td>aapl:OtherCountriesMember</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2021-09-25</td>\n",
       "      <td>us-gaap_noncurrentassets</td>\n",
       "      <td>NoncurrentAssets</td>\n",
       "      <td>Long-Lived Assets</td>\n",
       "      <td>Long-lived assets</td>\n",
       "      <td>Long-lived assets other than financial instrum...</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000032...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>us-gaap:noncurrentassets</td>\n",
       "      <td>if3ba800b42a9406abf364f6d45d67e23_I20220924</td>\n",
       "      <td>-6</td>\n",
       "      <td>id3VybDovL2RvY3MudjEvZG9jOmVmNWVmYjdhNzI4ZDQyO...</td>\n",
       "      <td>usd</td>\n",
       "      <td>42117000000</td>\n",
       "      <td>us-gaap_noncurrentassets</td>\n",
       "      <td>if3ba800b42a9406abf364f6d45d67e23_I20220924</td>\n",
       "      <td>0000320193</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-09-24</td>\n",
       "      <td>us-gaap_noncurrentassets</td>\n",
       "      <td>NoncurrentAssets</td>\n",
       "      <td>Long-Lived Assets</td>\n",
       "      <td>Long-lived assets</td>\n",
       "      <td>Long-lived assets other than financial instrum...</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000032...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>us-gaap:noncurrentassets</td>\n",
       "      <td>i4e82d58a300a4a309eacd18ec8c3e8c7_I20210925</td>\n",
       "      <td>-6</td>\n",
       "      <td>id3VybDovL2RvY3MudjEvZG9jOmVmNWVmYjdhNzI4ZDQyO...</td>\n",
       "      <td>usd</td>\n",
       "      <td>39440000000</td>\n",
       "      <td>us-gaap_noncurrentassets</td>\n",
       "      <td>i4e82d58a300a4a309eacd18ec8c3e8c7_I20210925</td>\n",
       "      <td>0000320193</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2021-09-25</td>\n",
       "      <td>us-gaap_noncurrentassets</td>\n",
       "      <td>NoncurrentAssets</td>\n",
       "      <td>Long-Lived Assets</td>\n",
       "      <td>Long-lived assets</td>\n",
       "      <td>Long-lived assets other than financial instrum...</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000032...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>us-gaap:unrecognizedtaxbenefits</td>\n",
       "      <td>if3ba800b42a9406abf364f6d45d67e23_I20220924</td>\n",
       "      <td>-8</td>\n",
       "      <td>id3VybDovL2RvY3MudjEvZG9jOmVmNWVmYjdhNzI4ZDQyO...</td>\n",
       "      <td>usd</td>\n",
       "      <td>16800000000</td>\n",
       "      <td>us-gaap_unrecognizedtaxbenefits</td>\n",
       "      <td>if3ba800b42a9406abf364f6d45d67e23_I20220924</td>\n",
       "      <td>0000320193</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-09-24</td>\n",
       "      <td>us-gaap_unrecognizedtaxbenefits</td>\n",
       "      <td>UnrecognizedTaxBenefits</td>\n",
       "      <td>Unrecognized Tax Benefits</td>\n",
       "      <td>Gross unrecognized tax benefits</td>\n",
       "      <td>Amount of unrecognized tax benefits.</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000032...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>us-gaap:unrecognizedtaxbenefitsthatwouldimpact...</td>\n",
       "      <td>if3ba800b42a9406abf364f6d45d67e23_I20220924</td>\n",
       "      <td>-8</td>\n",
       "      <td>id3VybDovL2RvY3MudjEvZG9jOmVmNWVmYjdhNzI4ZDQyO...</td>\n",
       "      <td>usd</td>\n",
       "      <td>8000000000</td>\n",
       "      <td>us-gaap_unrecognizedtaxbenefitsthatwouldimpact...</td>\n",
       "      <td>if3ba800b42a9406abf364f6d45d67e23_I20220924</td>\n",
       "      <td>0000320193</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2022-09-24</td>\n",
       "      <td>us-gaap_unrecognizedtaxbenefitsthatwouldimpact...</td>\n",
       "      <td>UnrecognizedTaxBenefitsThatWouldImpactEffectiv...</td>\n",
       "      <td>Unrecognized Tax Benefits that Would Impact Ef...</td>\n",
       "      <td>Gross unrecognized tax benefits that would imp...</td>\n",
       "      <td>The total amount of unrecognized tax benefits ...</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/000032...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1032 rows  19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               factName  \\\n",
       "0     us-gaap:revenueremainingperformanceobligatione...   \n",
       "1     us-gaap:revenueremainingperformanceobligatione...   \n",
       "2     us-gaap:revenueremainingperformanceobligatione...   \n",
       "3     us-gaap:revenueremainingperformanceobligatione...   \n",
       "4     us-gaap:revenueremainingperformanceobligatione...   \n",
       "...                                                 ...   \n",
       "1027                           us-gaap:noncurrentassets   \n",
       "1028                           us-gaap:noncurrentassets   \n",
       "1029                           us-gaap:noncurrentassets   \n",
       "1030                    us-gaap:unrecognizedtaxbenefits   \n",
       "1031  us-gaap:unrecognizedtaxbenefitsthatwouldimpact...   \n",
       "\n",
       "                                       contextRef decimals  \\\n",
       "0                                            None     None   \n",
       "1                                            None     None   \n",
       "2                                            None     None   \n",
       "3                                            None     None   \n",
       "4                                            None     None   \n",
       "...                                           ...      ...   \n",
       "1027  i9e3c16d46ab346b49d8f31a4458e6ddb_I20210925       -6   \n",
       "1028  if3ba800b42a9406abf364f6d45d67e23_I20220924       -6   \n",
       "1029  i4e82d58a300a4a309eacd18ec8c3e8c7_I20210925       -6   \n",
       "1030  if3ba800b42a9406abf364f6d45d67e23_I20220924       -8   \n",
       "1031  if3ba800b42a9406abf364f6d45d67e23_I20220924       -8   \n",
       "\n",
       "                                                 factId unitRef        value  \\\n",
       "0                                                  None    None   2022-09-25   \n",
       "1                                                  None    None   2023-10-01   \n",
       "2                                                  None    None   2024-09-29   \n",
       "3                                                  None    None   2025-09-28   \n",
       "4                                                  None    None   2022-09-25   \n",
       "...                                                 ...     ...          ...   \n",
       "1027  id3VybDovL2RvY3MudjEvZG9jOmVmNWVmYjdhNzI4ZDQyO...     usd   3716000000   \n",
       "1028  id3VybDovL2RvY3MudjEvZG9jOmVmNWVmYjdhNzI4ZDQyO...     usd  42117000000   \n",
       "1029  id3VybDovL2RvY3MudjEvZG9jOmVmNWVmYjdhNzI4ZDQyO...     usd  39440000000   \n",
       "1030  id3VybDovL2RvY3MudjEvZG9jOmVmNWVmYjdhNzI4ZDQyO...     usd  16800000000   \n",
       "1031  id3VybDovL2RvY3MudjEvZG9jOmVmNWVmYjdhNzI4ZDQyO...     usd   8000000000   \n",
       "\n",
       "                                          factNameMerge  \\\n",
       "0     us-gaap_revenueremainingperformanceobligatione...   \n",
       "1     us-gaap_revenueremainingperformanceobligatione...   \n",
       "2     us-gaap_revenueremainingperformanceobligatione...   \n",
       "3     us-gaap_revenueremainingperformanceobligatione...   \n",
       "4     us-gaap_revenueremainingperformanceobligatione...   \n",
       "...                                                 ...   \n",
       "1027                           us-gaap_noncurrentassets   \n",
       "1028                           us-gaap_noncurrentassets   \n",
       "1029                           us-gaap_noncurrentassets   \n",
       "1030                    us-gaap_unrecognizedtaxbenefits   \n",
       "1031  us-gaap_unrecognizedtaxbenefitsthatwouldimpact...   \n",
       "\n",
       "                                        contextId      entity  \\\n",
       "0                                             NaN         NaN   \n",
       "1                                             NaN         NaN   \n",
       "2                                             NaN         NaN   \n",
       "3                                             NaN         NaN   \n",
       "4                                             NaN         NaN   \n",
       "...                                           ...         ...   \n",
       "1027  i9e3c16d46ab346b49d8f31a4458e6ddb_I20210925  0000320193   \n",
       "1028  if3ba800b42a9406abf364f6d45d67e23_I20220924  0000320193   \n",
       "1029  i4e82d58a300a4a309eacd18ec8c3e8c7_I20210925  0000320193   \n",
       "1030  if3ba800b42a9406abf364f6d45d67e23_I20220924  0000320193   \n",
       "1031  if3ba800b42a9406abf364f6d45d67e23_I20220924  0000320193   \n",
       "\n",
       "                        segment startDate endDate    instant  \\\n",
       "0                           NaN       NaT     NaT        NaT   \n",
       "1                           NaN       NaT     NaT        NaT   \n",
       "2                           NaN       NaT     NaT        NaT   \n",
       "3                           NaN       NaT     NaT        NaT   \n",
       "4                           NaN       NaT     NaT        NaT   \n",
       "...                         ...       ...     ...        ...   \n",
       "1027  aapl:OtherCountriesMember       NaT     NaT 2021-09-25   \n",
       "1028                       None       NaT     NaT 2022-09-24   \n",
       "1029                       None       NaT     NaT 2021-09-25   \n",
       "1030                       None       NaT     NaT 2022-09-24   \n",
       "1031                       None       NaT     NaT 2022-09-24   \n",
       "\n",
       "                                               labelKey  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "1027                           us-gaap_noncurrentassets   \n",
       "1028                           us-gaap_noncurrentassets   \n",
       "1029                           us-gaap_noncurrentassets   \n",
       "1030                    us-gaap_unrecognizedtaxbenefits   \n",
       "1031  us-gaap_unrecognizedtaxbenefitsthatwouldimpact...   \n",
       "\n",
       "                                              localName  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "1027                                   NoncurrentAssets   \n",
       "1028                                   NoncurrentAssets   \n",
       "1029                                   NoncurrentAssets   \n",
       "1030                            UnrecognizedTaxBenefits   \n",
       "1031  UnrecognizedTaxBenefitsThatWouldImpactEffectiv...   \n",
       "\n",
       "                                              labelName  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "1027                                  Long-Lived Assets   \n",
       "1028                                  Long-Lived Assets   \n",
       "1029                                  Long-Lived Assets   \n",
       "1030                          Unrecognized Tax Benefits   \n",
       "1031  Unrecognized Tax Benefits that Would Impact Ef...   \n",
       "\n",
       "                                             terseLabel  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "1027                                  Long-lived assets   \n",
       "1028                                  Long-lived assets   \n",
       "1029                                  Long-lived assets   \n",
       "1030                    Gross unrecognized tax benefits   \n",
       "1031  Gross unrecognized tax benefits that would imp...   \n",
       "\n",
       "                                          documentation  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "1027  Long-lived assets other than financial instrum...   \n",
       "1028  Long-lived assets other than financial instrum...   \n",
       "1029  Long-lived assets other than financial instrum...   \n",
       "1030               Amount of unrecognized tax benefits.   \n",
       "1031  The total amount of unrecognized tax benefits ...   \n",
       "\n",
       "                                           metalinksURL  \n",
       "0                                                   NaN  \n",
       "1                                                   NaN  \n",
       "2                                                   NaN  \n",
       "3                                                   NaN  \n",
       "4                                                   NaN  \n",
       "...                                                 ...  \n",
       "1027  https://www.sec.gov/Archives/edgar/data/000032...  \n",
       "1028  https://www.sec.gov/Archives/edgar/data/000032...  \n",
       "1029  https://www.sec.gov/Archives/edgar/data/000032...  \n",
       "1030  https://www.sec.gov/Archives/edgar/data/000032...  \n",
       "1031  https://www.sec.gov/Archives/edgar/data/000032...  \n",
       "\n",
       "[1032 rows x 19 columns]"
      ]
     },
     "execution_count": 699,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facts_w_context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
