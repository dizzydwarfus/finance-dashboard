{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from sec_edgar_api import EdgarClient\n",
    "# from sec_edgar_downloader import Downloader\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import glob\n",
    "import logging\n",
    "from typing import Literal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download sec-edgar files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgar = EdgarClient(user_agent=\"<Sample Company Name> <Admin Contact>@<Sample Company Domain>\")\n",
    "edgar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgar.get_submissions(cik=\"320193\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgar.get_company_facts(cik=\"789019\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use sec-edgar-downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = Downloader()\n",
    "\n",
    "dl.get(\"10-Q\", \"0000789019\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse and Extract from .HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <table>, <tr>, <td>, and <th>: These tags are used to create tables. \n",
    "# <table> is for the table itself,\n",
    "# <tr> defines a row,\n",
    "# <td> defines a cell, and \n",
    "# <th> defines a header cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all html files in the directory\n",
    "html_files = glob.glob(r\"D:\\lianz\\Desktop\\Python\\personal_projects\\sec_data\\sec-edgar-filings\\0000320193\\*\\*\\*.html\")\n",
    "html_files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read HTML File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_html(file: str):\n",
    "    \"\"\"Reads html file and returns BeautifulSoup object\"\"\"\n",
    "    with open(f'{file}', 'r') as f:\n",
    "        contents = f.read()\n",
    "    soup = BeautifulSoup(contents, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find target table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_target_table(target_text, soup, search_type=Literal['loose','strict']):\n",
    "    \"\"\"\n",
    "    target_text: regex pattern\n",
    "    \n",
    "    soup: BeautifulSoup object\n",
    "    \n",
    "    search_type: 'loose' or 'strict'\n",
    "        - loose: search for target_text in the soup\n",
    "        - strict: search for target_text in the soup, then find the next table following the text\n",
    "\n",
    "    Returns a list of BeautifulSoup objects containing the target table(s) in html format.\n",
    "    \"\"\"\n",
    "    target_tables = None\n",
    "    \n",
    "    target_element = soup.find_all('div') # find_all returns a list\n",
    "    if search_type == 'loose':\n",
    "        target_tables = [i for i in target_element if target_text.search(i.text.lower())]\n",
    "    elif search_type == 'strict':\n",
    "        target_tables = [j.find_next('table') for j in [i for i in target_element if target_text.search(i.text)]]\n",
    "    \n",
    "    return target_tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Table from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_from_html(table_html):\n",
    "    \"\"\"\n",
    "    table_html: BeautifulSoup object\n",
    "    \n",
    "    Returns a dataframe of the table.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = [] # row x col [[1,2,3,4,5], [1,2,3,4,5], [1,2,3,4,5]] = 3 rows, 5 columns\n",
    "    # Loop over each row\n",
    "    for row in table_html.find_all('tr')[:]: #\n",
    "        # print(f'row: {row}')\n",
    "        cols = []\n",
    "        # Loop over each cell in the row\n",
    "        for cell in row.find_all(['td', 'th'])[:]:\n",
    "            # print(cell)\n",
    "            # Get the colspan and rowspan attributes, defaulting to 1 if they don't exist\n",
    "            colspan = int(cell.get('colspan', 1))\n",
    "            rowspan = int(cell.get('rowspan', 1))\n",
    "\n",
    "            # print(f'cell: {cell.text.strip()}\\ncolspan: {colspan}\\nrowspan: {rowspan}')\n",
    "            # If the cell spans multiple rows or columns, add copies of it to the cols list\n",
    "            for i in range(rowspan):\n",
    "                for j in range(colspan):\n",
    "                    cols.append(cell.text.strip())\n",
    "\n",
    "\n",
    "        # Add the cols list to the data list\n",
    "        data.append(cols)\n",
    "        # print(data)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.replace('', np.nan, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    \"\"\"\n",
    "    df: dataframe\n",
    "    \n",
    "    Process the dataframe to remove unnecessary rows and columns.\n",
    "    \n",
    "    Steps:\n",
    "    1. Find header row and use it to set column names\n",
    "    2. Remove non-header rows and columns where all values are NaN\n",
    "    3. Replace empty strings with NaN\n",
    "    4. Forward fill NaN values along columns\n",
    "    5. Remove rows where all values are NaN\n",
    "    6. Reset index after dropping rows\n",
    "    7. if column contains '$' then remove it\n",
    "    8. if row only contains same element then store the index and value in a dictionary\n",
    "    9. Use the dictionary to assign the value as first level of multiindex for the rows in between the rows to keep\n",
    "    10. replace any cell that contains : with empty string\n",
    "    11. Combine two columns to create a multiindex for the rows\n",
    "\n",
    "    Returns a cleaned dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    first_row = df.iloc[:, 0].notnull().idxmax()\n",
    "    # Use first three rows for the header\n",
    "    header = df.iloc[0:first_row].fillna('').agg(' '.join).str.strip()\n",
    "    df.columns = header\n",
    "\n",
    "    # Remove the rows used for header and reset index\n",
    "    df = df.iloc[first_row:].reset_index(drop=True)\n",
    "\n",
    "    # Remove columns where all values are NaN\n",
    "    df = df.dropna(how='all', axis=1)\n",
    "\n",
    "    # Replace empty strings with NaN\n",
    "    df.replace(\"\", np.nan, inplace=True)\n",
    "\n",
    "    # Forward fill NaN values along columns\n",
    "    df.fillna(method='ffill', axis=1, inplace=True)\n",
    "\n",
    "    # Remove rows where all values are NaN\n",
    "    df = df.dropna(how='all', axis=0)\n",
    "\n",
    "    # Reset index after dropping rows\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # if column contains '$' then remove it\n",
    "    col_to_keep = [i for i,x in enumerate(df.columns) if '$' not in df.iloc[:, i].values and '%' not in df.iloc[:, i].values and 'change' not in x.lower()]\n",
    "    df = df.iloc[:, col_to_keep]\n",
    "    df = df.loc[:, ~df.columns.duplicated(keep='first')]\n",
    "\n",
    "    # if row only contains same element then store the index and value in a dictionary\n",
    "    row_to_keep = {}\n",
    "    for i in range(len(df)):\n",
    "        if len(set(df.iloc[i, :])) == 1:\n",
    "            row_to_keep[i] = df.iloc[i, 0]\n",
    "\n",
    "    # Use the dictionary to assign the value as first level of multiindex for the rows in between the rows to keep\n",
    "    df['Category'] = df.iloc[:, 0]\n",
    "    for i in range(len(df)):\n",
    "        if i in row_to_keep.keys():\n",
    "            df.iloc[i, -1] = row_to_keep[i]\n",
    "        else:\n",
    "            df.iloc[i, -1] = df.iloc[i-1, -1]\n",
    "\n",
    "\n",
    "    row_to_keep = []\n",
    "    for i in range(len(df)):\n",
    "        if len(set(df.iloc[i, :])) != 1:\n",
    "            row_to_keep.append(i)\n",
    "\n",
    "    df = df.iloc[row_to_keep, :]\n",
    "    multiindex = df.iloc[:,[-1,0]]\n",
    "\n",
    "    # replace any cell that contains : with empty string\n",
    "    df = df.replace(to_replace=':', value='', regex=True)\n",
    "\n",
    "    # Combine two columns to create a multiindex for the rows\n",
    "    df.index = pd.MultiIndex.from_arrays([df.iloc[:,-1].values, df.iloc[:,0].values])\n",
    "    df = df.iloc[:, 1:-1]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the logging settings\n",
    "logging.basicConfig(level=logging.DEBUG, filename='loop_logs.log', filemode='w', format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_read = html_files[6]\n",
    "file_name = file_to_read.split(\"\\\\\")[-2]\n",
    "print(f'Reading html files... {file_name}')\n",
    "html_soup = read_html(file_to_read)\n",
    "target_table = find_target_table(re.compile(r'.*(Products and services|Net sales|Sales Data|net sales by operating segment).*'), html_soup, search_type='strict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = extract_table_from_html(target_table[4])\n",
    "df = clean_df(df)\n",
    "df = df.stack(level=0).reset_index(level=2).reset_index()\n",
    "df.columns = ['Category', 'Segment', 'Date_info', 'Value']\n",
    "df.to_dict('records')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.DataFrame()\n",
    "number_of_files = len(html_files)\n",
    "for file in html_files[:]:\n",
    "    logging.info(file)\n",
    "    file_name = file.split(\"\\\\\")[-2]\n",
    "    \n",
    "    try:\n",
    "        html_soup = read_html(file)\n",
    "        logging.info(f'Step 1: {file_name}: Read html successfully.')\n",
    "    \n",
    "        target_table = find_target_table(re.compile(r'.*(Sales Data|net sales by operating segment|net sales|products and services).*'), html_soup, search_type='strict')\n",
    "        logging.info(f'Step 2: {file_name}: Found target table successfully.')\n",
    "\n",
    "        assert target_table is not None, f'{file_name}: Target table not found.'\n",
    "        df = extract_table_from_html(target_table).iloc[:, :]\n",
    "        logging.info(f'Step 3: {file_name}: Extracted table successfully.')\n",
    "\n",
    "        df = clean_df(df)\n",
    "        logging.info(f'Step 4: {file_name}: Cleaned table successfully.')\n",
    "\n",
    "        df = df.stack(level=0).reset_index(level=2).reset_index()\n",
    "        logging.info(f'Step 5: {file_name}: Stacked table successfully.')\n",
    "    \n",
    "        df.columns = ['Category', 'Segment', 'Date_info', 'Value']\n",
    "        logging.info(f'Step 6: {file_name}: Renamed columns successfully.')\n",
    "    \n",
    "        all_df = pd.concat([all_df, df], axis=0, ignore_index=True)\n",
    "        logging.info(f'Step 7: {file_name}: Concatenated table successfully.')\n",
    "    except Exception as e:\n",
    "        logging.info(f'Step 7: {file_name}: Concatenated table unsuccessfully.')\n",
    "        logging.error(e)\n",
    "    \n",
    "    number_of_files -= 1\n",
    "    logging.info(f'Number of files left: {number_of_files}')\n",
    "    break\n",
    "all_df['Date'] = all_df['Date_info'].str.extract(r\"([A-Za-z]+\\s\\d{1,2},\\s\\d{4})\")\n",
    "\n",
    "# Extract three months ended text from date column\n",
    "all_df['Time Leading To'] = all_df['Date_info'].str.extract(r\"([A-Za-z]+\\s[A-Za-z]+\\s[A-Za-z]+)\")\n",
    "\n",
    "all_df.drop(columns=['Date_info'], inplace=True)\n",
    "\n",
    "# reorder columns\n",
    "all_df = all_df[['Category', 'Segment', 'Time Leading To', 'Date', 'Value']]\n",
    "all_df.to_csv('all_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_soup = read_html(html_files[6])\n",
    "target_text = re.compile(r'.*(sales data|net sales|products).*')\n",
    "target_element = html_soup.find_all('div')\n",
    "# all_text = [i for i in target_element if target_text.search(i.text.lower())]\n",
    "all_text = [j.find_next('table') for j in [i for i in target_element if target_text.search(i.text)]]\n",
    "len(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in all_text[:22]:\n",
    "    table = extract_table_from_html(i)\n",
    "    if len(table) != 0:\n",
    "        actual = table\n",
    "df = clean_df(actual)\n",
    "df = df.stack(level=0).reset_index(level=2).reset_index()\n",
    "\n",
    "df.columns = ['Category', 'Segment', 'Date_info', 'Value']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_dict(orient='records')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data_table_02 = pd.read_html(r\"sec-edgar-filings\\0000320193\\10-Q\\0000912057-02-004945\\filing-details.html\")\n",
    "sales_data_table_23 = pd.read_html(r\"sec-edgar-filings\\0000320193\\10-Q\\0000320193-23-000006\\filing-details.html\")\n",
    "sales_data_table_15 = pd.read_html(r\"sec-edgar-filings\\0000320193\\10-Q\\0001193125-15-259935\\filing-details.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_data_table_15[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sales_data_table_15[38]\n",
    "# Use first three rows for the header\n",
    "header = df.iloc[0:3].fillna('').agg(' '.join).str.strip()\n",
    "df.columns = header\n",
    "\n",
    "# Remove the rows used for header and reset index\n",
    "df = df.iloc[3:].reset_index(drop=True)\n",
    "\n",
    "# Remove columns where all values are NaN\n",
    "df = df.dropna(how='all', axis=1)\n",
    "\n",
    "# Replace empty strings with NaN\n",
    "df.replace(\"\", np.nan, inplace=True)\n",
    "\n",
    "# Forward fill NaN values along columns\n",
    "df.fillna(method='ffill', axis=1, inplace=True)\n",
    "\n",
    "# Remove rows where all values are NaN\n",
    "df = df.dropna(how='all', axis=0)\n",
    "\n",
    "# Reset index after dropping rows\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# if column contains '$' then remove it\n",
    "col_to_keep = [i for i,x in enumerate(df.columns) if '$' not in df.iloc[:, i].values]\n",
    "df = df.iloc[:, col_to_keep]\n",
    "df = df.loc[:, ~df.columns.duplicated(keep='first')]\n",
    "\n",
    "# if row only contains same element then store the index and value in a dictionary\n",
    "row_to_keep = {}\n",
    "for i in range(len(df)):\n",
    "    if len(set(df.iloc[i, :])) == 1:\n",
    "        row_to_keep[i] = df.iloc[i, 0]\n",
    "\n",
    "# Use the dictionary to assign the value as first level of multiindex for the rows in between the rows to keep\n",
    "df['Category'] = df.iloc[:, 0]\n",
    "for i in range(len(df)):\n",
    "    if i in row_to_keep.keys():\n",
    "        df.iloc[i, -1] = row_to_keep[i]\n",
    "    else:\n",
    "        df.iloc[i, -1] = df.iloc[i-1, -1]\n",
    "\n",
    "\n",
    "row_to_keep = []\n",
    "for i in range(len(df)):\n",
    "    if len(set(df.iloc[i, :])) != 1:\n",
    "        row_to_keep.append(i)\n",
    "\n",
    "df = df.iloc[row_to_keep, :]\n",
    "multiindex = df.iloc[:,[-1,0]]\n",
    "\n",
    "# replace any cell that contains : with empty string\n",
    "df = df.replace(to_replace=':', value='', regex=True)\n",
    "\n",
    "# Combine two columns to create a multiindex for the rows\n",
    "df.index = pd.MultiIndex.from_arrays([df.iloc[:,-1].values, df.iloc[:,0].values])\n",
    "df = df.iloc[:, 1:-1]\n",
    "\n",
    "# unpivot the dataframe based on the third column\n",
    "df = df.stack(level=0).reset_index(level=2).reset_index()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find a way to convert data in the tables to a key-value pair in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert df to dictionary and label with AAPL\n",
    "df = df.to_dict(orient='index')\n",
    "df = {'AAPL': df}\n",
    "df # shares in thousands, eps in per share, and others in millions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and send summary as mail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: create script .py file to download latest earnings and send summary to email\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test SEC API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "import logging\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import trange\n",
    "\n",
    "# Configure the logging settings\n",
    "logging.basicConfig(level=logging.DEBUG, filename='sec_logs.log', filemode='a', format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class SECData:\n",
    "    \"\"\"Class to retrieve data from SEC Edgar database.\n",
    "\n",
    "    Args:\n",
    "        requester_name (str): Name of the requester\n",
    "        requester_email (str): Email of the requester\n",
    "        taxonomy (str): us-gaap, ifrs-full, dei, or srt\n",
    "\n",
    "    Raises:\n",
    "        Exception: If taxonomy is not one of the following: us-gaap, ifrs-full, dei, or srt\n",
    "    \n",
    "    Attributes:\n",
    "        BASE_API_URL (str): Base url for SEC Edgar database\n",
    "        US_GAAP_TAXONOMY_URL (str): URL for us-gaap taxonomy\n",
    "        ALLOWED_TAXONOMIES (list): List of allowed taxonomies\n",
    "        headers (dict): Headers to be used for API calls\n",
    "        cik (DataFrame): DataFrame containing CIK and ticker\n",
    "        tags (list): List of tags in us-gaap taxonomy\n",
    "        taxonomy (str): us-gaap, ifrs-full, dei, or srt\n",
    "\n",
    "    Methods:\n",
    "        get_cik_list: Retrieves the full list of CIK available from SEC database.\n",
    "        get_ticker_cik: Get a specific ticker's CIK number. \n",
    "        get_usgaap_tags: Get the list of tags in us-gaap taxonomy.\n",
    "        get_submissions: Retrieves the list of submissions for a specific CIK.\n",
    "        get_company_concept: Retrieves the XBRL disclosures from a single company (CIK) \n",
    "            and concept (a taxonomy and tag) into a single JSON file.\n",
    "        get_company_facts: Retrieves the XBRL disclosures from a single company (CIK) \n",
    "            into a single JSON file.\n",
    "        get_frames: Retrieves one fact for each reporting entity that is last filed that most closely fits the calendrical period requested.\n",
    "    \"\"\"\n",
    "    \n",
    "    BASE_API_URL = \"https://data.sec.gov/\"\n",
    "    BASE_SEC_URL = \"https://www.sec.gov/\"\n",
    "    BASE_DIRECTORY_URL = \"https://www.sec.gov/Archives/edgar/data/\"\n",
    "    US_GAAP_TAXONOMY_URL = \"https://xbrl.fasb.org/us-gaap/2023/elts/us-gaap-2023.xsd\"\n",
    "    ALLOWED_TAXONOMIES = ['us-gaap', 'ifrs-full', 'dei', 'srt']\n",
    "    INDEX_EXTENSION = ['-index.html', '-index-headers.html']\n",
    "    FILE_EXTENSIONS = ['.xsd', '.htm', '_cal.xml', '_def.xml', '_lab.xml', '_pre.xml', '_htm.xml', '.xml']\n",
    "\n",
    "    def __init__(self, requester_company: str,requester_name: str, requester_email: str, taxonomy: str):\n",
    "        self.requester_company = requester_company\n",
    "        self.requester_name = requester_name\n",
    "        self.requester_email = requester_email\n",
    "        self.sec_headers = {\"User-Agent\": f\"{requester_company} {requester_name} {requester_email}\",\n",
    "                        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "                        \"Host\": \"www.sec.gov\"}\n",
    "        self.sec_data_headers = {\"User-Agent\": f\"{requester_company} {requester_name} {requester_email}\",\n",
    "                        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "                        \"Host\": \"data.sec.gov\"}\n",
    "        self.cik = self.get_cik_list()\n",
    "        self.tags = self.get_usgaap_tags()\n",
    "        if taxonomy not in self.ALLOWED_TAXONOMIES:\n",
    "            raise ValueError(\n",
    "                f\"Taxonomy {taxonomy} is not supported. Please use one of the following taxonomies: {self.ALLOWED_TAXONOMIES}\")\n",
    "        self.taxonomy = taxonomy\n",
    "\n",
    "\n",
    "    @sleep_and_retry\n",
    "    @limits(calls=10, period=1)\n",
    "    def rate_limited_request(self, url: str, headers: dict):\n",
    "        \"\"\"Rate limited request to SEC Edgar database.\n",
    "\n",
    "        Args:\n",
    "            url (str): URL to retrieve data from\n",
    "            headers (dict): Headers to be used for API calls\n",
    "\n",
    "        Returns:\n",
    "            response: Response from API call\n",
    "        \"\"\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            logging.error(f'''\n",
    "Request failed at URL: {url}''')\n",
    "        return response\n",
    "    \n",
    "\n",
    "    def get_cik_list(self):\n",
    "        \"\"\"Retrieves the full list of CIK available from SEC database.\n",
    "\n",
    "        Raises:\n",
    "            Exception: On failure to retrieve CIK list\n",
    "\n",
    "        Returns:\n",
    "            cik_df: DataFrame containing CIK and ticker\n",
    "        \"\"\"\n",
    "        url = r\"https://www.sec.gov/files/company_tickers.json\"\n",
    "        cik_raw = self.rate_limited_request(url, self.sec_headers)\n",
    "        cik_json = cik_raw.json()\n",
    "        cik_df = pd.DataFrame.from_dict(cik_json).T\n",
    "        return cik_df\n",
    "\n",
    "\n",
    "    def get_ticker_cik(self, ticker: str,):\n",
    "        \"\"\"Get a specific ticker's CIK number. \n",
    "        CIK########## is the entity's 10-digit Central Index Key (CIK).\n",
    "\n",
    "        Args:\n",
    "            ticker (str): public ticker symbol of the company\n",
    "\n",
    "        Returns:\n",
    "            cik: CIK number of the company excluding the leading 'CIK'\n",
    "        \"\"\"\n",
    "        ticker_cik = self.cik.query(f\"ticker == '{ticker}'\")['cik_str']\n",
    "        cik = f\"{ticker_cik.iloc[0]:010d}\"\n",
    "        return cik\n",
    "\n",
    "\n",
    "    def get_usgaap_tags(self, xsd_url: str = US_GAAP_TAXONOMY_URL):\n",
    "        \"\"\"Get the list of tags (elements) in us-gaap taxonomy or provide a different xsd_url to get tags from a different taxonomy.\n",
    "\n",
    "        Returns:\n",
    "            list of tags\n",
    "        \"\"\"        \n",
    "        response = self.rate_limited_request(xsd_url, headers=self.sec_headers)\n",
    "        xsd_content = response.text\n",
    "        root = ET.fromstring(xsd_content)\n",
    "\n",
    "        return [element.attrib['name'] for element in root.findall(\".//{http://www.w3.org/2001/XMLSchema}element\")]\n",
    "\n",
    "\n",
    "    def get_submissions(self, cik):\n",
    "        url = f\"{self.BASE_API_URL}submissions/CIK{cik}.json\"\n",
    "        response = self.rate_limited_request(url, headers=self.sec_data_headers)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\n",
    "                f\"Failed to retrieve submissions for CIK {cik}. Status code: {response.status_code}\")\n",
    "        data = json.loads(response.text)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def get_company_concept(self, cik: str, tag: str, taxonomy: str = 'us-gaap',):\n",
    "        \"\"\"The company-concept API returns all the XBRL disclosures from a single company (CIK) \n",
    "        and concept (a taxonomy and tag) into a single JSON file, with a separate array of facts \n",
    "        for each units on measure that the company has chosen to disclose \n",
    "        (e.g. net profits reported in U.S. dollars and in Canadian dollars).\n",
    "\n",
    "        Args:\n",
    "            cik (str): CIK number of the company. Get the list using self.cik\n",
    "            taxonomy (str): us-gaap, ifrs-full, dei, or srt\n",
    "            tag (str): taxonomy tag (e.g. Revenue, AccountsPayableCurrent). See full list from https://xbrl.fasb.org/us-gaap/2023/elts/us-gaap-2023.xsd\n",
    "\n",
    "        Raises:\n",
    "            Exception: On failure to retrieve company concept either due to invalid CIK, taxonomy, or tag\n",
    "\n",
    "        Returns:\n",
    "            data: JSON file containing all the XBRL disclosures from a single company (CIK)\n",
    "        \"\"\"\n",
    "        url = f\"{self.BASE_API_URL}api/xbrl/companyconcept/CIK{cik}/{taxonomy}/{tag}.json\"\n",
    "        response = self.rate_limited_request(url, headers=self.sec_data_headers)\n",
    "        data = json.loads(response.text)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def get_company_facts(self, cik):\n",
    "        url = f\"{self.BASE_API_URL}api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "        response = self.rate_limited_request(url, headers=self.sec_data_headers)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\n",
    "                f\"Failed to retrieve company facts for CIK {cik}. Status code: {response.status_code}\")\n",
    "        data = json.loads(response.text)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def get_frames(self, taxonomy, tag, unit, period):\n",
    "        \"\"\"The xbrl/frames API aggregates one fact for each reporting entity that is last filed that most closely fits the calendrical period requested. \n",
    "        This API supports for annual, quarterly and instantaneous data: https://data.sec.gov/api/xbrl/frames/us-gaap/AccountsPayableCurrent/USD/CY2019Q1I.json\n",
    "\n",
    "        Args:\n",
    "            taxonomy (str): us-gaap, ifrs-full, dei, or srt\n",
    "            tag (str): taxonomy tag (e.g. Revenue, AccountsPayableCurrent). See full list from https://xbrl.fasb.org/us-gaap/2023/elts/us-gaap-2023.xsd\n",
    "            unit (str): USD, USD-per-shares, etc.\n",
    "            period (str): CY#### for annual data (duration 365 days +/- 30 days), CY####Q# for quarterly data (duration 91 days +/- 30 days), CY####Q#I for instantaneous data\n",
    "\n",
    "        Raises:\n",
    "            Exception: (placeholder)\n",
    "\n",
    "        Returns:\n",
    "            data: json formatted response\n",
    "        \"\"\"        \n",
    "        url = f\"{self.BASE_API_URL}api/xbrl/frames/{taxonomy}/{tag}/{unit}/{period}.json\"\n",
    "        response = self.rate_limited_request(url, headers=self.sec_data_headers)\n",
    "        data = json.loads(response.text)\n",
    "        return data\n",
    "    \n",
    "    def get_data_as_dataframe(self, cik: str,):\n",
    "        \"\"\"Retrieves the XBRL disclosures from a single company (CIK) and returns it as a pandas dataframe.\n",
    "\n",
    "        Args:\n",
    "            cik (str): CIK number of the company. Get the list using self.cik\n",
    "\n",
    "        Returns:\n",
    "            df: pandas dataframe containing the XBRL disclosures from a single company (CIK)\n",
    "        \"\"\"\n",
    "        data = self.get_company_facts(cik)\n",
    "        \n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for tag in data['facts'][self.taxonomy]:\n",
    "            facts = data['facts']['us-gaap'][tag]['units']\n",
    "            unit_key = list(facts.keys())[0]\n",
    "            temp_df = pd.DataFrame(facts[unit_key])\n",
    "            temp_df['label'] = tag\n",
    "            df = pd.concat([df, temp_df], axis=0, ignore_index=True)\n",
    "        df = df.astype({'val': 'float64', \n",
    "                        'end': 'datetime64[ns]',\n",
    "                        'start': 'datetime64[ns]',\n",
    "                        'filed': 'datetime64[ns]'})\n",
    "        df['Months Ended'] = (df['end'] - df['start']).dt.days.div(30.4375).round(0)\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def get_index(self, cik: str = None,) -> dict:\n",
    "        \"\"\"Each CIK directory and all child subdirectories contain three files to assist in \n",
    "        automated crawling of these directories. \n",
    "        These are not visible through directory browsing.\n",
    "            - index.html (the web browser would normally receive these)\n",
    "            - index.xml (a XML structured version of the same content)\n",
    "            - index.json (a JSON structured vision of the same content)\n",
    "\n",
    "        Args:\n",
    "            cik (str): CIK number of the company. Get the list using self.cik\n",
    "\n",
    "        Returns:\n",
    "            json: pandas dataframe containing the XBRL disclosures from a single company (CIK)\n",
    "        \"\"\"\n",
    "        if cik is not None:\n",
    "            url = self.BASE_DIRECTORY_URL + cik + '/' + 'index.json'\n",
    "        \n",
    "        else:\n",
    "            url = self.BASE_DIRECTORY_URL + self.cik + '/' + 'index.json'\n",
    "\n",
    "        response = self.rate_limited_request(url, headers=self.sec_headers)\n",
    "        return response.json()\n",
    "\n",
    "class TickerData(SECData):\n",
    "    \"\"\"Inherited from SECData class. Retrieves data from SEC Edgar database based on ticker.\n",
    "    url is constructed based on the following: https://www.sec.gov/Archives/edgar/data/{cik}/{ascension_number}/{file_name}\n",
    "    cik is the CIK number of the company = access via get_ticker_cik\n",
    "    ascension_number is the accessionNumber column of filings_df\n",
    "    file name for xml is always '{ticker}-{reportDate}.{extension}\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ticker:str, requester_company: str = 'Financial API', requester_name: str = 'API Caller', requester_email: str = 'apicaller@gmail.com', taxonomy: str ='us-gaap',):\n",
    "        super().__init__(requester_company, requester_name, requester_email, taxonomy)\n",
    "        self.ticker = ticker.upper()\n",
    "        self.cik = self.get_ticker_cik(self.ticker)\n",
    "        self._submissions = None\n",
    "        self._filings = None\n",
    "        self.forms = self.filings['form'].unique()\n",
    "        self._index = self.get_index(self.cik)\n",
    "        self._filing_folder_urls = None\n",
    "        self._filing_urls = None\n",
    "\n",
    "    @property\n",
    "    def submissions(self,) -> dict:\n",
    "        if self._submissions is None:\n",
    "            self._submissions = self.get_submissions(self.cik)\n",
    "        return self._submissions\n",
    "\n",
    "    @property\n",
    "    def filings(self,) -> pd.DataFrame:\n",
    "        if self._filings is None:\n",
    "            self._filings = self.get_filings(self.submissions)\n",
    "        return self._filings\n",
    "\n",
    "    @property\n",
    "    def filing_folder_urls(self,) -> list:\n",
    "        if self._filing_folder_urls is None:\n",
    "            self._filing_folder_urls = self.get_filing_folder_urls()\n",
    "        return self._filing_folder_urls\n",
    "\n",
    "    @property\n",
    "    def filing_urls(self,) -> list:\n",
    "        if self._filing_urls is None:\n",
    "            self._filing_urls = self.get_filing_urls()\n",
    "            \n",
    "        return self._filing_urls\n",
    "\n",
    "\n",
    "    def get_filing_folder_urls(self,) -> list:\n",
    "        \"\"\"Get filing folder urls from index dict.\n",
    "\n",
    "        Args:\n",
    "            index (dict): index dict from get_index method\n",
    "\n",
    "        Returns:\n",
    "            filing_folder_urls (list): list of filing folder urls\n",
    "        \"\"\"\n",
    "        filing_folder_urls = [self.BASE_SEC_URL + self._index['directory']['name'] + '/' + folder['name'] for folder in self._index['directory']['item'] if folder['type'] == 'folder.gif']\n",
    "        return filing_folder_urls\n",
    "\n",
    "\n",
    "    def get_filing_urls(self,) -> list:\n",
    "        \"\"\"Get filing urls from filing folder urls.\n",
    "\n",
    "        Args:\n",
    "            filing_folder_urls (list): list of filing folder urls\n",
    "\n",
    "        Returns:\n",
    "            filing_urls (list): list of filing urls to .txt files\n",
    "        \"\"\"\n",
    "        filing_urls = []\n",
    "        with trange(len(self.filing_folder_urls), desc=f'Instantiating filing urls for {self.ticker}...') as t:\n",
    "            for i in t:\n",
    "                logging.info(t)\n",
    "                try:\n",
    "                    soup = self.get_file_data(self.filing_folder_urls[i])\n",
    "                    for link in soup.find_all('a'):\n",
    "                        if link.get('href').endswith('.txt'):\n",
    "                            filing_urls.append(self.BASE_SEC_URL + link.get('href'))\n",
    "                except Exception as e:\n",
    "                    logging.error(f'Failed to instantiate filing urls for {self.ticker}...')\n",
    "                    logging.error(e)\n",
    "                    t.write(f'Failed to instantiate filing urls for {self.ticker}...')\n",
    "                    continue\n",
    "        return filing_urls\n",
    "    \n",
    "\n",
    "    def get_filings(self, submissions: dict):\n",
    "        \"\"\"Get filings from submissions dict.\n",
    "\n",
    "        Args:\n",
    "            submissions (dict): submissions dict from get_submissions method\n",
    "        \n",
    "        Returns:\n",
    "            filings (DataFrame): DataFrame containing filings\n",
    "        \"\"\"\n",
    "        filings = pd.DataFrame(submissions['filings']['recent'])\n",
    "\n",
    "        # Convert reportDate, filingDate, acceptanceDateTime columns to datetime\n",
    "        filings['reportDate'] = pd.to_datetime(filings['reportDate'])\n",
    "        filings['filingDate'] = pd.to_datetime(filings['filingDate'])\n",
    "        filings['acceptanceDateTime'] = pd.to_datetime(\n",
    "            filings['acceptanceDateTime'])\n",
    "        filings['file_url'] = self.BASE_DIRECTORY_URL + self.cik + '/' + filings['accessionNumber'].str.replace('-', '') + '/' + filings['accessionNumber'] + '.txt'\n",
    "        return filings\n",
    "\n",
    "\n",
    "    def get_file_data(self, file_url: str) -> BeautifulSoup:\n",
    "        \"\"\"Get file data from file url which can be retrieved by calling self.get_file_url method.\n",
    "\n",
    "        Args:\n",
    "            file_url (str): File url to retrieve data from on the SEC website\n",
    "\n",
    "        Returns:\n",
    "            data: File data as a BeautifulSoup object\n",
    "        \"\"\"\n",
    "        data = self.rate_limited_request(url=file_url, headers=self.sec_headers)\n",
    "        soup = BeautifulSoup(data.content, \"lxml\")\n",
    "        return soup\n",
    "    \n",
    "    def search_tags(self, soup: BeautifulSoup, pattern: str) -> BeautifulSoup:\n",
    "        \"\"\"Search for tags in BeautifulSoup object.\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): BeautifulSoup object\n",
    "            pattern (str): pattern to search for\n",
    "\n",
    "        Returns:\n",
    "            soup: BeautifulSoup object\n",
    "        \"\"\"\n",
    "        return soup.find_all(pattern)\n",
    "    \n",
    "    def search_context(self, soup: BeautifulSoup) -> pd.DataFrame:\n",
    "        \"\"\"Search for context in company .txt filing. \n",
    "        Context provides information about the entity, segment, and time period for facts in the filing.\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): BeautifulSoup object\n",
    "\n",
    "        Returns:\n",
    "            df: DataFrame containing context information with columns \n",
    "            {\n",
    "                'contextId': str,\n",
    "                'entity': str,\n",
    "                'segment': str,\n",
    "                'startDate': 'datetime64[ns]',\n",
    "                'endDate': 'datetime64[ns]',\n",
    "                'instant': 'datetime64[ns]'\n",
    "            }\n",
    "        \"\"\"\n",
    "        contexts = self.search_tags(soup, '^context$')\n",
    "        dict_list = []\n",
    "        columns = {'contextId': str, 'entity': str, 'segment': str, 'startDate': 'datetime64[ns]', 'endDate': 'datetime64[ns]', 'instant': 'datetime64[ns]'}\n",
    "        for tag in contexts:\n",
    "            temp_dict = {}\n",
    "            temp_dict['contextId'] = tag.attrs['id']\n",
    "            temp_dict['entity'] = tag.find(\"entity\").text.split()[0] if tag.find(\"entity\") is not None else None\n",
    "            temp_dict['segment'] = tag.find(\"segment\").text.strip() if tag.find(\"segment\") is not None else None\n",
    "            temp_dict['startDate'] = tag.find(\"startdate\").text if tag.find(\"startdate\") is not None else None\n",
    "            temp_dict['endDate'] = tag.find(\"enddate\").text if tag.find(\"enddate\") is not None else None\n",
    "            temp_dict['instant'] = tag.find(\"instant\").text if tag.find(\"instant\") is not None else None\n",
    "            dict_list.append(temp_dict)\n",
    "\n",
    "        df = pd.DataFrame(dict_list, columns=columns.keys()).astype(columns)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get facts as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cik  = sec.get_ticker_cik('MSFT')\n",
    "companyfacts_df = sec.get_data_as_dataframe(cik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get XSD tags for quarterly filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl = TickerData(requester_company='Financial Docs', requester_name='John Doe', requester_email='financial@gmail.com', taxonomy='us-gaap', ticker='aapl')\n",
    "msft = TickerData(requester_company='Financial Docs', requester_name='John Doe', requester_email='financial@gmail.com', taxonomy='us-gaap', ticker='MSFT')\n",
    "nvda = TickerData(requester_company='Financial Docs', requester_name='John Doe', requester_email='financial@gmail.com', taxonomy='us-gaap', ticker='NVDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with trange(len(aapl.filing_urls),) as t:\n",
    "    for i in t:\n",
    "        t.set_description(f'Iteration {i}')\n",
    "        try:\n",
    "            soup = aapl.get_file_data(aapl.filing_urls[i])\n",
    "            form_type = soup.find(re.compile('type')).text.split()[0] if soup.find(re.compile('type')) is not None else None\n",
    "            t.set_postfix(form_type=form_type, request_url=aapl.filing_urls[i])\n",
    "            logging.info(t)\n",
    "        except Exception as e:\n",
    "            print(f'Error processing filing URL {aapl.filing_urls[i]}: {e}')\n",
    "            logging.error(f'Error processing filing URL {aapl.filing_urls[i]}: {e}')\n",
    "            t.write(f'Error processing filing URL {aapl.filing_urls[i]}: {e}')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find reporting time frame of reportDate\n",
    "soup = msft.get_file_data(msft.filings.iloc[0]['file_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msft_us_gaap = msft.tag_search(soup, 'us-gaap')\n",
    "for i,x in enumerate(msft_us_gaap):\n",
    "    if x['tag name'].find('revenue') != -1 and len(x['attributes']) > 0:\n",
    "        print(x['tag name'])\n",
    "        print(x['attributes'])\n",
    "        print(x['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Link Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msft_label_links = msft.tag_search(soup, '^link:label$')\n",
    "\n",
    "[i for i in msft_label_links if i['attributes']['xlink:label'].lower().find('us-gaap:revenuefromcontractwithcustomerexcludingassessedtax'.replace(':','_')) != -1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataScience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
