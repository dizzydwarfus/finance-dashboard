{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test SEC API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class MyLogger:\n",
    "    def __init__(self, name: str = __name__, level: str = 'debug', log_file: str = 'logs.log'):\n",
    "        # Initialize logger\n",
    "        self.logging_level = logging.DEBUG if level == 'debug' else logging.INFO\n",
    "        self.scrape_logger = logging.getLogger(name)\n",
    "        self.scrape_logger.setLevel(self.logging_level)\n",
    "\n",
    "        # Check if the self.scrape_logger already has handlers to avoid duplicate logging.\n",
    "        if not self.scrape_logger.hasHandlers():\n",
    "            # Create a file handler\n",
    "            file_handler = logging.FileHandler(log_file, mode='a')\n",
    "            file_handler.setLevel(self.logging_level)\n",
    "\n",
    "            # Create a stream handler\n",
    "            stream_handler = logging.StreamHandler()\n",
    "            stream_handler.setLevel(self.logging_level)\n",
    "\n",
    "            # Create a logging format\n",
    "            formatter = logging.Formatter(\n",
    "                '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            file_handler.setFormatter(formatter)\n",
    "            stream_handler.setFormatter(formatter)\n",
    "\n",
    "            # Add the handlers to the self.scrape_logger\n",
    "            self.scrape_logger.addHandler(file_handler)\n",
    "            self.scrape_logger.addHandler(stream_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MyLogger\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "from utils._logger import MyLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import trange\n",
    "import re\n",
    "\n",
    "\n",
    "def convert_keys_to_lowercase(d):\n",
    "    \"\"\"Recursively convert all keys in a dictionary to lowercase.\n",
    "\n",
    "    Args:\n",
    "        d (dict): Dictionary to convert\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with all keys converted to lowercase\n",
    "    \"\"\"\n",
    "    new_dict = {}\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            v = convert_keys_to_lowercase(v)\n",
    "        new_key = re.sub(r'[^a-zA-Z0-9]', '', k.lower())\n",
    "        new_dict[new_key] = v\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "def indexify_url(folder_url: str) -> str:\n",
    "    \"\"\"Converts url to index url.\n",
    "\n",
    "    Args:\n",
    "        url (str): url to convert to index url\n",
    "\n",
    "    Returns:\n",
    "        str: index url\n",
    "    \"\"\"\n",
    "    return folder_url + '/index.json'\n",
    "\n",
    "\n",
    "class SECData(MyLogger):\n",
    "    \"\"\"Class to retrieve data from SEC Edgar database.\n",
    "\n",
    "    Args:\n",
    "        requester_name (str): Name of the requester\n",
    "        requester_email (str): Email of the requester\n",
    "        taxonomy (str): us-gaap, ifrs-full, dei, or srt\n",
    "\n",
    "    Raises:\n",
    "        Exception: If taxonomy is not one of the following: us-gaap, ifrs-full, dei, or srt\n",
    "\n",
    "    Attributes:\n",
    "        BASE_API_URL (str): Base url for SEC Edgar database\n",
    "        US_GAAP_TAXONOMY_URL (str): URL for us-gaap taxonomy\n",
    "        ALLOWED_TAXONOMIES (list): List of allowed taxonomies\n",
    "        headers (dict): Headers to be used for API calls\n",
    "        cik (DataFrame): DataFrame containing CIK and ticker\n",
    "        tags (list): List of tags in us-gaap taxonomy\n",
    "        taxonomy (str): us-gaap, ifrs-full, dei, or srt\n",
    "\n",
    "    Methods:\n",
    "        get_cik_list: Retrieves the full list of CIK available from SEC database.\n",
    "        get_ticker_cik: Get a specific ticker's CIK number. \n",
    "        get_usgaap_tags: Get the list of tags in us-gaap taxonomy.\n",
    "        get_submissions: Retrieves the list of submissions for a specific CIK.\n",
    "        get_company_concept: Retrieves the XBRL disclosures from a single company (CIK) \n",
    "            and concept (a taxonomy and tag) into a single JSON file.\n",
    "        get_company_facts: Retrieves the XBRL disclosures from a single company (CIK) \n",
    "            into a single JSON file.\n",
    "        get_frames: Retrieves one fact for each reporting entity that is last filed that most closely fits the calendrical period requested.\n",
    "    \"\"\"\n",
    "\n",
    "    BASE_API_URL = \"https://data.sec.gov/\"\n",
    "    BASE_SEC_URL = \"https://www.sec.gov/\"\n",
    "    BASE_DIRECTORY_URL = \"https://www.sec.gov/Archives/edgar/data/\"\n",
    "    SIC_LIST_URL = \"https://www.sec.gov/corpfin/division-of-corporation-finance-standard-industrial-classification-sic-code-list\"\n",
    "    US_GAAP_TAXONOMY_URL = \"https://xbrl.fasb.org/us-gaap/2023/elts/us-gaap-2023.xsd\"\n",
    "    ALLOWED_TAXONOMIES = {'us-gaap', 'ifrs-full', 'dei', 'srt'}\n",
    "    INDEX_EXTENSION = {'-index.html', '-index-headers.html'}\n",
    "    DIRECTORY_INDEX = {'index.json', 'index.xml', 'index.html'}\n",
    "    FILE_EXTENSIONS = {'.xsd', '.htm', '_cal.xml',\n",
    "                       '_def.xml', '_lab.xml', '_pre.xml', '_htm.xml', '.xml'}\n",
    "    SCRAPE_FILE_EXTENSIONS = {'_lab','_def','_pre','_cal'}\n",
    "\n",
    "    def __init__(self, requester_company: str = 'Financial API', requester_name: str = 'API Caller', requester_email: str = 'apicaller@gmail.com', taxonomy: str = 'us-gaap',):\n",
    "        super().__init__(name='sec-scraper', level='debug', log_file='././logs.log')\n",
    "\n",
    "        self.requester_company = requester_company\n",
    "        self.requester_name = requester_name\n",
    "        self.requester_email = requester_email\n",
    "        self.sec_headers = {\"User-Agent\": f\"{requester_company} {requester_name} {requester_email}\",\n",
    "                            \"Accept-Encoding\": \"gzip, deflate\",\n",
    "                            \"Host\": \"www.sec.gov\"}\n",
    "        self.sec_data_headers = {\"User-Agent\": f\"{requester_company} {requester_name} {requester_email}\",\n",
    "                                 \"Accept-Encoding\": \"gzip, deflate\",\n",
    "                                 \"Host\": \"data.sec.gov\"}\n",
    "        self._cik_list = None\n",
    "        self._tags = None\n",
    "        if taxonomy not in self.ALLOWED_TAXONOMIES:\n",
    "            raise ValueError(\n",
    "                f\"Taxonomy {taxonomy} is not supported. Please use one of the following taxonomies: {self.ALLOWED_TAXONOMIES}\")\n",
    "        self.taxonomy = taxonomy\n",
    "\n",
    "    @property\n",
    "    def cik_list(self,):\n",
    "        if self._cik_list is None:\n",
    "            self._cik_list = self.get_cik_list()\n",
    "        return self._cik_list\n",
    "\n",
    "    @property\n",
    "    def tags(self,):\n",
    "        if self._tags is None:\n",
    "            self._tags = self.get_usgaap_tags()\n",
    "        return self._tags\n",
    "\n",
    "    @sleep_and_retry\n",
    "    @limits(calls=10, period=1)\n",
    "    def rate_limited_request(self, url: str, headers: dict):\n",
    "        \"\"\"Rate limited request to SEC Edgar database.\n",
    "\n",
    "        Args:\n",
    "            url (str): URL to retrieve data from\n",
    "            headers (dict): Headers to be used for API calls\n",
    "\n",
    "        Returns:\n",
    "            response: Response from API call\n",
    "        \"\"\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            self.scrape_logger.error(f'''Request failed at URL: {url}''')\n",
    "        else:\n",
    "            self.scrape_logger.info(f'''Request successful at URL: {url}''')\n",
    "        return response\n",
    "\n",
    "    def get_cik_list(self):\n",
    "        \"\"\"Retrieves the full list of CIK available from SEC database.\n",
    "\n",
    "        Raises:\n",
    "            Exception: On failure to retrieve CIK list\n",
    "\n",
    "        Returns:\n",
    "            cik_df: DataFrame containing CIK and ticker\n",
    "        \"\"\"\n",
    "        url = r\"https://www.sec.gov/files/company_tickers.json\"\n",
    "        cik_raw = self.rate_limited_request(url, self.sec_headers)\n",
    "        cik_json = cik_raw.json()\n",
    "        cik_df = pd.DataFrame.from_dict(cik_json).T\n",
    "        return cik_df\n",
    "\n",
    "    def get_ticker_cik(self, ticker: str,):\n",
    "        \"\"\"Get a specific ticker's CIK number. \n",
    "        CIK########## is the entity's 10-digit Central Index Key (CIK).\n",
    "\n",
    "        Args:\n",
    "            ticker (str): public ticker symbol of the company\n",
    "\n",
    "        Returns:\n",
    "            cik: CIK number of the company excluding the leading 'CIK'\n",
    "        \"\"\"\n",
    "        ticker_cik = self.cik_list.query(\n",
    "            f\"ticker == '{ticker.upper()}'\")['cik_str']\n",
    "        cik = f\"{ticker_cik.iloc[0]:010d}\"\n",
    "        return cik\n",
    "\n",
    "    def get_usgaap_tags(self, xsd_url: str = US_GAAP_TAXONOMY_URL):\n",
    "        \"\"\"Get the list of tags (elements) in us-gaap taxonomy or provide a different xsd_url to get tags from a different taxonomy.\n",
    "\n",
    "        Returns:\n",
    "            list of tags\n",
    "        \"\"\"\n",
    "        response = self.rate_limited_request(xsd_url, headers=self.sec_headers)\n",
    "        xsd_content = response.text\n",
    "        root = ET.fromstring(xsd_content)\n",
    "\n",
    "        return [element.attrib['name'] for element in root.findall(\".//{http://www.w3.org/2001/XMLSchema}element\")]\n",
    "\n",
    "    def get_submissions(self, cik: str = None, submission_file: str = None) -> dict:\n",
    "        if cik is not None:\n",
    "            url = f\"{self.BASE_API_URL}submissions/CIK{cik}.json\"\n",
    "        elif submission_file is not None:\n",
    "            url = f\"{self.BASE_API_URL}submissions/{submission_file}\"\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Please provide either a CIK number or a submission file.\")\n",
    "        response = self.rate_limited_request(\n",
    "            url, headers=self.sec_data_headers)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\n",
    "                f\"Failed to retrieve submissions. Status code: {response.status_code}\")\n",
    "        data = json.loads(response.text)\n",
    "        return data\n",
    "\n",
    "    def get_company_concept(self, cik: str, tag: str, taxonomy: str = 'us-gaap',):\n",
    "        \"\"\"The company-concept API returns all the XBRL disclosures from a single company (CIK) \n",
    "        and concept (a taxonomy and tag) into a single JSON file, with a separate array of facts \n",
    "        for each units on measure that the company has chosen to disclose \n",
    "        (e.g. net profits reported in U.S. dollars and in Canadian dollars).\n",
    "\n",
    "        Args:\n",
    "            cik (str): CIK number of the company. Get the list using self.cik\n",
    "            taxonomy (str): us-gaap, ifrs-full, dei, or srt\n",
    "            tag (str): taxonomy tag (e.g. Revenue, AccountsPayableCurrent). See full list from https://xbrl.fasb.org/us-gaap/2023/elts/us-gaap-2023.xsd\n",
    "\n",
    "        Raises:\n",
    "            Exception: On failure to retrieve company concept either due to invalid CIK, taxonomy, or tag\n",
    "\n",
    "        Returns:\n",
    "            data: JSON file containing all the XBRL disclosures from a single company (CIK)\n",
    "        \"\"\"\n",
    "        url = f\"{self.BASE_API_URL}api/xbrl/companyconcept/CIK{cik}/{taxonomy}/{tag}.json\"\n",
    "        response = self.rate_limited_request(\n",
    "            url, headers=self.sec_data_headers)\n",
    "        data = json.loads(response.text)\n",
    "        return data\n",
    "\n",
    "    def get_company_facts(self, cik):\n",
    "        url = f\"{self.BASE_API_URL}api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "        response = self.rate_limited_request(\n",
    "            url, headers=self.sec_data_headers)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\n",
    "                f\"Failed to retrieve company facts for CIK {cik}. Status code: {response.status_code}\")\n",
    "        data = json.loads(response.text)\n",
    "        return data\n",
    "\n",
    "    def get_frames(self, taxonomy, tag, unit, period):\n",
    "        \"\"\"The xbrl/frames API aggregates one fact for each reporting entity that is last filed that most closely fits the calendrical period requested. \n",
    "        This API supports for annual, quarterly and instantaneous data: https://data.sec.gov/api/xbrl/frames/us-gaap/AccountsPayableCurrent/USD/CY2019Q1I.json\n",
    "\n",
    "        Args:\n",
    "            taxonomy (str): us-gaap, ifrs-full, dei, or srt\n",
    "            tag (str): taxonomy tag (e.g. Revenue, AccountsPayableCurrent). See full list from https://xbrl.fasb.org/us-gaap/2023/elts/us-gaap-2023.xsd\n",
    "            unit (str): USD, USD-per-shares, etc.\n",
    "            period (str): CY#### for annual data (duration 365 days +/- 30 days), CY####Q# for quarterly data (duration 91 days +/- 30 days), CY####Q#I for instantaneous data\n",
    "\n",
    "        Raises:\n",
    "            Exception: (placeholder)\n",
    "\n",
    "        Returns:\n",
    "            data: json formatted response\n",
    "        \"\"\"\n",
    "        url = f\"{self.BASE_API_URL}api/xbrl/frames/{taxonomy}/{tag}/{unit}/{period}.json\"\n",
    "        response = self.rate_limited_request(\n",
    "            url, headers=self.sec_data_headers)\n",
    "        data = json.loads(response.text)\n",
    "        return data\n",
    "\n",
    "    def get_data_as_dataframe(self, cik: str,):\n",
    "        \"\"\"Retrieves the XBRL disclosures from a single company (CIK) and returns it as a pandas dataframe.\n",
    "\n",
    "        Args:\n",
    "            cik (str): CIK number of the company. Get the list using self.cik\n",
    "\n",
    "        Returns:\n",
    "            df: pandas dataframe containing the XBRL disclosures from a single company (CIK)\n",
    "        \"\"\"\n",
    "        data = self.get_company_facts(cik)\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for tag in data['facts'][self.taxonomy]:\n",
    "            facts = data['facts']['us-gaap'][tag]['units']\n",
    "            unit_key = list(facts.keys())[0]\n",
    "            temp_df = pd.DataFrame(facts[unit_key])\n",
    "            temp_df['label'] = tag\n",
    "            df = pd.concat([df, temp_df], axis=0, ignore_index=True)\n",
    "        df = df.astype({'val': 'float64',\n",
    "                        'end': 'datetime64[ns]',\n",
    "                        'start': 'datetime64[ns]',\n",
    "                        'filed': 'datetime64[ns]'})\n",
    "        df['Months Ended'] = (df['end'] - df['start']\n",
    "                              ).dt.days.div(30.4375).round(0)\n",
    "        return df\n",
    "\n",
    "    def get_cik_index(self, cik: str = None,) -> dict:\n",
    "        \"\"\"Each CIK directory and all child subdirectories contain three files to assist in \n",
    "        automated crawling of these directories. \n",
    "        These are not visible through directory browsing.\n",
    "            - index.html (the web browser would normally receive these)\n",
    "            - index.xml (a XML structured version of the same content)\n",
    "            - index.json (a JSON structured vision of the same content)\n",
    "\n",
    "        Args:\n",
    "            cik (str): CIK number of the company. Get the list using self.cik\n",
    "\n",
    "        Returns:\n",
    "            json: pandas dataframe containing the XBRL disclosures from a single company (CIK)\n",
    "        \"\"\"\n",
    "        if cik is not None:\n",
    "            url = self.BASE_DIRECTORY_URL + cik + '/' + 'index.json'\n",
    "\n",
    "        else:\n",
    "            url = self.BASE_DIRECTORY_URL + self.cik + '/' + 'index.json'\n",
    "\n",
    "        response = self.rate_limited_request(url, headers=self.sec_headers)\n",
    "        return response.json()\n",
    "\n",
    "    def get_sic_list(self, sic_list_url: str = SIC_LIST_URL) -> dict:\n",
    "        \"\"\"Get the list of SIC codes from SEC website.\n",
    "\n",
    "        Args:\n",
    "            sic_list_url (str): URL to the list of SIC codes\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing the SIC codes and descriptions\n",
    "        \"\"\"\n",
    "        response = self.rate_limited_request(\n",
    "            sic_list_url, headers=self.sec_headers)\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"lxml\")\n",
    "        sic_table = soup.find('table', {'class': 'list'})\n",
    "        sic_list = []\n",
    "        for row in sic_table.find_all('tr')[1:]:\n",
    "            sic_dict = {'_id': None,\n",
    "                        'Office': None, 'Industry Title': None}\n",
    "            sic_dict['_id'] = row.text.split('\\n')[1]\n",
    "            sic_dict['Office'] = row.text.split('\\n')[2]\n",
    "            sic_dict['Industry Title'] = row.text.split('\\n')[3]\n",
    "            sic_list.append(sic_dict)\n",
    "\n",
    "        return sic_list\n",
    "\n",
    "\n",
    "class TickerData(SECData):\n",
    "    \"\"\"Inherited from SECData class. Retrieves data from SEC Edgar database based on ticker.\n",
    "    url is constructed based on the following: https://www.sec.gov/Archives/edgar/data/{cik}/{ascension_number}/{file_name}\n",
    "    cik is the CIK number of the company = access via get_ticker_cik\n",
    "    ascension_number is the accessionNumber column of filings_df\n",
    "    file name for xml is always '{ticker}-{reportDate}.{extension}\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ticker: str, requester_company: str = 'Financial API', requester_name: str = 'API Caller', requester_email: str = 'apicaller@gmail.com', taxonomy: str = 'us-gaap',):\n",
    "        super().__init__(requester_company, requester_name, requester_email, taxonomy)\n",
    "        self.ticker = ticker.upper()\n",
    "        self.cik = self.get_ticker_cik(self.ticker)\n",
    "        self._submissions = self.get_submissions(self.cik)\n",
    "        self._filings = None\n",
    "        self._forms = None\n",
    "        self._index = self.get_cik_index(self.cik)\n",
    "        self._filing_folder_urls = None\n",
    "        self._filing_urls = None\n",
    "\n",
    "    @property\n",
    "    def submissions(self,) -> dict:\n",
    "        if self._submissions is not None:\n",
    "            self._submissions['cik'] = self.cik\n",
    "            self._submissions['filings'] = self.filings.replace(\n",
    "                {pd.NaT: None}).to_dict('records')\n",
    "        return self._submissions\n",
    "\n",
    "    @property\n",
    "    def filings(self,) -> pd.DataFrame:\n",
    "        if self._filings is None:\n",
    "            self._filings = self.get_filings()\n",
    "        return self._filings\n",
    "\n",
    "    @property\n",
    "    def latest_filing(self,) -> pd.DataFrame:\n",
    "        return self.filings.iloc[0, :].to_dict() if len(self.filings) > 0 else None\n",
    "\n",
    "    @property\n",
    "    def latest_10Q(self,) -> pd.DataFrame:\n",
    "        return self.filings.query(\"form == '10-Q'\").iloc[0, :].to_dict() if len(self.filings.query(\"form == '10-Q'\")) > 0 else None\n",
    "\n",
    "    @property\n",
    "    def latest_10K(self,) -> pd.DataFrame:\n",
    "        return self.filings.query(\"form == '10-K'\").iloc[0, :].to_dict() if len(self.filings.query(\"form == '10-K'\")) > 0 else None\n",
    "\n",
    "    @property\n",
    "    def latest_8K(self,) -> pd.DataFrame:\n",
    "        return self.filings.query(\"form == '8-K'\").iloc[0, :].to_dict() if len(self.filings.query(\"form == '8-K'\")) > 0 else None\n",
    "\n",
    "    @property\n",
    "    def filing_folder_urls(self,) -> list:\n",
    "        if self._filing_folder_urls is None:\n",
    "            self._filing_folder_urls = self._get_filing_folder_urls()\n",
    "        return self._filing_folder_urls\n",
    "\n",
    "    @property\n",
    "    def filing_urls(self,) -> list:\n",
    "        if self._filing_urls is None:\n",
    "            self._filing_urls = self.filings['file_url'].tolist()\n",
    "\n",
    "        return self._filing_urls\n",
    "\n",
    "    @property\n",
    "    def forms(self,) -> list:\n",
    "        if self._forms is None:\n",
    "            self._forms = self.filings['form'].unique()\n",
    "        return self._forms\n",
    "\n",
    "    def _get_filing_folder_urls(self,) -> list:\n",
    "        \"\"\"Get filing folder urls from index dict.\n",
    "\n",
    "        Args:\n",
    "            index (dict): index dict from get_index method\n",
    "\n",
    "        Returns:s\n",
    "            filing_folder_urls (list): list of filing folder urls\n",
    "        \"\"\"\n",
    "\n",
    "        filing_folder_urls = [self.BASE_SEC_URL + self._index['directory']['name'] + '/' + folder['name']\n",
    "                              for folder in self._index['directory']['item'] if folder['type'] == 'folder.gif']\n",
    "        return filing_folder_urls\n",
    "\n",
    "    def _get_filing_urls(self,) -> list:\n",
    "        \"\"\"(DEPRECATED)\n",
    "        ---The filing urls are implemented in the get_filings method.---\n",
    "\n",
    "        Get filing urls from filing folder urls.\n",
    "\n",
    "        Args:\n",
    "            filing_folder_urls (list): list of filing folder urls\n",
    "\n",
    "        Returns:\n",
    "            filing_urls (list): list of filing urls to .txt files\n",
    "        \"\"\"\n",
    "        filing_urls = []\n",
    "        with trange(len(self.filing_folder_urls), desc=f'Instantiating filing urls for {self.ticker}...') as t:\n",
    "            for i in t:\n",
    "                self.scrape_logger.info(t)\n",
    "                try:\n",
    "                    soup = self.get_file_data(self.filing_folder_urls[i])\n",
    "                    for link in soup.find_all('a'):\n",
    "                        if link.get('href').endswith('.txt'):\n",
    "                            filing_urls.append(\n",
    "                                self.BASE_SEC_URL + link.get('href'))\n",
    "                except Exception as e:\n",
    "                    self.scrape_logger.error(\n",
    "                        f'Failed to instantiate filing urls for {self.ticker}...')\n",
    "                    self.scrape_logger.error(e)\n",
    "                    t.write(\n",
    "                        f'Failed to instantiate filing urls for {self.ticker}...')\n",
    "                    continue\n",
    "        return filing_urls\n",
    "\n",
    "    def get_filing_folder_index(self, folder_url: str, return_df: bool = True) -> dict | pd.DataFrame:\n",
    "        \"\"\"Get filing folder index from folder url.\n",
    "\n",
    "        Args:\n",
    "            folder_url (str): folder url to retrieve data from\n",
    "            return_df (bool, optional): Whether to return a DataFrame or dict. Defaults to True.\n",
    "        \n",
    "        Returns:\n",
    "            index (dict): index dict or dataframe\n",
    "        \"\"\"\n",
    "        index_url = indexify_url(folder_url)\n",
    "        index = self.rate_limited_request(index_url, headers=self.sec_headers)\n",
    "        return pd.DataFrame(index.json()['directory']['item']) if return_df else index.json()['directory']['item']\n",
    "\n",
    "    def get_filings(self,) -> dict:\n",
    "        \"\"\"Get filings and urls to .txt from submissions dict.\n",
    "\n",
    "        Args:\n",
    "            submissions (dict): submissions dict from get_submissions method\n",
    "\n",
    "        Returns:\n",
    "            filings (dict): dictionary containing filings\n",
    "        \"\"\"\n",
    "        self.scrape_logger.info(\n",
    "            f'Making http request for {self.ticker} filings...')\n",
    "        filings = self._submissions['filings']['recent']\n",
    "\n",
    "        if len(self._submissions['filings']) > 1:\n",
    "            self.scrape_logger.info(\n",
    "                f'Additional filings found for {self.ticker}...')\n",
    "            for file in self._submissions['filings']['files']:\n",
    "                additional_filing = self.get_submissions(\n",
    "                    submission_file=file['name'])\n",
    "                filings = {key: filings[key] + additional_filing[key]\n",
    "                           for key in filings.keys()}\n",
    "\n",
    "        filings = pd.DataFrame(filings)\n",
    "        # Convert reportDate, filingDate, acceptanceDateTime columns to datetime\n",
    "        filings['reportDate'] = pd.to_datetime(filings['reportDate'])\n",
    "        filings['filingDate'] = pd.to_datetime(filings['filingDate'])\n",
    "        filings['acceptanceDateTime'] = pd.to_datetime(\n",
    "            filings['acceptanceDateTime'])\n",
    "        filings['cik'] = self.cik\n",
    "\n",
    "        filings = filings.loc[~pd.isnull(filings['reportDate'])]\n",
    "\n",
    "        # get folder url for each row\n",
    "        filings['folder_url'] = self.BASE_DIRECTORY_URL + \\\n",
    "            self.cik + '/' + filings['accessionNumber'].str.replace('-', '')\n",
    "\n",
    "        # get file url for each row\n",
    "        filings['file_url'] = filings['folder_url'] + \\\n",
    "            '/' + filings['accessionNumber'] + '.txt'\n",
    "\n",
    "        return filings\n",
    "\n",
    "    def get_file_data(self, file_url: str) -> BeautifulSoup:\n",
    "        \"\"\"Get file data from file url which can be retrieved by calling self.get_file_url method.\n",
    "\n",
    "        Args:\n",
    "            file_url (str): File url to retrieve data from on the SEC website\n",
    "\n",
    "        Returns:\n",
    "            data: File data as a BeautifulSoup object\n",
    "        \"\"\"\n",
    "        data = self.rate_limited_request(\n",
    "            url=file_url, headers=self.sec_headers)\n",
    "        try:\n",
    "            soup = BeautifulSoup(data.content, \"lxml\")\n",
    "            self.scrape_logger.info(\n",
    "                f'Parsed file data from {file_url} successfully.')\n",
    "            return soup\n",
    "\n",
    "        except Exception as e:\n",
    "            self.scrape_logger.error(\n",
    "                f'Failed to parse file data from {file_url}. Error: {e}')\n",
    "            raise Exception(\n",
    "                f'Failed to parse file data from {file_url}. Error: {e}')\n",
    "\n",
    "    # TODO: replace search_xxx methods with strategy pattern\n",
    "\n",
    "    def get_elements(self, folder_url: str, index_df: pd.DataFrame, scrape_file_extension: str) -> pd.DataFrame:\n",
    "        \"\"\"Get elements from .xml files from folder_url.\n",
    "\n",
    "        Args:\n",
    "            folder_url (str): folder url to retrieve data from\n",
    "            index_df (pd.DataFrame): dataframe containing files in the filing folder\n",
    "            scrape_file_extension (str): .xml file extension to scrape\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: returns a dataframe containing the elements, attributes, text\n",
    "        \"\"\"\n",
    "        xml = index_df.query(f\"name.str.contains('{scrape_file_extension}')\")\n",
    "        xml_content = self.rate_limited_request(folder_url + '/' + xml['name'].iloc[0], headers=self.sec_headers).content\n",
    "\n",
    "        xml_soup = BeautifulSoup(xml_content, 'lxml-xml')\n",
    "        labels = xml_soup.find_all()\n",
    "        labels_list = []\n",
    "        for i in labels[1:]:\n",
    "            label_dict = dict(**i.attrs, labelText=i.text.strip())\n",
    "            labels_list.append(label_dict)\n",
    "        return pd.DataFrame(labels_list)\n",
    "    \n",
    "    def search_tags(self, soup: BeautifulSoup, pattern: str) -> BeautifulSoup:\n",
    "        \"\"\"Search for tags in BeautifulSoup object.\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): BeautifulSoup object\n",
    "            pattern (str): regex pattern to search for\n",
    "\n",
    "        Returns:\n",
    "            soup: BeautifulSoup object\n",
    "        \"\"\"\n",
    "        return soup.find_all(re.compile(pattern))\n",
    "\n",
    "    def search_context(self, soup: BeautifulSoup) -> pd.DataFrame:\n",
    "        \"\"\"Search for context in company .txt filing. \n",
    "        Context provides information about the entity, segment, and time period for facts in the filing.\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): BeautifulSoup object\n",
    "\n",
    "        Returns:\n",
    "            df: DataFrame containing context information with columns \n",
    "            {\n",
    "                'contextId': str,\n",
    "                'entity': str,\n",
    "                'segment': str,\n",
    "                'startDate': 'datetime64[ns]',\n",
    "                'endDate': 'datetime64[ns]',\n",
    "                'instant': 'datetime64[ns]'\n",
    "            }\n",
    "        \"\"\"\n",
    "        contexts = self.search_tags(soup, '^context$')\n",
    "        dict_list = []\n",
    "        columns = {'contextId': str, 'entity': str, 'segment': str,\n",
    "                   'startDate': 'datetime64[ns]', 'endDate': 'datetime64[ns]', 'instant': 'datetime64[ns]'}\n",
    "        for tag in contexts:\n",
    "            temp_dict = {}\n",
    "            temp_dict['contextId'] = tag.attrs.get('id')\n",
    "            temp_dict['entity'] = tag.find(\"entity\").text.split()[\n",
    "                0] if tag.find(\"entity\") is not None else None\n",
    "            temp_dict['segment'] = tag.find(\"segment\").text.strip(\n",
    "            ) if tag.find(\"segment\") is not None else None\n",
    "            temp_dict['startDate'] = tag.find(\"startdate\").text if tag.find(\n",
    "                \"startdate\") is not None else None\n",
    "            temp_dict['endDate'] = tag.find(\"enddate\").text if tag.find(\n",
    "                \"enddate\") is not None else None\n",
    "            temp_dict['instant'] = tag.find(\"instant\").text if tag.find(\n",
    "                \"instant\") is not None else None\n",
    "            dict_list.append(temp_dict)\n",
    "\n",
    "        df = pd.DataFrame(dict_list, columns=columns.keys()).astype(columns)\n",
    "        return df\n",
    "\n",
    "    def search_linklabels(self, soup: BeautifulSoup) -> pd.DataFrame:\n",
    "        \"\"\"Search for link labels in company .txt filing. \n",
    "        Link labels provide information about the relationship between facts and their corresponding concepts.\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): BeautifulSoup object\n",
    "\n",
    "        Returns:\n",
    "            df: DataFrame containing link label information with columns \n",
    "            {\n",
    "                'linkLabelId': str,\n",
    "                'xlinkLabel': str,\n",
    "                'xlinkRole': str,\n",
    "                'xlinkType': str,\n",
    "                'xlmnsXml': str,\n",
    "                'xmlLang': str,\n",
    "                'label': str\n",
    "            }\n",
    "        \"\"\"\n",
    "        links = self.search_tags(soup, '^link:label$')\n",
    "        dict_list = []\n",
    "        columns = {'linkLabelId': str, 'xlinkLabel': str, 'xlinkRole': str,\n",
    "                   'xlinkType': str, 'xlmnsXml': str, 'xmlLang': str, 'label': str}\n",
    "\n",
    "        for tag in links:\n",
    "            temp_dict = {}\n",
    "            temp_dict['linkLabelId'] = tag.attrs.get('id')\n",
    "            temp_dict['xlinkLabel'] = tag.attrs.get('xlink:label')\n",
    "            temp_dict['xlinkRole'] = tag.attrs.get('xlink:role')\n",
    "            temp_dict['xlinkType'] = tag.attrs.get('xlink:type')\n",
    "            temp_dict['xlmnsXml'] = tag.attrs.get('xmlns:xml')\n",
    "            temp_dict['xmlLang'] = tag.attrs.get('xml:lang')\n",
    "            temp_dict['label'] = tag.text if tag.text is not None else None\n",
    "            dict_list.append(temp_dict)\n",
    "\n",
    "        df = pd.DataFrame(dict_list, columns=columns.keys()).astype(columns)\n",
    "        return df\n",
    "\n",
    "    def search_facts(self, soup: BeautifulSoup) -> pd.DataFrame:\n",
    "        \"\"\"Search for facts in company .txt filing. \n",
    "        Facts provide the actual data values for the XBRL disclosures.\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): BeautifulSoup object\n",
    "\n",
    "        Returns:\n",
    "            df: DataFrame containing fact information with columns \n",
    "            {\n",
    "                'factName': str,\n",
    "                'contextRef': str,\n",
    "                'decimals': int,\n",
    "                'factId': str,\n",
    "                'unitRef': str,\n",
    "                'value': str\n",
    "            }\n",
    "        \"\"\"\n",
    "        facts = self.search_tags(soup, '^us-gaap:')\n",
    "        dict_list = []\n",
    "        columns = {'factName': str, 'contextRef': str, 'decimals': int, 'factId': str,\n",
    "                   'unitRef': str, 'value': str}\n",
    "\n",
    "        for tag in facts:\n",
    "            temp_dict = {}\n",
    "            temp_dict['factName'] = tag.name\n",
    "            temp_dict['contextRef'] = tag.attrs.get('contextref')\n",
    "            temp_dict['decimals'] = tag.attrs.get('decimals')\n",
    "            temp_dict['factId'] = tag.attrs.get('id')\n",
    "            temp_dict['unitRef'] = tag.attrs.get('unitref')\n",
    "            temp_dict['value'] = tag.text\n",
    "            dict_list.append(temp_dict)\n",
    "\n",
    "        df = pd.DataFrame(dict_list, columns=columns.keys())\n",
    "        return df\n",
    "\n",
    "    def get_metalinks(self, metalinks_url: str) -> pd.DataFrame:\n",
    "        \"\"\"Get metalinks from metalinks url.\n",
    "\n",
    "        Args:\n",
    "            metalinks_url (str): metalinks url to retrieve data from\n",
    "\n",
    "        Returns:\n",
    "            df: DataFrame containing metalinks information with columns \n",
    "            {\n",
    "                'labelKey': str,\n",
    "                'localName': str,\n",
    "                'labelName': int,\n",
    "                'terseLabel': str,\n",
    "                'documentation': str,\n",
    "            }\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.rate_limited_request(\n",
    "                url=metalinks_url, headers=self.sec_headers).json()\n",
    "            metalinks_instance = convert_keys_to_lowercase(\n",
    "                response['instance'])\n",
    "            instance_key = list(metalinks_instance.keys())[0]\n",
    "            dict_list = []\n",
    "            for i in metalinks_instance[instance_key]['tag']:\n",
    "                dict_list.append(dict(labelKey=i.lower(),\n",
    "                                      localName=metalinks_instance[instance_key]['tag'][i].get(\n",
    "                                          'localname'),\n",
    "                                      labelName=metalinks_instance[instance_key]['tag'][i].get(\n",
    "                                          'lang').get('enus').get('role').get('label'),\n",
    "                                      terseLabel=metalinks_instance[instance_key]['tag'][i].get(\n",
    "                                          'lang').get('enus').get('role').get('terselabel'),\n",
    "                                      documentation=metalinks_instance[instance_key]['tag'][i].get('lang').get('enus').get('role').get('documentation'),))\n",
    "\n",
    "            df = pd.DataFrame.from_dict(dict_list)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            self.scrape_logger.error(\n",
    "                f'Failed to retrieve metalinks from {metalinks_url}. Error: {e}')\n",
    "            return None\n",
    "\n",
    "    def get_facts_for_each_filing(self, filing: dict) -> dict:\n",
    "        \"\"\"Get facts for each filing.\n",
    "\n",
    "        Args:\n",
    "            filing_url (str): filing url to retrieve data from (link to .txt file in filing directory)\n",
    "            folder_url (str): folder url to retrieve data from (link to filing directory)\n",
    "        Returns:\n",
    "            df: DataFrame containing facts information with columns \n",
    "            {\n",
    "                'factName': str,\n",
    "                'contextRef': str,\n",
    "                'decimals': int,\n",
    "                'factId': str,\n",
    "                'unitRef': str,\n",
    "                'value': str,\n",
    "                'contextId': str,\n",
    "                'entity': str,\n",
    "                'segment': str,\n",
    "                'startDate': 'datetime64[ns]',\n",
    "                'endDate': 'datetime64[ns]',\n",
    "                'instant': 'datetime64[ns]',\n",
    "                # 'labelKey': str,\n",
    "                # 'localName': str,\n",
    "                # 'labelName': int,\n",
    "                # 'terseLabel': str,\n",
    "                # 'documentation': str,\n",
    "                'accessionNumber': str,\n",
    "            }\n",
    "        \"\"\"\n",
    "        columns_to_keep = ['factName', 'contextRef', 'decimals', 'factId', 'unitRef', 'value', 'segment', 'startDate',\n",
    "                           'endDate', 'instant', 'accessionNumber']\n",
    "        soup = self.get_file_data(filing['file_url'])\n",
    "        facts = self.search_facts(soup)\n",
    "        context = self.search_context(soup)\n",
    "        # metalinks = self.get_metalinks(\n",
    "        #     filing['folder_url'] + '/MetaLinks.json')\n",
    "\n",
    "        # if metalinks is None:\n",
    "        #     return None\n",
    "        context['segment'] = context['segment'].str.replace(\n",
    "            pat=r'[^a-zA-Z0-9]', repl='', regex=True).str.lower()\n",
    "        df = facts.merge(context, how='left', left_on='contextRef', right_on='contextId')\n",
    "            # .merge(metalinks, how='left', left_on='segment', right_on='labelKey')\n",
    "\n",
    "        df['ticker'] = self.ticker\n",
    "        df['cik'] = self.cik\n",
    "        df['accessionNumber'] = filing['accessionNumber']\n",
    "\n",
    "        df = df.loc[~df['unitRef'].isnull(), columns_to_keep].replace({\n",
    "            pd.NaT: None})\n",
    "\n",
    "        return facts, context, df.to_dict('records')\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        class_name = type(self).__name__\n",
    "        main_attrs = ['ticker', 'cik', 'submissions', 'filings']\n",
    "        available_methods = [method_name for method_name in dir(self) if callable(\n",
    "            getattr(self, method_name)) and not method_name.startswith(\"_\")]\n",
    "        return f\"\"\"{class_name}({self.ticker})\n",
    "    CIK: {self.cik}\n",
    "    Latest filing: {self.latest_filing['filingDate'].strftime('%Y-%m-%d') if self.latest_filing else 'No filing found'} for Form {self.latest_filing['form'] if self.latest_filing else None}. Access via: {self.latest_filing['folder_url'] if self.latest_filing else None}\n",
    "    Latest 10-Q: {self.latest_10Q['filingDate'].strftime('%Y-%m-%d') if self.latest_10Q else 'No filing found'}. Access via: {self.latest_10Q['folder_url'] if self.latest_10Q else None}\n",
    "    Latest 10-K: {self.latest_10K['filingDate'].strftime('%Y-%m-%d') if self.latest_10K else 'No filing found'}. Access via: {self.latest_10K['folder_url'] if self.latest_10K else None}\"\"\"\n",
    "\n",
    "    def __repr_html__(self) -> str:\n",
    "        class_name = type(self).__name__\n",
    "        main_attrs = ['ticker', 'cik', 'submissions', 'filings']\n",
    "        available_methods = [method_name for method_name in dir(self) if callable(\n",
    "            getattr(self, method_name)) and not method_name.startswith(\"_\")]\n",
    "        latest_filing_date = self.latest_filing['filingDate'].strftime(\n",
    "            '%Y-%m-%d') if self.latest_filing else 'No filing found'\n",
    "        latest_filing_form = self.latest_filing['form'] if self.latest_filing else None\n",
    "        latest_filing_folder_url = self.latest_filing['folder_url'] if self.latest_filing else None\n",
    "        latest_10Q_date = self.latest_10Q['filingDate'].strftime(\n",
    "            '%Y-%m-%d') if self.latest_10Q else 'No filing found'\n",
    "        latest_10Q_folder_url = self.latest_10Q['folder_url'] if self.latest_10Q else None\n",
    "        latest_10K_date = self.latest_10K['filingDate'].strftime(\n",
    "            '%Y-%m-%d') if self.latest_10K else 'No filing found'\n",
    "        latest_10K_folder_url = self.latest_10K['folder_url'] if self.latest_10K else None\n",
    "        return f\"\"\"\n",
    "        <div style=\"border: 1px solid #ccc; padding: 10px; margin: 10px;\">\n",
    "            <h3>{self.submissions['name']}</h3>\n",
    "            <h5>{self.submissions['sicDescription']}</h5>\n",
    "            <p><strong>Ticker:</strong> {self.ticker}</p>\n",
    "            <p><strong>CIK:</strong> {self.cik}</p>\n",
    "            <p><strong>Latest filing:</strong> {latest_filing_date} for Form {latest_filing_form}. Access via: <a href=\"{latest_filing_folder_url}\">{latest_filing_folder_url}</a></p>\n",
    "            <p><strong>Latest 10-Q:</strong> {latest_10Q_date}. Access via: <a href=\"{latest_10Q_folder_url}\">{latest_10Q_folder_url}</a></p>\n",
    "            <p><strong>Latest 10-K:</strong> {latest_10K_date}. Access via: <a href=\"{latest_10K_folder_url}\">{latest_10K_folder_url}</a></p>\n",
    "        </div>\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MongoDB Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient, ASCENDING, IndexModel, UpdateOne\n",
    "from pymongo.errors import OperationFailure\n",
    "from dotenv import load_dotenv\n",
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class SECDatabase(MyLogger):\n",
    "    def __init__(self, connection_string):\n",
    "        super().__init__(name='SECDatabase', level='DEBUG', log_file='././logs.log')\n",
    "        self.client = MongoClient(connection_string)\n",
    "        self.db = self.client.SECRawData\n",
    "        self.tickerdata = self.db.TickerData\n",
    "        self.tickerfilings = self.db.TickerFilings\n",
    "        self.sicdb = self.db.SICList\n",
    "        self.factsdb = self.db.Facts\n",
    "        try:\n",
    "            self.tickerdata.create_indexes(\n",
    "                [IndexModel([('cik', ASCENDING)], unique=True)])\n",
    "        except OperationFailure as e:\n",
    "            self.scrape_logger.error(e)\n",
    "\n",
    "        try:\n",
    "            self.tickerfilings.create_indexes([IndexModel(\n",
    "                [('accessionNumber', ASCENDING)], unique=True), IndexModel([('form', ASCENDING)])])\n",
    "        except OperationFailure as e:\n",
    "            self.scrape_logger.error(e)\n",
    "\n",
    "        try:\n",
    "            self.factsdb.create_indexes(\n",
    "                [IndexModel([('factId', ASCENDING)], unique=True)])\n",
    "\n",
    "        except OperationFailure as e:\n",
    "            self.scrape_logger.error(e)\n",
    "\n",
    "    @property\n",
    "    def get_server_info(self):\n",
    "        return self.client.server_info()\n",
    "\n",
    "    @property\n",
    "    def get_collection_names(self):\n",
    "        return self.db.list_collection_names()\n",
    "\n",
    "    @property\n",
    "    def get_tickerdata_index_information(self):\n",
    "        return self.tickerdata.index_information()\n",
    "\n",
    "    @property\n",
    "    def get_tickerfilings_index_information(self):\n",
    "        return self.tickerfilings.index_information()\n",
    "\n",
    "    def get_tickerdata(self, cik: str = None, ticker: str = None):\n",
    "        if cik is not None:\n",
    "            return self.tickerdata.find_one({'cik': cik})\n",
    "        elif ticker is not None:\n",
    "            return self.tickerdata.find_one({'tickers': ticker.upper()})\n",
    "        else:\n",
    "            raise Exception('Please provide either a CIK or ticker.')\n",
    "\n",
    "    def update_sic_list(self, sic_list: list) -> None:\n",
    "        \"\"\"Update SIC list in SEC database.\n",
    "\n",
    "        Args:\n",
    "            sic_list (list): List of SIC codes and descriptions\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for i in range(len(sic_list)):\n",
    "                self.sicdb.update_one({'_id': sic_list[i]['_id']}, {'$set': sic_list[i]}, upsert=True)\n",
    "            self.scrape_logger.info(\n",
    "                f'Successfully updated SIC list in SEC database.')\n",
    "        except Exception as e:\n",
    "            self.scrape_logger.error(\n",
    "                f'Failed to update SIC list in SEC database. Error: {e}')\n",
    "        return None\n",
    "\n",
    "    def insert_submission(self, submission: dict):\n",
    "        \"\"\"Insert submissions into SEC database.\n",
    "\n",
    "        Args:\n",
    "            ticker (TickerData): TickerData object\n",
    "\n",
    "        Returns:\n",
    "            str: empty string if successful\n",
    "            str: ticker's cik if failed\n",
    "        \"\"\"\n",
    "        submission['lastUpdated'] = dt.datetime.now()\n",
    "        try:\n",
    "            self.tickerdata.update_one({'cik': submission['cik']}, {\n",
    "                                       '$set': submission}, upsert=True)\n",
    "            self.scrape_logger.info(\n",
    "                f'Inserted submissions for {submission[\"cik\"]} into SEC database.')\n",
    "\n",
    "        except Exception as e:\n",
    "            self.scrape_logger.error(\n",
    "                f'Failed to insert submissions for {submission[\"cik\"]} into SEC database. Error: {e}')\n",
    "            return submission['cik']\n",
    "        return None\n",
    "\n",
    "    def insert_filings(self, cik: str, filings: list):\n",
    "        \"\"\"Insert filings into SEC database. Each submission has many filings.\n",
    "\n",
    "        Args:\n",
    "            ticker (TickerData): TickerData object\n",
    "\n",
    "        Returns:\n",
    "            str: empty string if successful\n",
    "            str: ticker's cik if failed\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for doc in filings:\n",
    "                doc['lastUpdated'] = dt.datetime.now()\n",
    "\n",
    "            update_requests = [UpdateOne({'accessionNumber': doc['accessionNumber']}, {\n",
    "                                         '$set': doc}, upsert=True) for doc in filings]\n",
    "\n",
    "            self.tickerfilings.bulk_write(update_requests)\n",
    "            self.scrape_logger.info(\n",
    "                f'Sucessfully updated filings for {cik}...')\n",
    "\n",
    "        except Exception as e:\n",
    "            self.scrape_logger.error(\n",
    "                f'Failed to insert filings for {cik}...{e}')\n",
    "            return cik\n",
    "        return None\n",
    "\n",
    "    def insert_facts(self, accession: str, facts: list):\n",
    "        \"\"\"Insert facts into SEC database. Each filing has many facts.\n",
    "\n",
    "        Args:\n",
    "            facts (list): A list containing facts for a single filing\n",
    "\n",
    "        Returns:\n",
    "            str: empty string if successful\n",
    "            str: ticker's cik if failed\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for doc in facts:\n",
    "                doc['lastUpdated'] = dt.datetime.now()\n",
    "\n",
    "            fact_update_requests = [UpdateOne({'factId': fact['factId']}, {\n",
    "                                              '$set': fact}, upsert=True) for fact in facts]\n",
    "\n",
    "            self.factsdb.bulk_write(fact_update_requests)\n",
    "            self.scrape_logger.info(f'Updated facts for {accession}...')\n",
    "\n",
    "        except Exception as e:\n",
    "            self.scrape_logger.error(\n",
    "                f'Failed to insert facts for {accession}...{e}')\n",
    "            return accession\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec = SECData()\n",
    "mongo = SECDatabase(os.getenv('mongodb_sec'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to insert submission, filings, and facts for each filing into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec = SECData()\n",
    "sic_dict = sec.get_sic_list()\n",
    "mongo = SECDatabase(connection_string=os.getenv('mongodb_sec'))\n",
    "\n",
    "failed_submissions = []\n",
    "failed_filings = []\n",
    "failed_facts = []\n",
    "\n",
    "with trange(len(sec.cik_list['ticker'][:50]), desc='Instantiating ticker...',) as t:\n",
    "    for item in t:\n",
    "        ticker = sec.cik_list['ticker'].iloc[item] # Get ticker from cik_list\n",
    "        t.set_postfix(ticker=ticker, cik=sec.cik_list['cik_str'].iloc[item])\n",
    "\n",
    "        # Initialize and instantiate TickerData object\n",
    "        try:\n",
    "            symbol = TickerData(ticker=ticker)\n",
    "            cik = symbol.cik # get cik of ticker\n",
    "            symbol.submissions['lastUpdated'] = dt.datetime.now()\n",
    "            symbol.submissions['office'] = mongo.sicdb.find_one({'_id': symbol.submissions['sic']})['Office']\n",
    "            sec.scrape_logger.info(f'{t}')\n",
    "            sec.scrape_logger.info(f'\\nInstantiated {symbol}...')\n",
    "        except Exception as e:\n",
    "            sec.scrape_logger.info(f'{t}')\n",
    "            sec.scrape_logger.error(f'Failed to instantiate {ticker} with cik {cik}...{e}')\n",
    "            continue\n",
    "\n",
    "        filings = symbol.submissions.pop('filings')\n",
    "        # print(filings)\n",
    "        # Insert submissions to TickerData collection\n",
    "        inserted_submission = mongo.insert_submission(submission=symbol._submissions)\n",
    "        if inserted_submission is not None:\n",
    "            failed_submissions.append(inserted_submission)\n",
    "\n",
    "        # Insert filings to TickerFilings collection\n",
    "        inserted_filing = mongo.insert_filings(cik=cik, filings=filings)\n",
    "        if inserted_filing is not None:\n",
    "            failed_filings.append(inserted_filing)\n",
    "\n",
    "        # # Insert facts to Facts collection\n",
    "        # for doc in filings:\n",
    "        #     doc['lastUpdated'] = dt.datetime.now()\n",
    "\n",
    "        #     if doc['form'] == '10-Q' or doc['form'] == '10-K':\n",
    "        #         try:\n",
    "        #             facts = symbol.get_facts_for_each_filing(doc)\n",
    "        #             inserted_facts = mongo.insert_facts(accession=doc['accessionNumber'], facts=facts)\n",
    "        #             if inserted_facts is not None:\n",
    "        #                 failed_facts.append(inserted_facts)\n",
    "        #         except Exception as e:\n",
    "        #             sec.scrape_logger.error(f'TickerData().get_facts_for_each_filing() function failed for {doc[\"accessionNumber\"]}...{e}')\n",
    "        #             failed_facts.append(doc['accessionNumber'])\n",
    "            \n",
    "        sec.scrape_logger.info(f'Successfully updated {ticker}({cik})...\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather labels, definitions, and calculations xml data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:167: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:167: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/var/folders/hg/wck4x05567dcx5xstz11b4qm0000gn/T/ipykernel_93263/419073395.py:167: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  '[^0-9\\.\\-]|(^\\d+\\-\\d+\\-\\d+$)')) & (merged_facts['value'] != \"\")].copy()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_filing_facts(ticker: TickerData, filings_to_scrape: list,):\n",
    "    \"\"\"\n",
    "    Scrape facts, context, labels, definitions, calculations, metalinks from filings_to_scrape\n",
    "\n",
    "    ### Parameters\n",
    "    ----------\n",
    "    ticker : TickerData\n",
    "        TickerData object\n",
    "    filings_to_scrape : list\n",
    "        list of filings dict to scrape\n",
    "\n",
    "    ### Returns\n",
    "    -------\n",
    "    all_labels : pd.DataFrame\n",
    "        all labels scraped\n",
    "    all_calc : pd.DataFrame\n",
    "        all calculations scraped\n",
    "    all_defn : pd.DataFrame\n",
    "        all definitions scraped\n",
    "    all_context : pd.DataFrame\n",
    "        all contexts scraped\n",
    "    all_facts : pd.DataFrame\n",
    "        all facts scraped\n",
    "    all_metalinks : pd.DataFrame    \n",
    "        all metalinks scraped\n",
    "    all_merged_facts : pd.DataFrame\n",
    "        all merged facts scraped\n",
    "    failed_folders : list\n",
    "        list of failed folders\n",
    "    \"\"\"\n",
    "    all_labels = pd.DataFrame()\n",
    "    all_calc = pd.DataFrame()\n",
    "    all_defn = pd.DataFrame()\n",
    "    all_context = pd.DataFrame()\n",
    "    all_facts = pd.DataFrame()\n",
    "    all_metalinks = pd.DataFrame()\n",
    "    all_merged_facts = pd.DataFrame()\n",
    "    failed_folders = []\n",
    "\n",
    "    for file in filings_to_scrape:\n",
    "        if (file.get('form') != '10-Q' or file.get('form') != '10-K') and file.get('filingDate') < dt.datetime(2009, 1, 1):\n",
    "            continue\n",
    "\n",
    "        accessionNumber = file.get('accessionNumber')\n",
    "        folder_url = file.get('folder_url')\n",
    "        file_url = file.get('file_url')\n",
    "        ticker.scrape_logger.info(\n",
    "            file.get('filingDate').strftime('%Y-%m-%d') + ': ' + folder_url)\n",
    "\n",
    "        soup = ticker.get_file_data(file_url=file_url)\n",
    "\n",
    "        # Scrape facts, context, metalinks\n",
    "        try:\n",
    "            metalinks = ticker.get_metalinks(\n",
    "                folder_url=folder_url + '/MetaLinks.json')\n",
    "            metalinks['accessionNumber'] = accessionNumber\n",
    "            all_metalinks = pd.concat(\n",
    "                [all_metalinks, metalinks], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            ticker.scrape_logger.error(\n",
    "                f'Failed to scrape metalinks for {folder_url}...{e}')\n",
    "            failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                                  error=f'Failed to scrape metalinks for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            facts = ticker.search_facts(soup=soup)\n",
    "            facts['accessionNumber'] = accessionNumber\n",
    "            all_facts = pd.concat([all_facts, facts], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            ticker.scrape_logger.error(\n",
    "                f'Failed to scrape facts for {folder_url}...{e}')\n",
    "            failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                                  error=f'Failed to scrape facts for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "            pass\n",
    "        try:\n",
    "            context = ticker.search_context(soup=soup)\n",
    "            context['accessionNumber'] = accessionNumber\n",
    "            all_context = pd.concat([all_context, context], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            ticker.scrape_logger.error(\n",
    "                f'Failed to scrape context for {folder_url}...{e}')\n",
    "            failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                                  error=f'Failed to scrape context for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "            pass\n",
    "\n",
    "        index_df = ticker.get_filing_folder_index(folder_url=folder_url)\n",
    "\n",
    "        try:  # Scrape labels\n",
    "            labels = ticker.get_elements(folder_url=folder_url, index_df=index_df,\n",
    "                                         scrape_file_extension='_lab').query(\"`xlink:type` == 'resource'\")\n",
    "            labels['xlink:role'] = labels['xlink:role'].str.split(\n",
    "                '/').apply(lambda x: x[-1])\n",
    "            labels['xlink:label'] = labels['xlink:label'].str\\\n",
    "                .replace('(lab_)|(_en-US)', '', regex=True).str\\\n",
    "                .split('_')\\\n",
    "                .apply(lambda x: ':'.join(x[:2]))\\\n",
    "                .str.lower()\n",
    "            labels['accessionNumber'] = accessionNumber\n",
    "            all_labels = pd.concat([all_labels, labels], ignore_index=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            ticker.scrape_logger.error(\n",
    "                f'Failed to scrape labels for {folder_url}...{e}')\n",
    "            failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                                  error=f'Failed to scrape labels for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "            pass\n",
    "\n",
    "        try:  # Scrape calculations\n",
    "            calc = ticker.get_elements(folder_url=folder_url, index_df=index_df,\n",
    "                                       scrape_file_extension='_cal').query(\"`xlink:type` == 'arc'\")\n",
    "            calc['accessionNumber'] = accessionNumber\n",
    "            all_calc = pd.concat([all_calc, calc], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            ticker.scrape_logger.error(\n",
    "                f'Failed to scrape calc for {folder_url}...{e}')\n",
    "            failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                                  error=f'Failed to scrape calc for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "            pass\n",
    "\n",
    "        try:  # Scrape definitions\n",
    "            defn = ticker.get_elements(folder_url=folder_url, index_df=index_df,\n",
    "                                       scrape_file_extension='_def').query(\"`xlink:type` == 'arc'\")\n",
    "            defn['accessionNumber'] = accessionNumber\n",
    "            all_defn = pd.concat([all_defn, defn], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            ticker.scrape_logger.error(\n",
    "                f'Failed to scrape defn for {folder_url}...{e}')\n",
    "            failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                                  error=f'Failed to scrape defn for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "            pass\n",
    "\n",
    "        if len(facts) == 0:\n",
    "            ticker.scrape_logger.info(\n",
    "                f'No facts found for {ticker.ticker}({ticker.cik})-{folder_url}...\\n')\n",
    "            continue\n",
    "\n",
    "        ticker.scrape_logger.info(\n",
    "            f'Merging facts with context and labels. Current facts length: {len(facts)}...')\n",
    "        try:\n",
    "            merged_facts = facts.merge(context, how='left', left_on='contextRef', right_on='contextId')\\\n",
    "                .merge(labels.query(\"`xlink:role` == 'label'\"), how='left', left_on='factName', right_on='xlink:label')\n",
    "            merged_facts = merged_facts.drop(\n",
    "                ['accessionNumber_x', 'accessionNumber_y'], axis=1)\n",
    "            ticker.scrape_logger.info(\n",
    "                f'Successfully merged facts with context and labels. Merged facts length: {len(merged_facts)}...')\n",
    "        except Exception as e:\n",
    "            ticker.scrape_logger.error(\n",
    "                f'Failed to merge facts with context and labels for {folder_url}...{e}')\n",
    "            failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                                  error=f'Failed to merge facts with context and labels for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "            pass\n",
    "\n",
    "        all_merged_facts = pd.concat(\n",
    "            [all_merged_facts, merged_facts], ignore_index=True)\n",
    "        ticker.scrape_logger.info(\n",
    "            f'Successfully scraped {ticker.ticker}({ticker.cik})-{folder_url}...\\n')\n",
    "\n",
    "    all_merged_facts = all_merged_facts.loc[~all_merged_facts['labelText'].isnull(), [\n",
    "        'labelText', 'segment', 'startDate', 'endDate', 'instant', 'value', 'unitRef']]\n",
    "\n",
    "    return all_labels, all_calc, all_defn, all_context, all_facts, all_metalinks, all_merged_facts, failed_folders\n",
    "\n",
    "\n",
    "def clean_values_in_facts(merged_facts: pd.DataFrame):\n",
    "    df = merged_facts.loc[(~merged_facts['value'].str.contains(\n",
    "        '[^0-9\\.\\-]|(^\\d+\\-\\d+\\-\\d+$)')) & (merged_facts['value'] != \"\")].copy()\n",
    "    df['value'] = df['value'].astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_values_in_segment(merged_facts: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Segment column of merged facts is cleaned to remove \"ticker:\" and \"us-gaap:\" prepend, and to split camel case into separate words (e.g. \"us-gaap:RevenuesBeforeTax\" becomes \"Revenues Before Tax\"). \n",
    "\n",
    "    Args:\n",
    "        merged_facts (pd.DataFrame): merged facts data frame from get_filing_facts.\n",
    "\n",
    "    Returns:\n",
    "        merged_facts (pd.DataFrame): merged facts data frame with segment column cleaned\n",
    "    \"\"\"\n",
    "    prepends = [i[0] for i in merged_facts.loc[(merged_facts['segment'].str.contains(':')) & (~merged_facts['segment'].isna())]['segment'].str.extract(r'(.*:)').drop_duplicates().values]\n",
    "    pattern = '|'.join(prepends)\n",
    "\n",
    "    merged_facts['segment'] = merged_facts['segment']\\\n",
    "        .str.replace(pat=pattern, repl='', regex=True)\\\n",
    "        .str.replace(pat=r'([A-Z])', repl=r' \\1', regex=True).str.strip()\n",
    "        # .apply(lambda x: x[-1] if isinstance(x, list) else x)\\\n",
    "\n",
    "    return merged_facts\n",
    "\n",
    "\n",
    "def split_facts_into_start_instant(merged_facts: pd.DataFrame):\n",
    "    \"\"\"Splits facts into start/end and instant\n",
    "\n",
    "    Args:\n",
    "        merged_facts (pd.DataFrame): merged facts data frame from get_filing_facts\n",
    "\n",
    "    Returns:\n",
    "        merged_facts: merged facts data frame without duplicates on the columns labelText, segment, startDate, endDate, instant, value\n",
    "        start_end: start/end facts data frame where startDate and endDate are not null\n",
    "        instant: instant facts data frame where instant is not null\n",
    "    \"\"\"\n",
    "    merged_facts.drop_duplicates(subset=[\n",
    "        'labelText', 'segment', 'startDate', 'endDate', 'instant', 'value'], keep='last', inplace=True)\n",
    "\n",
    "    start_end = merged_facts.dropna(axis=0, subset=['startDate', 'endDate'])[['labelText', 'segment', 'unitRef',\n",
    "                                                                              'startDate', 'endDate', 'value']].sort_values(by=['labelText', 'segment', 'startDate', 'endDate',])\n",
    "    instant = merged_facts.dropna(axis=0, subset=['instant'])[\n",
    "        ['labelText', 'segment', 'unitRef', 'instant', 'value']].sort_values(by=['labelText', 'segment', 'instant',])\n",
    "\n",
    "    return merged_facts, start_end, instant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 23:46:50,820 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/files/company_tickers.json\n",
      "2023-12-19 23:46:51,463 - sec-scraper - INFO - Request successful at URL: https://data.sec.gov/submissions/CIK0001045810.json\n",
      "2023-12-19 23:46:51,918 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/index.json\n",
      "2023-12-19 23:46:53,748 - sec-scraper - INFO - 2009-03-13: https://www.sec.gov/Archives/edgar/data/0001045810/000104581009000013\n",
      "2023-12-19 23:46:54,992 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581009000013/0001045810-09-000013.txt\n",
      "2023-12-19 23:46:55,481 - sec-scraper - INFO - Parsed file data from https://www.sec.gov/Archives/edgar/data/0001045810/000104581009000013/0001045810-09-000013.txt successfully.\n",
      "2023-12-19 23:46:55,483 - sec-scraper - ERROR - Failed to scrape metalinks for https://www.sec.gov/Archives/edgar/data/0001045810/000104581009000013...TickerData.get_metalinks() got an unexpected keyword argument 'folder_url'\n",
      "2023-12-19 23:46:55,894 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581009000013/index.json\n",
      "2023-12-19 23:46:55,899 - sec-scraper - ERROR - Failed to scrape labels for https://www.sec.gov/Archives/edgar/data/0001045810/000104581009000013...single positional indexer is out-of-bounds\n",
      "2023-12-19 23:46:55,900 - sec-scraper - ERROR - Failed to scrape calc for https://www.sec.gov/Archives/edgar/data/0001045810/000104581009000013...single positional indexer is out-of-bounds\n",
      "2023-12-19 23:46:55,902 - sec-scraper - ERROR - Failed to scrape defn for https://www.sec.gov/Archives/edgar/data/0001045810/000104581009000013...single positional indexer is out-of-bounds\n",
      "2023-12-19 23:46:55,902 - sec-scraper - INFO - No facts found for NVDA(0001045810)-https://www.sec.gov/Archives/edgar/data/0001045810/000104581009000013...\n",
      "\n",
      "2023-12-19 23:46:55,903 - sec-scraper - INFO - 2010-03-18: https://www.sec.gov/Archives/edgar/data/0001045810/000104581010000006\n",
      "2023-12-19 23:46:56,654 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581010000006/0001045810-10-000006.txt\n",
      "2023-12-19 23:46:58,901 - sec-scraper - INFO - Parsed file data from https://www.sec.gov/Archives/edgar/data/0001045810/000104581010000006/0001045810-10-000006.txt successfully.\n",
      "2023-12-19 23:46:58,904 - sec-scraper - ERROR - Failed to scrape metalinks for https://www.sec.gov/Archives/edgar/data/0001045810/000104581010000006...TickerData.get_metalinks() got an unexpected keyword argument 'folder_url'\n",
      "2023-12-19 23:46:59,352 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581010000006/index.json\n",
      "2023-12-19 23:46:59,773 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581010000006/nvda-20100131_lab.xml\n",
      "2023-12-19 23:47:00,369 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581010000006/nvda-20100131_cal.xml\n",
      "2023-12-19 23:47:00,802 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581010000006/nvda-20100131_def.xml\n",
      "2023-12-19 23:47:00,811 - sec-scraper - INFO - Merging facts with context and labels. Current facts length: 284...\n",
      "2023-12-19 23:47:00,817 - sec-scraper - INFO - Successfully merged facts with context and labels. Merged facts length: 284...\n",
      "2023-12-19 23:47:00,817 - sec-scraper - INFO - Successfully scraped NVDA(0001045810)-https://www.sec.gov/Archives/edgar/data/0001045810/000104581010000006...\n",
      "\n",
      "2023-12-19 23:47:00,818 - sec-scraper - INFO - 2011-03-16: https://www.sec.gov/Archives/edgar/data/0001045810/000104581011000015\n",
      "2023-12-19 23:47:01,728 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581011000015/0001045810-11-000015.txt\n",
      "2023-12-19 23:47:03,909 - sec-scraper - INFO - Parsed file data from https://www.sec.gov/Archives/edgar/data/0001045810/000104581011000015/0001045810-11-000015.txt successfully.\n",
      "2023-12-19 23:47:03,911 - sec-scraper - ERROR - Failed to scrape metalinks for https://www.sec.gov/Archives/edgar/data/0001045810/000104581011000015...TickerData.get_metalinks() got an unexpected keyword argument 'folder_url'\n",
      "2023-12-19 23:47:04,961 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581011000015/index.json\n",
      "2023-12-19 23:47:05,346 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581011000015/nvda-20110130_lab.xml\n",
      "2023-12-19 23:47:05,813 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581011000015/nvda-20110130_cal.xml\n",
      "2023-12-19 23:47:06,243 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581011000015/nvda-20110130_def.xml\n",
      "2023-12-19 23:47:06,260 - sec-scraper - INFO - Merging facts with context and labels. Current facts length: 931...\n",
      "2023-12-19 23:47:06,265 - sec-scraper - INFO - Successfully merged facts with context and labels. Merged facts length: 931...\n",
      "2023-12-19 23:47:06,274 - sec-scraper - INFO - Successfully scraped NVDA(0001045810)-https://www.sec.gov/Archives/edgar/data/0001045810/000104581011000015...\n",
      "\n",
      "2023-12-19 23:47:06,274 - sec-scraper - INFO - 2012-03-13: https://www.sec.gov/Archives/edgar/data/0001045810/000104581012000013\n",
      "2023-12-19 23:47:07,111 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581012000013/0001045810-12-000013.txt\n",
      "2023-12-19 23:47:09,437 - sec-scraper - INFO - Parsed file data from https://www.sec.gov/Archives/edgar/data/0001045810/000104581012000013/0001045810-12-000013.txt successfully.\n",
      "2023-12-19 23:47:09,439 - sec-scraper - ERROR - Failed to scrape metalinks for https://www.sec.gov/Archives/edgar/data/0001045810/000104581012000013...TickerData.get_metalinks() got an unexpected keyword argument 'folder_url'\n",
      "2023-12-19 23:47:10,239 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581012000013/index.json\n",
      "2023-12-19 23:47:10,733 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581012000013/nvda-20120129_lab.xml\n",
      "2023-12-19 23:47:11,332 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581012000013/nvda-20120129_cal.xml\n",
      "2023-12-19 23:47:11,827 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581012000013/nvda-20120129_def.xml\n",
      "2023-12-19 23:47:11,869 - sec-scraper - INFO - Merging facts with context and labels. Current facts length: 1096...\n",
      "2023-12-19 23:47:11,873 - sec-scraper - INFO - Successfully merged facts with context and labels. Merged facts length: 1096...\n",
      "2023-12-19 23:47:11,886 - sec-scraper - INFO - Successfully scraped NVDA(0001045810)-https://www.sec.gov/Archives/edgar/data/0001045810/000104581012000013...\n",
      "\n",
      "2023-12-19 23:47:11,886 - sec-scraper - INFO - 2013-03-12: https://www.sec.gov/Archives/edgar/data/0001045810/000104581013000008\n",
      "2023-12-19 23:47:12,911 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581013000008/0001045810-13-000008.txt\n",
      "2023-12-19 23:47:15,911 - sec-scraper - INFO - Parsed file data from https://www.sec.gov/Archives/edgar/data/0001045810/000104581013000008/0001045810-13-000008.txt successfully.\n",
      "2023-12-19 23:47:15,913 - sec-scraper - ERROR - Failed to scrape metalinks for https://www.sec.gov/Archives/edgar/data/0001045810/000104581013000008...TickerData.get_metalinks() got an unexpected keyword argument 'folder_url'\n",
      "2023-12-19 23:47:16,699 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581013000008/index.json\n",
      "2023-12-19 23:47:17,294 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581013000008/nvda-20130127_lab.xml\n",
      "2023-12-19 23:47:17,919 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581013000008/nvda-20130127_cal.xml\n",
      "2023-12-19 23:47:18,494 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581013000008/nvda-20130127_def.xml\n",
      "2023-12-19 23:47:18,536 - sec-scraper - INFO - Merging facts with context and labels. Current facts length: 1082...\n",
      "2023-12-19 23:47:18,540 - sec-scraper - INFO - Successfully merged facts with context and labels. Merged facts length: 1082...\n",
      "2023-12-19 23:47:18,552 - sec-scraper - INFO - Successfully scraped NVDA(0001045810)-https://www.sec.gov/Archives/edgar/data/0001045810/000104581013000008...\n",
      "\n",
      "2023-12-19 23:47:18,552 - sec-scraper - INFO - 2014-03-13: https://www.sec.gov/Archives/edgar/data/0001045810/000104581014000030\n",
      "2023-12-19 23:47:19,493 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581014000030/0001045810-14-000030.txt\n",
      "2023-12-19 23:47:22,967 - sec-scraper - INFO - Parsed file data from https://www.sec.gov/Archives/edgar/data/0001045810/000104581014000030/0001045810-14-000030.txt successfully.\n",
      "2023-12-19 23:47:22,969 - sec-scraper - ERROR - Failed to scrape metalinks for https://www.sec.gov/Archives/edgar/data/0001045810/000104581014000030...TickerData.get_metalinks() got an unexpected keyword argument 'folder_url'\n",
      "2023-12-19 23:47:23,635 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581014000030/index.json\n",
      "2023-12-19 23:47:24,065 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581014000030/nvda-20140126_lab.xml\n",
      "2023-12-19 23:47:24,469 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581014000030/nvda-20140126_cal.xml\n",
      "2023-12-19 23:47:25,089 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581014000030/nvda-20140126_def.xml\n",
      "2023-12-19 23:47:25,131 - sec-scraper - INFO - Merging facts with context and labels. Current facts length: 1085...\n",
      "2023-12-19 23:47:25,136 - sec-scraper - INFO - Successfully merged facts with context and labels. Merged facts length: 1085...\n",
      "2023-12-19 23:47:25,151 - sec-scraper - INFO - Successfully scraped NVDA(0001045810)-https://www.sec.gov/Archives/edgar/data/0001045810/000104581014000030...\n",
      "\n",
      "2023-12-19 23:47:25,152 - sec-scraper - INFO - 2015-03-12: https://www.sec.gov/Archives/edgar/data/0001045810/000104581015000036\n",
      "2023-12-19 23:47:25,929 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581015000036/0001045810-15-000036.txt\n",
      "2023-12-19 23:47:27,963 - sec-scraper - INFO - Parsed file data from https://www.sec.gov/Archives/edgar/data/0001045810/000104581015000036/0001045810-15-000036.txt successfully.\n",
      "2023-12-19 23:47:27,964 - sec-scraper - ERROR - Failed to scrape metalinks for https://www.sec.gov/Archives/edgar/data/0001045810/000104581015000036...TickerData.get_metalinks() got an unexpected keyword argument 'folder_url'\n",
      "2023-12-19 23:47:28,668 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581015000036/index.json\n",
      "2023-12-19 23:47:29,306 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581015000036/nvda-20150125_lab.xml\n",
      "2023-12-19 23:47:29,898 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581015000036/nvda-20150125_cal.xml\n",
      "2023-12-19 23:47:30,356 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581015000036/nvda-20150125_def.xml\n",
      "2023-12-19 23:47:30,393 - sec-scraper - INFO - Merging facts with context and labels. Current facts length: 1087...\n",
      "2023-12-19 23:47:30,397 - sec-scraper - INFO - Successfully merged facts with context and labels. Merged facts length: 1087...\n",
      "2023-12-19 23:47:30,415 - sec-scraper - INFO - Successfully scraped NVDA(0001045810)-https://www.sec.gov/Archives/edgar/data/0001045810/000104581015000036...\n",
      "\n",
      "2023-12-19 23:47:30,416 - sec-scraper - INFO - 2016-03-17: https://www.sec.gov/Archives/edgar/data/0001045810/000104581016000205\n",
      "2023-12-19 23:47:31,520 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581016000205/0001045810-16-000205.txt\n",
      "2023-12-19 23:47:33,259 - sec-scraper - INFO - Parsed file data from https://www.sec.gov/Archives/edgar/data/0001045810/000104581016000205/0001045810-16-000205.txt successfully.\n",
      "2023-12-19 23:47:33,261 - sec-scraper - ERROR - Failed to scrape metalinks for https://www.sec.gov/Archives/edgar/data/0001045810/000104581016000205...TickerData.get_metalinks() got an unexpected keyword argument 'folder_url'\n",
      "2023-12-19 23:47:34,453 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581016000205/index.json\n",
      "2023-12-19 23:47:34,987 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581016000205/nvda-20160131_lab.xml\n",
      "2023-12-19 23:47:35,434 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581016000205/nvda-20160131_cal.xml\n",
      "2023-12-19 23:47:36,205 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581016000205/nvda-20160131_def.xml\n",
      "2023-12-19 23:47:36,244 - sec-scraper - INFO - Merging facts with context and labels. Current facts length: 1087...\n",
      "2023-12-19 23:47:36,250 - sec-scraper - INFO - Successfully merged facts with context and labels. Merged facts length: 1087...\n",
      "2023-12-19 23:47:36,271 - sec-scraper - INFO - Successfully scraped NVDA(0001045810)-https://www.sec.gov/Archives/edgar/data/0001045810/000104581016000205...\n",
      "\n",
      "2023-12-19 23:47:36,271 - sec-scraper - INFO - 2017-03-01: https://www.sec.gov/Archives/edgar/data/0001045810/000104581017000027\n",
      "2023-12-19 23:47:37,035 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581017000027/0001045810-17-000027.txt\n",
      "2023-12-19 23:47:38,976 - sec-scraper - INFO - Parsed file data from https://www.sec.gov/Archives/edgar/data/0001045810/000104581017000027/0001045810-17-000027.txt successfully.\n",
      "2023-12-19 23:47:38,978 - sec-scraper - ERROR - Failed to scrape metalinks for https://www.sec.gov/Archives/edgar/data/0001045810/000104581017000027...TickerData.get_metalinks() got an unexpected keyword argument 'folder_url'\n",
      "2023-12-19 23:47:39,722 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581017000027/index.json\n",
      "2023-12-19 23:47:40,272 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581017000027/nvda-20170129_lab.xml\n",
      "2023-12-19 23:47:40,883 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581017000027/nvda-20170129_cal.xml\n",
      "2023-12-19 23:47:41,381 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581017000027/nvda-20170129_def.xml\n",
      "2023-12-19 23:47:41,418 - sec-scraper - INFO - Merging facts with context and labels. Current facts length: 1131...\n",
      "2023-12-19 23:47:41,424 - sec-scraper - INFO - Successfully merged facts with context and labels. Merged facts length: 1131...\n",
      "2023-12-19 23:47:41,448 - sec-scraper - INFO - Successfully scraped NVDA(0001045810)-https://www.sec.gov/Archives/edgar/data/0001045810/000104581017000027...\n",
      "\n",
      "2023-12-19 23:47:41,448 - sec-scraper - INFO - 2018-02-28: https://www.sec.gov/Archives/edgar/data/0001045810/000104581018000010\n",
      "2023-12-19 23:47:42,186 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581018000010/0001045810-18-000010.txt\n",
      "2023-12-19 23:47:43,883 - sec-scraper - INFO - Parsed file data from https://www.sec.gov/Archives/edgar/data/0001045810/000104581018000010/0001045810-18-000010.txt successfully.\n",
      "2023-12-19 23:47:43,885 - sec-scraper - ERROR - Failed to scrape metalinks for https://www.sec.gov/Archives/edgar/data/0001045810/000104581018000010...TickerData.get_metalinks() got an unexpected keyword argument 'folder_url'\n",
      "2023-12-19 23:47:44,431 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581018000010/index.json\n",
      "2023-12-19 23:47:44,998 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581018000010/nvda-20180128_lab.xml\n",
      "2023-12-19 23:47:45,665 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581018000010/nvda-20180128_cal.xml\n",
      "2023-12-19 23:47:46,179 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581018000010/nvda-20180128_def.xml\n",
      "2023-12-19 23:47:46,203 - sec-scraper - INFO - Merging facts with context and labels. Current facts length: 1083...\n",
      "2023-12-19 23:47:46,209 - sec-scraper - INFO - Successfully merged facts with context and labels. Merged facts length: 1083...\n",
      "2023-12-19 23:47:46,232 - sec-scraper - INFO - Successfully scraped NVDA(0001045810)-https://www.sec.gov/Archives/edgar/data/0001045810/000104581018000010...\n",
      "\n",
      "2023-12-19 23:47:46,232 - sec-scraper - INFO - 2019-02-21: https://www.sec.gov/Archives/edgar/data/0001045810/000104581019000023\n",
      "2023-12-19 23:47:46,993 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581019000023/0001045810-19-000023.txt\n",
      "2023-12-19 23:47:48,742 - sec-scraper - INFO - Parsed file data from https://www.sec.gov/Archives/edgar/data/0001045810/000104581019000023/0001045810-19-000023.txt successfully.\n",
      "2023-12-19 23:47:48,744 - sec-scraper - ERROR - Failed to scrape metalinks for https://www.sec.gov/Archives/edgar/data/0001045810/000104581019000023...TickerData.get_metalinks() got an unexpected keyword argument 'folder_url'\n",
      "2023-12-19 23:47:49,417 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581019000023/index.json\n",
      "2023-12-19 23:47:49,936 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581019000023/nvda-20190127_lab.xml\n",
      "2023-12-19 23:47:50,447 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581019000023/nvda-20190127_cal.xml\n",
      "2023-12-19 23:47:50,964 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581019000023/nvda-20190127_def.xml\n",
      "2023-12-19 23:47:50,997 - sec-scraper - INFO - Merging facts with context and labels. Current facts length: 1096...\n",
      "2023-12-19 23:47:51,001 - sec-scraper - INFO - Successfully merged facts with context and labels. Merged facts length: 1096...\n",
      "2023-12-19 23:47:51,028 - sec-scraper - INFO - Successfully scraped NVDA(0001045810)-https://www.sec.gov/Archives/edgar/data/0001045810/000104581019000023...\n",
      "\n",
      "2023-12-19 23:47:51,028 - sec-scraper - INFO - 2020-02-20: https://www.sec.gov/Archives/edgar/data/0001045810/000104581020000010\n",
      "2023-12-19 23:47:51,876 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581020000010/0001045810-20-000010.txt\n",
      "2023-12-19 23:47:53,281 - sec-scraper - INFO - Parsed file data from https://www.sec.gov/Archives/edgar/data/0001045810/000104581020000010/0001045810-20-000010.txt successfully.\n",
      "2023-12-19 23:47:53,283 - sec-scraper - ERROR - Failed to scrape metalinks for https://www.sec.gov/Archives/edgar/data/0001045810/000104581020000010...TickerData.get_metalinks() got an unexpected keyword argument 'folder_url'\n",
      "2023-12-19 23:47:53,948 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581020000010/index.json\n",
      "2023-12-19 23:47:54,515 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581020000010/nvda-20200126_lab.xml\n",
      "2023-12-19 23:47:55,041 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581020000010/nvda-20200126_cal.xml\n",
      "2023-12-19 23:47:55,591 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581020000010/nvda-20200126_def.xml\n",
      "2023-12-19 23:47:55,621 - sec-scraper - INFO - Merging facts with context and labels. Current facts length: 1156...\n",
      "2023-12-19 23:47:55,627 - sec-scraper - INFO - Successfully merged facts with context and labels. Merged facts length: 1156...\n",
      "2023-12-19 23:47:55,656 - sec-scraper - INFO - Successfully scraped NVDA(0001045810)-https://www.sec.gov/Archives/edgar/data/0001045810/000104581020000010...\n",
      "\n",
      "2023-12-19 23:47:55,657 - sec-scraper - INFO - 2021-02-26: https://www.sec.gov/Archives/edgar/data/0001045810/000104581021000010\n",
      "2023-12-19 23:47:56,412 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581021000010/0001045810-21-000010.txt\n",
      "2023-12-19 23:47:58,228 - sec-scraper - INFO - Parsed file data from https://www.sec.gov/Archives/edgar/data/0001045810/000104581021000010/0001045810-21-000010.txt successfully.\n",
      "2023-12-19 23:47:58,229 - sec-scraper - ERROR - Failed to scrape metalinks for https://www.sec.gov/Archives/edgar/data/0001045810/000104581021000010...TickerData.get_metalinks() got an unexpected keyword argument 'folder_url'\n",
      "2023-12-19 23:47:58,991 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581021000010/index.json\n",
      "2023-12-19 23:47:59,663 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581021000010/nvda-20210131_lab.xml\n",
      "2023-12-19 23:48:00,172 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581021000010/nvda-20210131_cal.xml\n",
      "2023-12-19 23:48:00,976 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581021000010/nvda-20210131_def.xml\n",
      "2023-12-19 23:48:01,029 - sec-scraper - INFO - Merging facts with context and labels. Current facts length: 1176...\n",
      "2023-12-19 23:48:01,034 - sec-scraper - INFO - Successfully merged facts with context and labels. Merged facts length: 1176...\n",
      "2023-12-19 23:48:01,069 - sec-scraper - INFO - Successfully scraped NVDA(0001045810)-https://www.sec.gov/Archives/edgar/data/0001045810/000104581021000010...\n",
      "\n",
      "2023-12-19 23:48:01,069 - sec-scraper - INFO - 2022-03-18: https://www.sec.gov/Archives/edgar/data/0001045810/000104581022000036\n",
      "2023-12-19 23:48:01,812 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581022000036/0001045810-22-000036.txt\n",
      "2023-12-19 23:48:03,472 - sec-scraper - INFO - Parsed file data from https://www.sec.gov/Archives/edgar/data/0001045810/000104581022000036/0001045810-22-000036.txt successfully.\n",
      "2023-12-19 23:48:03,473 - sec-scraper - ERROR - Failed to scrape metalinks for https://www.sec.gov/Archives/edgar/data/0001045810/000104581022000036...TickerData.get_metalinks() got an unexpected keyword argument 'folder_url'\n",
      "2023-12-19 23:48:04,018 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581022000036/index.json\n",
      "2023-12-19 23:48:04,473 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581022000036/nvda-20220130_lab.xml\n",
      "2023-12-19 23:48:05,013 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581022000036/nvda-20220130_cal.xml\n",
      "2023-12-19 23:48:05,431 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581022000036/nvda-20220130_def.xml\n",
      "2023-12-19 23:48:05,479 - sec-scraper - INFO - Merging facts with context and labels. Current facts length: 1175...\n",
      "2023-12-19 23:48:05,484 - sec-scraper - INFO - Successfully merged facts with context and labels. Merged facts length: 1175...\n",
      "2023-12-19 23:48:05,520 - sec-scraper - INFO - Successfully scraped NVDA(0001045810)-https://www.sec.gov/Archives/edgar/data/0001045810/000104581022000036...\n",
      "\n",
      "2023-12-19 23:48:05,521 - sec-scraper - INFO - 2023-02-24: https://www.sec.gov/Archives/edgar/data/0001045810/000104581023000017\n",
      "2023-12-19 23:48:06,350 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581023000017/0001045810-23-000017.txt\n",
      "2023-12-19 23:48:07,993 - sec-scraper - INFO - Parsed file data from https://www.sec.gov/Archives/edgar/data/0001045810/000104581023000017/0001045810-23-000017.txt successfully.\n",
      "2023-12-19 23:48:07,994 - sec-scraper - ERROR - Failed to scrape metalinks for https://www.sec.gov/Archives/edgar/data/0001045810/000104581023000017...TickerData.get_metalinks() got an unexpected keyword argument 'folder_url'\n",
      "2023-12-19 23:48:08,566 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581023000017/index.json\n",
      "2023-12-19 23:48:08,952 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581023000017/nvda-20230129_lab.xml\n",
      "2023-12-19 23:48:09,429 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581023000017/nvda-20230129_cal.xml\n",
      "2023-12-19 23:48:09,839 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001045810/000104581023000017/nvda-20230129_def.xml\n",
      "2023-12-19 23:48:09,875 - sec-scraper - INFO - Merging facts with context and labels. Current facts length: 1247...\n",
      "2023-12-19 23:48:09,880 - sec-scraper - INFO - Successfully merged facts with context and labels. Merged facts length: 1247...\n",
      "2023-12-19 23:48:09,916 - sec-scraper - INFO - Successfully scraped NVDA(0001045810)-https://www.sec.gov/Archives/edgar/data/0001045810/000104581023000017...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ticker = TickerData('NVDA')\n",
    "\n",
    "start_date = dt.datetime(2009, 1, 1) # after XBRL implementation\n",
    "query = {\n",
    "    'cik': ticker.cik,\n",
    "    'form': {'$in': ['10-K']},\n",
    "    'filingDate': {'$gte': start_date},\n",
    "}\n",
    "filings_to_scrape = [i for i in mongo.tickerfilings.find(query).sort('filingDate', 1)]\n",
    "\n",
    "all_labels, all_calc, all_defn, all_context, all_facts, all_metalinks, all_merged_facts, failed_folders = get_filing_facts(ticker=ticker, filings_to_scrape=filings_to_scrape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all_labels, all_calc, all_defn to xlsx on different sheets\n",
    "with pd.ExcelWriter(f'././data/{ticker.ticker}_all_data.xlsx') as writer:\n",
    "    all_facts.to_excel(writer, sheet_name='facts', index=False)\n",
    "    all_context.to_excel(writer, sheet_name='context', index=False)\n",
    "    all_labels.to_excel(writer, sheet_name='labels', index=False)\n",
    "    all_merged_facts.to_excel(writer, sheet_name='merged_facts', index=False)\n",
    "    all_calc.to_excel(writer, sheet_name='calc', index=False)\n",
    "    all_defn.to_excel(writer, sheet_name='defn', index=False)\n",
    "    all_metalinks.to_excel(writer, sheet_name='metalinks', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = clean_values_in_facts(all_merged_facts)\n",
    "final_df = clean_values_in_segment(final_df, ticker.ticker)\n",
    "# final_df, start_end, instant = split_facts_into_start_instant(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labelText</th>\n",
       "      <th>segment</th>\n",
       "      <th>startDate</th>\n",
       "      <th>endDate</th>\n",
       "      <th>instant</th>\n",
       "      <th>value</th>\n",
       "      <th>unitRef</th>\n",
       "      <th>period</th>\n",
       "      <th>Months Ended</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accounts Payable, Current</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1.513300e+10</td>\n",
       "      <td>usd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accounts Payable, Current</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1.645900e+10</td>\n",
       "      <td>usd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Accounts Receivable, Net, Current</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>4.767000e+09</td>\n",
       "      <td>usd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Accounts Receivable, Net, Current</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1.300000e+09</td>\n",
       "      <td>usd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Accounts Receivable, Net, Current</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1.700000e+09</td>\n",
       "      <td>usd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10831</th>\n",
       "      <td>Depreciation</td>\n",
       "      <td>Amazon Web Services Segment Member</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1.065300e+10</td>\n",
       "      <td>usd</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Twelve Months Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10832</th>\n",
       "      <td>Depreciation</td>\n",
       "      <td>Amazon Web Services Segment Member</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>NaT</td>\n",
       "      <td>9.876000e+09</td>\n",
       "      <td>usd</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Twelve Months Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10833</th>\n",
       "      <td>Depreciation</td>\n",
       "      <td>None</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1.623900e+10</td>\n",
       "      <td>usd</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Twelve Months Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10834</th>\n",
       "      <td>Depreciation</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2.290900e+10</td>\n",
       "      <td>usd</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Twelve Months Ended</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10835</th>\n",
       "      <td>Depreciation</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2.492400e+10</td>\n",
       "      <td>usd</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Twelve Months Ended</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9336 rows  9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               labelText                             segment  \\\n",
       "0              Accounts Payable, Current                                 NaN   \n",
       "1              Accounts Payable, Current                                 NaN   \n",
       "2      Accounts Receivable, Net, Current                                 NaN   \n",
       "3      Accounts Receivable, Net, Current                                 NaN   \n",
       "4      Accounts Receivable, Net, Current                                 NaN   \n",
       "...                                  ...                                 ...   \n",
       "10831                       Depreciation  Amazon Web Services Segment Member   \n",
       "10832                       Depreciation  Amazon Web Services Segment Member   \n",
       "10833                       Depreciation                                None   \n",
       "10834                       Depreciation                                None   \n",
       "10835                       Depreciation                                None   \n",
       "\n",
       "       startDate    endDate instant         value unitRef  period  \\\n",
       "0            NaT        NaT     NaT  1.513300e+10     usd     NaN   \n",
       "1            NaT        NaT     NaT  1.645900e+10     usd     NaN   \n",
       "2            NaT        NaT     NaT  4.767000e+09     usd     NaN   \n",
       "3            NaT        NaT     NaT  1.300000e+09     usd     NaN   \n",
       "4            NaT        NaT     NaT  1.700000e+09     usd     NaN   \n",
       "...          ...        ...     ...           ...     ...     ...   \n",
       "10831 2021-01-01 2021-12-31     NaT  1.065300e+10     usd    12.0   \n",
       "10832 2022-01-01 2022-12-31     NaT  9.876000e+09     usd    12.0   \n",
       "10833 2020-01-01 2020-12-31     NaT  1.623900e+10     usd    12.0   \n",
       "10834 2021-01-01 2021-12-31     NaT  2.290900e+10     usd    12.0   \n",
       "10835 2022-01-01 2022-12-31     NaT  2.492400e+10     usd    12.0   \n",
       "\n",
       "              Months Ended  \n",
       "0                     None  \n",
       "1                     None  \n",
       "2                     None  \n",
       "3                     None  \n",
       "4                     None  \n",
       "...                    ...  \n",
       "10831  Twelve Months Ended  \n",
       "10832  Twelve Months Ended  \n",
       "10833  Twelve Months Ended  \n",
       "10834  Twelve Months Ended  \n",
       "10835  Twelve Months Ended  \n",
       "\n",
       "[9336 rows x 9 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_monthly_period(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['period'] = pd.to_timedelta(df['endDate'] - df['startDate']).dt.days / 30.25\n",
    "    df['period'] = df['period'].round(0)\n",
    "    df['Months Ended'] = np.select(\n",
    "        [\n",
    "            df['period'] == 3,\n",
    "            df['period'] == 6,\n",
    "            df['period'] == 9,\n",
    "            df['period'] == 12,\n",
    "        ],\n",
    "        [\n",
    "            \"Three Months Ended\",\n",
    "            \"Six Months Ended\",\n",
    "            \"Nine Months Ended\",\n",
    "            \"Twelve Months Ended\",\n",
    "        ],\n",
    "        default=None\n",
    "    )\n",
    "    return df\n",
    "\n",
    "get_monthly_period(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec = SECData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = TickerData(ticker='V')\n",
    "soup = symbol.get_file_data(symbol.filings.loc[symbol.filings['form'] == '10-K', 'file_url'].iloc[0])\n",
    "\n",
    "contexts = symbol.search_tags(soup, '^context$')\n",
    "dict_list = []\n",
    "columns = {'contextId': str, 'entity': str, 'segment': str,\n",
    "            'startDate': 'datetime64[ns]', 'endDate': 'datetime64[ns]', 'instant': 'datetime64[ns]'}\n",
    "for tag in contexts:\n",
    "    temp_dict = {}\n",
    "    temp_dict['contextId'] = tag.attrs.get('id')\n",
    "    temp_dict['entity'] = tag.find(\"entity\").text.split()[\n",
    "        0] if tag.find(\"entity\").text is not None else None\n",
    "    temp_dict['segment'] = tag.find(\"segment\").text.strip(\n",
    "    ) if tag.find(\"segment\") is not None else None\n",
    "    temp_dict['startDate'] = tag.find(\"startdate\").text if tag.find(\n",
    "        \"startdate\") is not None else None\n",
    "    temp_dict['endDate'] = tag.find(\"enddate\").text if tag.find(\n",
    "        \"enddate\") is not None else None\n",
    "    temp_dict['instant'] = tag.find(\"instant\").text if tag.find(\n",
    "        \"instant\") is not None else None\n",
    "    dict_list.append(temp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse using GPT (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = symbol.search_context(soup)[0]\n",
    "data = {\n",
    "    'id': context['id'],\n",
    "    'entity': {\n",
    "        'identifier': {\n",
    "            'scheme': context.find('identifier')['scheme'],\n",
    "            'value': context.find('identifier').text\n",
    "        }\n",
    "    },\n",
    "    'period': {\n",
    "        'startDate': context.find('startdate').text,\n",
    "        'endDate': context.find('enddate').text\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import XMLOutputParser\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessage,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import json\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "parser = XMLOutputParser(tags=['id', 'entity', 'period'])\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assistant that parses XML files for a company's financial statements from the SEC Edgar database.\"\n",
    "                \"The XML content will be provided by the user.\"\n",
    "                \"You will parse the output and return it in the json format.\"\n",
    "                \"{format_instructions}\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{xml}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "context_list = []\n",
    "total_cost = 0\n",
    "total_tokens = 0\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "with trange(len(contexts[:]), desc='Scraping contexts...') as t:\n",
    "    for i in t:\n",
    "        with get_openai_callback() as cb:\n",
    "            t.set_postfix(context=contexts[i].attrs.get('id'))\n",
    "            output = llm(template.format_messages(format_instructions=parser.get_format_instructions(), xml=contexts[i]))\n",
    "            total_cost += cb.total_cost\n",
    "            total_tokens += cb.total_tokens\n",
    "            context_list.append(json.loads(output.content))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance-dashboard-kernel",
   "language": "python",
   "name": "finance-dashboard-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
