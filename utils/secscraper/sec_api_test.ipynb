{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test SEC API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class MyLogger:\n",
    "    def __init__(self, name: str = __name__, level: str = 'debug', log_file: str = 'logs.log'):\n",
    "        # Initialize logger\n",
    "        self.logging_level = logging.DEBUG if level == 'debug' else logging.INFO\n",
    "        self.scrape_logger = logging.getLogger(name)\n",
    "        self.scrape_logger.setLevel(self.logging_level)\n",
    "\n",
    "        # Check if the self.scrape_logger already has handlers to avoid duplicate logging.\n",
    "        if not self.scrape_logger.hasHandlers():\n",
    "            # Create a file handler\n",
    "            file_handler = logging.FileHandler(log_file, mode='a')\n",
    "            file_handler.setLevel(self.logging_level)\n",
    "\n",
    "            # Create a stream handler\n",
    "            stream_handler = logging.StreamHandler()\n",
    "            stream_handler.setLevel(self.logging_level)\n",
    "\n",
    "            # Create a logging format\n",
    "            formatter = logging.Formatter(\n",
    "                '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            file_handler.setFormatter(formatter)\n",
    "            stream_handler.setFormatter(formatter)\n",
    "\n",
    "            # Add the handlers to the self.scrape_logger\n",
    "            self.scrape_logger.addHandler(file_handler)\n",
    "            self.scrape_logger.addHandler(stream_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in libraries\n",
    "import logging\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "# Third-party libraries\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "def convert_keys_to_lowercase(d):\n",
    "    \"\"\"Recursively convert all keys in a dictionary to lowercase.\n",
    "\n",
    "    Args:\n",
    "        d (dict): Dictionary to convert\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with all keys converted to lowercase\n",
    "    \"\"\"\n",
    "    new_dict = {}\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            v = convert_keys_to_lowercase(v)\n",
    "        new_key = re.sub(r'[^a-zA-Z0-9]', '', k.lower())\n",
    "        new_dict[new_key] = v\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "def indexify_url(folder_url: str) -> str:\n",
    "    \"\"\"Converts url to index url.\n",
    "\n",
    "    Args:\n",
    "        url (str): url to convert to index url\n",
    "\n",
    "    Returns:\n",
    "        str: index url\n",
    "    \"\"\"\n",
    "    return folder_url + '/index.json'\n",
    "\n",
    "\n",
    "class SearchStrategy(ABC):\n",
    "    @abstractmethod\n",
    "    def get_pattern(self) -> str:\n",
    "        pass\n",
    "\n",
    "\n",
    "class ContextSearchStrategy(SearchStrategy):\n",
    "    def get_pattern(self) -> str:\n",
    "        return '^context$'\n",
    "\n",
    "\n",
    "class LinkLabelSearchStrategy(SearchStrategy):\n",
    "    def get_pattern(self) -> str:\n",
    "        return '^link:label$'\n",
    "\n",
    "\n",
    "class FactSearchStrategy(SearchStrategy):\n",
    "    def get_pattern(self) -> str:\n",
    "        return '^us-gaap:'\n",
    "\n",
    "\n",
    "class SECData(MyLogger):\n",
    "    \"\"\"Class to retrieve data from SEC Edgar database.\n",
    "\n",
    "    Args:\n",
    "        requester_name (str): Name of the requester\n",
    "        requester_email (str): Email of the requester\n",
    "        taxonomy (str): us-gaap, ifrs-full, dei, or srt\n",
    "\n",
    "    Raises:\n",
    "        Exception: If taxonomy is not one of the following: us-gaap, ifrs-full, dei, or srt\n",
    "\n",
    "    Attributes:\n",
    "        BASE_API_URL (str): Base url for SEC Edgar database\n",
    "        US_GAAP_TAXONOMY_URL (str): URL for us-gaap taxonomy\n",
    "        ALLOWED_TAXONOMIES (list): List of allowed taxonomies\n",
    "        headers (dict): Headers to be used for API calls\n",
    "        cik (DataFrame): DataFrame containing CIK and ticker\n",
    "        tags (list): List of tags in us-gaap taxonomy\n",
    "        taxonomy (str): us-gaap, ifrs-full, dei, or srt\n",
    "\n",
    "    Methods:\n",
    "        get_cik_list: Retrieves the full list of CIK available from SEC database.\n",
    "        get_ticker_cik: Get a specific ticker's CIK number. \n",
    "        get_usgaap_tags: Get the list of tags in us-gaap taxonomy.\n",
    "        get_submissions: Retrieves the list of submissions for a specific CIK.\n",
    "        get_company_concept: Retrieves the XBRL disclosures from a single company (CIK) \n",
    "            and concept (a taxonomy and tag) into a single JSON file.\n",
    "        get_company_facts: Retrieves the XBRL disclosures from a single company (CIK) \n",
    "            into a single JSON file.\n",
    "        get_frames: Retrieves one fact for each reporting entity that is last filed that most closely fits the calendrical period requested.\n",
    "    \"\"\"\n",
    "\n",
    "    BASE_API_URL = \"https://data.sec.gov/\"\n",
    "    BASE_SEC_URL = \"https://www.sec.gov/\"\n",
    "    BASE_DIRECTORY_URL = \"https://www.sec.gov/Archives/edgar/data/\"\n",
    "    SIC_LIST_URL = \"https://www.sec.gov/corpfin/division-of-corporation-finance-standard-industrial-classification-sic-code-list\"\n",
    "    US_GAAP_TAXONOMY_URL = \"http://xbrl.fasb.org/us-gaap/2024/elts/us-gaap-2024.xsd\"\n",
    "    SRT_TAXONOMY_URL = \"http://xbrl.fasb.org/srt/2024/elts/srt-std-2024.xsd\"\n",
    "    ALLOWED_TAXONOMIES = {'us-gaap', 'ifrs-full', 'dei', 'srt'}\n",
    "    INDEX_EXTENSION = {'-index.html', '-index-headers.html'}\n",
    "    DIRECTORY_INDEX = {'index.json', 'index.xml', 'index.html'}\n",
    "    FILE_EXTENSIONS = {'.xsd', '.htm', '_cal.xml',\n",
    "                       '_def.xml', '_lab.xml', '_pre.xml', '_htm.xml', '.xml'}\n",
    "\n",
    "    SCRAPE_FILE_EXTENSIONS = {'_lab', '_def', '_pre', '_cal'}\n",
    "\n",
    "    def __init__(self, requester_company: str = 'Financial API', requester_name: str = 'API Caller', requester_email: str = 'apicaller@gmail.com', taxonomy: str = 'us-gaap',):\n",
    "        super().__init__(name='sec-scraper', level='debug', log_file='././logs.log')\n",
    "\n",
    "        self.requester_company = requester_company\n",
    "        self.requester_name = requester_name\n",
    "        self.requester_email = requester_email\n",
    "        self.sec_headers = {\"User-Agent\": f\"{requester_company} {requester_name} {requester_email}\",\n",
    "                            \"Accept-Encoding\": \"gzip, deflate\",\n",
    "                            \"Host\": \"www.sec.gov\"}\n",
    "        self.sec_data_headers = {\"User-Agent\": f\"{requester_company} {requester_name} {requester_email}\",\n",
    "                                 \"Accept-Encoding\": \"gzip, deflate\",\n",
    "                                 \"Host\": \"data.sec.gov\"}\n",
    "        self._cik_list = None\n",
    "        self._us_gaap_tags = None\n",
    "        self._srt_tags = None\n",
    "        if taxonomy not in self.ALLOWED_TAXONOMIES:\n",
    "            raise ValueError(\n",
    "                f\"Taxonomy {taxonomy} is not supported. Please use one of the following taxonomies: {self.ALLOWED_TAXONOMIES}\")\n",
    "        self.taxonomy = taxonomy\n",
    "\n",
    "    @property\n",
    "    def cik_list(self,):\n",
    "        if self._cik_list is None:\n",
    "            self._cik_list = self.get_cik_list()\n",
    "        return self._cik_list\n",
    "\n",
    "    @property\n",
    "    def us_gaap_tags(self,):\n",
    "        if self._us_gaap_tags is None:\n",
    "            self._us_gaap_tags = self.get_tags(xsd_url=self.US_GAAP_TAXONOMY_URL)\n",
    "        return self._us_gaap_tags\n",
    "    \n",
    "    @property\n",
    "    def srt_tags(self,):\n",
    "        if self._srt_tags is None:\n",
    "            self._srt_tags = self.get_tags(xsd_url=self.SRT_TAXONOMY_URL)\n",
    "        return self._srt_tags\n",
    "\n",
    "    @sleep_and_retry\n",
    "    @limits(calls=10, period=1)\n",
    "    def rate_limited_request(self, url: str, headers: dict):\n",
    "        \"\"\"Rate limited request to SEC Edgar database.\n",
    "\n",
    "        Args:\n",
    "            url (str): URL to retrieve data from\n",
    "            headers (dict): Headers to be used for API calls\n",
    "\n",
    "        Returns:\n",
    "            response: Response from API call\n",
    "        \"\"\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code != 200:\n",
    "            self.scrape_logger.error(f'''Request failed at URL: {url}''')\n",
    "        else:\n",
    "            self.scrape_logger.info(f'''Request successful at URL: {url}''')\n",
    "        return response\n",
    "\n",
    "    def get_cik_list(self):\n",
    "        \"\"\"Retrieves the full list of CIK available from SEC database.\n",
    "\n",
    "        Raises:\n",
    "            Exception: On failure to retrieve CIK list\n",
    "\n",
    "        Returns:\n",
    "            cik_df: DataFrame containing CIK and ticker\n",
    "        \"\"\"\n",
    "        url = r\"https://www.sec.gov/files/company_tickers.json\"\n",
    "        cik_raw = self.rate_limited_request(url, self.sec_headers)\n",
    "        cik_json = cik_raw.json()\n",
    "        cik_df = pd.DataFrame.from_dict(cik_json).T\n",
    "        return cik_df\n",
    "\n",
    "    def get_ticker_cik(self, ticker: str,):\n",
    "        \"\"\"Get a specific ticker's CIK number. \n",
    "        CIK########## is the entity's 10-digit Central Index Key (CIK).\n",
    "\n",
    "        Args:\n",
    "            ticker (str): public ticker symbol of the company\n",
    "\n",
    "        Returns:\n",
    "            cik: CIK number of the company excluding the leading 'CIK'\n",
    "        \"\"\"\n",
    "        ticker_cik = self.cik_list.query(\n",
    "            f\"ticker == '{ticker.upper()}'\")['cik_str']\n",
    "        cik = f\"{ticker_cik.iloc[0]:010d}\"\n",
    "        return cik\n",
    "\n",
    "    def get_tags(self, xsd_url: str = US_GAAP_TAXONOMY_URL):\n",
    "        \"\"\"Get the list of tags (elements) in us-gaap taxonomy or provide a different xsd_url to get tags from a different taxonomy.\n",
    "\n",
    "        Returns:\n",
    "            list of tags\n",
    "        \"\"\"\n",
    "        url = requests.get(xsd_url).content\n",
    "        us_gaap_df = pd.DataFrame([element.attrs for element in BeautifulSoup(url, 'lxml').find_all('xs:element')])\n",
    "\n",
    "        return us_gaap_df\n",
    "\n",
    "    def get_submissions(self, cik: str = None, submission_file: str = None) -> dict:\n",
    "        if cik is not None:\n",
    "            url = f\"{self.BASE_API_URL}submissions/CIK{cik}.json\"\n",
    "        elif submission_file is not None:\n",
    "            url = f\"{self.BASE_API_URL}submissions/{submission_file}\"\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Please provide either a CIK number or a submission file.\")\n",
    "        response = self.rate_limited_request(\n",
    "            url, headers=self.sec_data_headers)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\n",
    "                f\"Failed to retrieve submissions. Status code: {response.status_code}\")\n",
    "        data = json.loads(response.text)\n",
    "        return data\n",
    "\n",
    "    def get_company_concept(self, cik: str, tag: str, taxonomy: str = 'us-gaap',):\n",
    "        \"\"\"The company-concept API returns all the XBRL disclosures from a single company (CIK) \n",
    "        and concept (a taxonomy and tag) into a single JSON file, with a separate array of facts \n",
    "        for each units on measure that the company has chosen to disclose \n",
    "        (e.g. net profits reported in U.S. dollars and in Canadian dollars).\n",
    "\n",
    "        Args:\n",
    "            cik (str): CIK number of the company. Get the list using self.cik\n",
    "            taxonomy (str): us-gaap, ifrs-full, dei, or srt\n",
    "            tag (str): taxonomy tag (e.g. Revenue, AccountsPayableCurrent). See full list from https://xbrl.fasb.org/us-gaap/2023/elts/us-gaap-2023.xsd\n",
    "\n",
    "        Raises:\n",
    "            Exception: On failure to retrieve company concept either due to invalid CIK, taxonomy, or tag\n",
    "\n",
    "        Returns:\n",
    "            data: JSON file containing all the XBRL disclosures from a single company (CIK)\n",
    "        \"\"\"\n",
    "        url = f\"{self.BASE_API_URL}api/xbrl/companyconcept/CIK{cik}/{taxonomy}/{tag}.json\"\n",
    "        response = self.rate_limited_request(\n",
    "            url, headers=self.sec_data_headers)\n",
    "        data = json.loads(response.text)\n",
    "        return data\n",
    "\n",
    "    def get_company_facts(self, cik):\n",
    "        url = f\"{self.BASE_API_URL}api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "        response = self.rate_limited_request(\n",
    "            url, headers=self.sec_data_headers)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(\n",
    "                f\"Failed to retrieve company facts for CIK {cik}. Status code: {response.status_code}\")\n",
    "        data = json.loads(response.text)\n",
    "        return data\n",
    "\n",
    "    def get_frames(self, taxonomy, tag, unit, period):\n",
    "        \"\"\"The xbrl/frames API aggregates one fact for each reporting entity that is last filed that most closely fits the calendrical period requested. \n",
    "        This API supports for annual, quarterly and instantaneous data: https://data.sec.gov/api/xbrl/frames/us-gaap/AccountsPayableCurrent/USD/CY2019Q1I.json\n",
    "\n",
    "        Args:\n",
    "            taxonomy (str): us-gaap, ifrs-full, dei, or srt\n",
    "            tag (str): taxonomy tag (e.g. Revenue, AccountsPayableCurrent). See full list from https://xbrl.fasb.org/us-gaap/2023/elts/us-gaap-2023.xsd\n",
    "            unit (str): USD, USD-per-shares, etc.\n",
    "            period (str): CY#### for annual data (duration 365 days +/- 30 days), CY####Q# for quarterly data (duration 91 days +/- 30 days), CY####Q#I for instantaneous data\n",
    "\n",
    "        Raises:\n",
    "            Exception: (placeholder)\n",
    "\n",
    "        Returns:\n",
    "            data: json formatted response\n",
    "        \"\"\"\n",
    "        url = f\"{self.BASE_API_URL}api/xbrl/frames/{taxonomy}/{tag}/{unit}/{period}.json\"\n",
    "        response = self.rate_limited_request(\n",
    "            url, headers=self.sec_data_headers)\n",
    "        data = json.loads(response.text)\n",
    "        return data\n",
    "\n",
    "    def get_data_as_dataframe(self, cik: str,):\n",
    "        \"\"\"Retrieves the XBRL disclosures from a single company (CIK) and returns it as a pandas dataframe.\n",
    "\n",
    "        Args:\n",
    "            cik (str): CIK number of the company. Get the list using self.cik\n",
    "\n",
    "        Returns:\n",
    "            df: pandas dataframe containing the XBRL disclosures from a single company (CIK)\n",
    "        \"\"\"\n",
    "        data = self.get_company_facts(cik)\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "\n",
    "        for tag in data['facts'][self.taxonomy]:\n",
    "            facts = data['facts']['us-gaap'][tag]['units']\n",
    "            unit_key = list(facts.keys())[0]\n",
    "            temp_df = pd.DataFrame(facts[unit_key])\n",
    "            temp_df['label'] = tag\n",
    "            df = pd.concat([df, temp_df], axis=0, ignore_index=True)\n",
    "        df = df.astype({'val': 'float64',\n",
    "                        'end': 'datetime64[ns]',\n",
    "                        'start': 'datetime64[ns]',\n",
    "                        'filed': 'datetime64[ns]'})\n",
    "        df['Months Ended'] = (df['end'] - df['start']\n",
    "                              ).dt.days.div(30.4375).round(0)\n",
    "        return df\n",
    "\n",
    "    def get_cik_index(self, cik: str = None,) -> dict:\n",
    "        \"\"\"Each CIK directory and all child subdirectories contain three files to assist in \n",
    "        automated crawling of these directories. \n",
    "        These are not visible through directory browsing.\n",
    "            - index.html (the web browser would normally receive these)\n",
    "            - index.xml (a XML structured version of the same content)\n",
    "            - index.json (a JSON structured vision of the same content)\n",
    "\n",
    "        Args:\n",
    "            cik (str): CIK number of the company. Get the list using self.cik\n",
    "\n",
    "        Returns:\n",
    "            json: pandas dataframe containing the XBRL disclosures from a single company (CIK)\n",
    "        \"\"\"\n",
    "        if cik is not None:\n",
    "            url = self.BASE_DIRECTORY_URL + cik + '/' + 'index.json'\n",
    "\n",
    "        else:\n",
    "            url = self.BASE_DIRECTORY_URL + self.cik + '/' + 'index.json'\n",
    "\n",
    "        response = self.rate_limited_request(url, headers=self.sec_headers)\n",
    "        return response.json()\n",
    "\n",
    "    def get_sic_list(self, sic_list_url: str = SIC_LIST_URL) -> dict:\n",
    "        \"\"\"Get the list of SIC codes from SEC website.\n",
    "\n",
    "        Args:\n",
    "            sic_list_url (str): URL to the list of SIC codes\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame containing the SIC codes and descriptions\n",
    "        \"\"\"\n",
    "        response = self.rate_limited_request(\n",
    "            sic_list_url, headers=self.sec_headers)\n",
    "\n",
    "        soup = BeautifulSoup(response.content, \"lxml\")\n",
    "        sic_table = soup.find('table', {'class': 'list'})\n",
    "        sic_list = []\n",
    "        for row in sic_table.find_all('tr')[1:]:\n",
    "            sic_dict = {'_id': None,\n",
    "                        'Office': None, 'Industry Title': None}\n",
    "            sic_dict['_id'] = row.text.split('\\n')[1]\n",
    "            sic_dict['Office'] = row.text.split('\\n')[2]\n",
    "            sic_dict['Industry Title'] = row.text.split('\\n')[3]\n",
    "            sic_list.append(sic_dict)\n",
    "\n",
    "        return sic_list\n",
    "\n",
    "class TickerData(SECData):\n",
    "    \"\"\"Inherited from SECData class. Retrieves data from SEC Edgar database based on ticker.\n",
    "    url is constructed based on the following: https://www.sec.gov/Archives/edgar/data/{cik}/{ascension_number}/{file_name}\n",
    "    cik is the CIK number of the company = access via get_ticker_cik\n",
    "    ascension_number is the accessionNumber column of filings_df\n",
    "    file name for xml is always '{ticker}-{reportDate}.{extension}\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ticker: str, requester_company: str = 'Financial API', requester_name: str = 'API Caller', requester_email: str = 'apicaller@gmail.com', taxonomy: str = 'us-gaap',search_strategy: SearchStrategy = None):\n",
    "        super().__init__(requester_company, requester_name, requester_email, taxonomy,)\n",
    "        self.search_strategy = search_strategy\n",
    "        self.ticker = ticker.upper()\n",
    "        self.cik = self.get_ticker_cik(self.ticker)\n",
    "        self._submissions = self.get_submissions(self.cik)\n",
    "        self._filings = None\n",
    "        self._forms = None\n",
    "        self._index = self.get_cik_index(self.cik)\n",
    "        self._filing_folder_urls = None\n",
    "        self._filing_urls = None\n",
    "\n",
    "    @property\n",
    "    def submissions(self,) -> dict:\n",
    "        if self._submissions is not None:\n",
    "            self._submissions['cik'] = self.cik\n",
    "            self._submissions['filings'] = self.filings.replace(\n",
    "                {pd.NaT: None}).to_dict('records')\n",
    "        return self._submissions\n",
    "\n",
    "    @property\n",
    "    def filings(self,) -> pd.DataFrame:\n",
    "        if self._filings is None:\n",
    "            self._filings = self.get_filings()\n",
    "        return self._filings\n",
    "\n",
    "    @property\n",
    "    def latest_filing(self,) -> pd.DataFrame:\n",
    "        return self.filings.iloc[0, :].to_dict() if len(self.filings) > 0 else None\n",
    "\n",
    "    @property\n",
    "    def latest_10Q(self,) -> pd.DataFrame:\n",
    "        return self.filings.query(\"form == '10-Q'\").iloc[0, :].to_dict() if len(self.filings.query(\"form == '10-Q'\")) > 0 else None\n",
    "\n",
    "    @property\n",
    "    def latest_10K(self,) -> pd.DataFrame:\n",
    "        return self.filings.query(\"form == '10-K'\").iloc[0, :].to_dict() if len(self.filings.query(\"form == '10-K'\")) > 0 else None\n",
    "\n",
    "    @property\n",
    "    def latest_8K(self,) -> pd.DataFrame:\n",
    "        return self.filings.query(\"form == '8-K'\").iloc[0, :].to_dict() if len(self.filings.query(\"form == '8-K'\")) > 0 else None\n",
    "\n",
    "    @property\n",
    "    def filing_folder_urls(self,) -> list:\n",
    "        if self._filing_folder_urls is None:\n",
    "            self._filing_folder_urls = self._get_filing_folder_urls()\n",
    "        return self._filing_folder_urls\n",
    "\n",
    "    @property\n",
    "    def filing_urls(self,) -> list:\n",
    "        if self._filing_urls is None:\n",
    "            self._filing_urls = self.filings['file_url'].tolist()\n",
    "\n",
    "        return self._filing_urls\n",
    "\n",
    "    @property\n",
    "    def forms(self,) -> list:\n",
    "        if self._forms is None:\n",
    "            self._forms = self.filings['form'].unique()\n",
    "        return self._forms\n",
    "\n",
    "    def set_search_strategy(self, search_strategy: SearchStrategy):\n",
    "        self.search_strategy = search_strategy\n",
    "\n",
    "    def _get_filing_folder_urls(self,) -> list:\n",
    "        \"\"\"Get filing folder urls from index dict.\n",
    "\n",
    "        Args:\n",
    "            index (dict): index dict from get_index method\n",
    "\n",
    "        Returns:s\n",
    "            filing_folder_urls (list): list of filing folder urls\n",
    "        \"\"\"\n",
    "\n",
    "        filing_folder_urls = [self.BASE_SEC_URL + self._index['directory']['name'] + '/' + folder['name']\n",
    "                              for folder in self._index['directory']['item'] if folder['type'] == 'folder.gif']\n",
    "        return filing_folder_urls\n",
    "\n",
    "    def get_filing_folder_index(self, folder_url: str, return_df: bool = True):\n",
    "        \"\"\"Get filing folder index from folder url.\n",
    "\n",
    "        Args:\n",
    "            folder_url (str): folder url to retrieve data from\n",
    "            return_df (bool, optional): Whether to return a DataFrame or dict. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            index (dict): index dict or dataframe\n",
    "        \"\"\"\n",
    "        index_url = indexify_url(folder_url)\n",
    "        index = self.rate_limited_request(index_url, headers=self.sec_headers)\n",
    "        return pd.DataFrame(index.json()['directory']['item']) if return_df else index.json()['directory']['item']\n",
    "\n",
    "    def get_filings(self,) -> dict:\n",
    "        \"\"\"Get filings and urls to .txt from submissions dict.\n",
    "\n",
    "        Args:\n",
    "            submissions (dict): submissions dict from get_submissions method\n",
    "\n",
    "        Returns:\n",
    "            filings (dict): dictionary containing filings\n",
    "        \"\"\"\n",
    "        self.scrape_logger.info(\n",
    "            f'Making http request for {self.ticker} filings...')\n",
    "        filings = self._submissions['filings']['recent']\n",
    "\n",
    "        if len(self._submissions['filings']) > 1:\n",
    "            self.scrape_logger.info(\n",
    "                f'Additional filings found for {self.ticker}...')\n",
    "            for file in self._submissions['filings']['files']:\n",
    "                additional_filing = self.get_submissions(\n",
    "                    submission_file=file['name'])\n",
    "                filings = {key: filings[key] + additional_filing[key]\n",
    "                           for key in filings.keys()}\n",
    "\n",
    "        filings = pd.DataFrame(filings)\n",
    "        # Convert reportDate, filingDate, acceptanceDateTime columns to datetime\n",
    "        filings['reportDate'] = pd.to_datetime(filings['reportDate'])\n",
    "        filings['filingDate'] = pd.to_datetime(filings['filingDate'])\n",
    "        filings['acceptanceDateTime'] = pd.to_datetime(\n",
    "            filings['acceptanceDateTime'])\n",
    "        filings['cik'] = self.cik\n",
    "\n",
    "        filings = filings.loc[~pd.isnull(filings['reportDate'])]\n",
    "\n",
    "        # get folder url for each row\n",
    "        filings['folder_url'] = self.BASE_DIRECTORY_URL + \\\n",
    "            self.cik + '/' + filings['accessionNumber'].str.replace('-', '')\n",
    "\n",
    "        # get file url for each row\n",
    "        filings['file_url'] = filings['folder_url'] + \\\n",
    "            '/' + filings['accessionNumber'] + '.txt'\n",
    "\n",
    "        return filings\n",
    "\n",
    "    def get_file_data(self, file_url: str) -> BeautifulSoup:\n",
    "        \"\"\"Get file data from file url which can be retrieved by calling self.get_file_url method.\n",
    "\n",
    "        Args:\n",
    "            file_url (str): File url to retrieve data from on the SEC website\n",
    "\n",
    "        Returns:\n",
    "            data: File data as a BeautifulSoup object\n",
    "        \"\"\"\n",
    "        data = self.rate_limited_request(\n",
    "            url=file_url, headers=self.sec_headers)\n",
    "        try:\n",
    "            soup = BeautifulSoup(data.content, \"lxml\")\n",
    "            self.scrape_logger.info(\n",
    "                f'Parsed file data from {file_url} successfully.')\n",
    "            return soup\n",
    "\n",
    "        except Exception as e:\n",
    "            self.scrape_logger.error(\n",
    "                f'Failed to parse file data from {file_url}. Error: {e}')\n",
    "            raise Exception(\n",
    "                f'Failed to parse file data from {file_url}. Error: {e}')\n",
    "\n",
    "    def get_elements(self, folder_url: str, index_df: pd.DataFrame, scrape_file_extension: str) -> pd.DataFrame:\n",
    "        \"\"\"Get elements from .xml files from folder_url.\n",
    "\n",
    "        Args:\n",
    "            folder_url (str): folder url to retrieve data from\n",
    "            index_df (pd.DataFrame): dataframe containing files in the filing folder\n",
    "            scrape_file_extension (str): .xml file extension to scrape\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: returns a dataframe containing the elements, attributes, text\n",
    "        \"\"\"\n",
    "        xml = index_df.query(f\"name.str.contains('{scrape_file_extension}')\")\n",
    "        xml_content = self.rate_limited_request(\n",
    "            folder_url + '/' + xml['name'].iloc[0], headers=self.sec_headers).content\n",
    "\n",
    "        xml_soup = BeautifulSoup(xml_content, 'lxml-xml')\n",
    "        labels = xml_soup.find_all()\n",
    "        labels_list = []\n",
    "        for i in labels[1:]:\n",
    "            label_dict = dict(**i.attrs, labelText=i.text.strip())\n",
    "            labels_list.append(label_dict)\n",
    "        return pd.DataFrame(labels_list)\n",
    "\n",
    "    def search_tags(self, soup: BeautifulSoup, pattern: str = None) -> List[Tag]:\n",
    "        \"\"\"Search for tags in BeautifulSoup object. Strategy can be set using self.set_search_strategy method.\n",
    "\n",
    "        Args:\n",
    "            soup (BeautifulSoup): BeautifulSoup object\n",
    "            pattern (str): regex pattern to search for\n",
    "\n",
    "        Returns:\n",
    "            soup: BeautifulSoup object\n",
    "        \"\"\"\n",
    "        if self.search_strategy is None and pattern is None:\n",
    "            raise Exception('Search strategy not set and no pattern provided.')\n",
    "        if pattern is None:\n",
    "            pattern = self.search_strategy.get_pattern()\n",
    "        return soup.find_all(re.compile(pattern))\n",
    "\n",
    "    # To add more search methods, add a SearchStrategy abstract class with get_pattern method and add a method here\n",
    "    def search_context(self, soup: BeautifulSoup) -> List[Tag]:\n",
    "        self.set_search_strategy(ContextSearchStrategy())\n",
    "        return self.search_tags(soup)\n",
    "\n",
    "    def search_linklabels(self, soup: BeautifulSoup) -> List[Tag]:\n",
    "        self.set_search_strategy(LinkLabelSearchStrategy())\n",
    "        return self.search_tags(soup)\n",
    "\n",
    "    def search_facts(self, soup: BeautifulSoup) -> List[Tag]:\n",
    "        self.set_search_strategy(FactSearchStrategy())\n",
    "        return self.search_tags(soup)\n",
    "\n",
    "    def get_metalinks(self, metalinks_url: str) -> pd.DataFrame:\n",
    "        \"\"\"Get metalinks from metalinks url.\n",
    "\n",
    "        Args:\n",
    "            metalinks_url (str): metalinks url to retrieve data from\n",
    "\n",
    "        Returns:\n",
    "            df: DataFrame containing metalinks information with columns \n",
    "            {\n",
    "                'labelKey': str,\n",
    "                'localName': str,\n",
    "                'labelName': int,\n",
    "                'terseLabel': str,\n",
    "                'documentation': str,\n",
    "            }\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = self.rate_limited_request(\n",
    "                url=metalinks_url, headers=self.sec_headers).json()\n",
    "            metalinks_instance = convert_keys_to_lowercase(\n",
    "                response['instance'])\n",
    "            instance_key = list(metalinks_instance.keys())[0]\n",
    "            dict_list = []\n",
    "            for i in metalinks_instance[instance_key]['tag']:\n",
    "                dict_list.append(dict(labelKey=i.lower(),\n",
    "                                      localName=metalinks_instance[instance_key]['tag'][i].get(\n",
    "                                          'localname'),\n",
    "                                      labelName=metalinks_instance[instance_key]['tag'][i].get(\n",
    "                                          'lang').get('enus').get('role').get('label'),\n",
    "                                      terseLabel=metalinks_instance[instance_key]['tag'][i].get(\n",
    "                                          'lang').get('enus').get('role').get('terselabel'),\n",
    "                                      documentation=metalinks_instance[instance_key]['tag'][i].get('lang').get('enus').get('role').get('documentation'),))\n",
    "\n",
    "            df = pd.DataFrame.from_dict(dict_list)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            self.scrape_logger.error(\n",
    "                f'Failed to retrieve metalinks from {metalinks_url}. Error: {e}')\n",
    "            return None\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        class_name = type(self).__name__\n",
    "        main_attrs = ['ticker', 'cik', 'submissions', 'filings']\n",
    "        available_methods = [method_name for method_name in dir(self) if callable(\n",
    "            getattr(self, method_name)) and not method_name.startswith(\"_\")]\n",
    "        return f\"\"\"{class_name}({self.ticker})\n",
    "    CIK: {self.cik}\n",
    "    Latest filing: {self.latest_filing['filingDate'].strftime('%Y-%m-%d') if self.latest_filing else 'No filing found'} for Form {self.latest_filing['form'] if self.latest_filing else None}. Access via: {self.latest_filing['folder_url'] if self.latest_filing else None}\n",
    "    Latest 10-Q: {self.latest_10Q['filingDate'].strftime('%Y-%m-%d') if self.latest_10Q else 'No filing found'}. Access via: {self.latest_10Q['folder_url'] if self.latest_10Q else None}\n",
    "    Latest 10-K: {self.latest_10K['filingDate'].strftime('%Y-%m-%d') if self.latest_10K else 'No filing found'}. Access via: {self.latest_10K['folder_url'] if self.latest_10K else None}\"\"\"\n",
    "\n",
    "    def __repr_html__(self) -> str:\n",
    "        class_name = type(self).__name__\n",
    "        main_attrs = ['ticker', 'cik', 'submissions', 'filings']\n",
    "        available_methods = [method_name for method_name in dir(self) if callable(\n",
    "            getattr(self, method_name)) and not method_name.startswith(\"_\")]\n",
    "        latest_filing_date = self.latest_filing['filingDate'].strftime(\n",
    "            '%Y-%m-%d') if self.latest_filing else 'No filing found'\n",
    "        latest_filing_form = self.latest_filing['form'] if self.latest_filing else None\n",
    "        latest_filing_folder_url = self.latest_filing['folder_url'] if self.latest_filing else None\n",
    "        latest_10Q_date = self.latest_10Q['filingDate'].strftime(\n",
    "            '%Y-%m-%d') if self.latest_10Q else 'No filing found'\n",
    "        latest_10Q_folder_url = self.latest_10Q['folder_url'] if self.latest_10Q else None\n",
    "        latest_10K_date = self.latest_10K['filingDate'].strftime(\n",
    "            '%Y-%m-%d') if self.latest_10K else 'No filing found'\n",
    "        latest_10K_folder_url = self.latest_10K['folder_url'] if self.latest_10K else None\n",
    "        return f\"\"\"\n",
    "        <div style=\"border: 1px solid #ccc; padding: 10px; margin: 10px;\">\n",
    "            <h3>{self.submissions['name']}</h3>\n",
    "            <h5>{self.submissions['sicDescription']}</h5>\n",
    "            <p><strong>Ticker:</strong> {self.ticker}</p>\n",
    "            <p><strong>CIK:</strong> {self.cik}</p>\n",
    "            <p><strong>Latest filing:</strong> {latest_filing_date} for Form {latest_filing_form}. Access via: <a href=\"{latest_filing_folder_url}\">{latest_filing_folder_url}</a></p>\n",
    "            <p><strong>Latest 10-Q:</strong> {latest_10Q_date}. Access via: <a href=\"{latest_10Q_folder_url}\">{latest_10Q_folder_url}</a></p>\n",
    "            <p><strong>Latest 10-K:</strong> {latest_10K_date}. Access via: <a href=\"{latest_10K_folder_url}\">{latest_10K_folder_url}</a></p>\n",
    "        </div>\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MongoDB Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in libraries\n",
    "import datetime as dt\n",
    "\n",
    "# Third party libraries\n",
    "import streamlit as st\n",
    "from pymongo import MongoClient, ASCENDING\n",
    "from pymongo import MongoClient, ASCENDING, IndexModel, UpdateOne\n",
    "from pymongo.errors import OperationFailure\n",
    "\n",
    "@st.cache_resource\n",
    "def init_connection(secrets_name: str = 'mongo'):\n",
    "    return MongoClient(**st.secrets[secrets_name])\n",
    "\n",
    "\n",
    "@st.cache_resource(ttl=86400)  # only refresh after 24h\n",
    "def get_data():\n",
    "    client = init_connection()\n",
    "    db = client.FinanceApp\n",
    "    balance_sheet_collection = db.balance_sheet\n",
    "    income_collection = db.income_statement\n",
    "    cash_collection = db.cash_flow_statement\n",
    "    company_profile = db.company_profile\n",
    "    historical = db.historical\n",
    "    stock_split = db.stock_split\n",
    "    return balance_sheet_collection, income_collection, cash_collection, company_profile, historical, stock_split\n",
    "\n",
    "\n",
    "class SECDatabase(MyLogger):\n",
    "    def __init__(self, connection_string):\n",
    "        super().__init__(name='SECDatabase', level='DEBUG', log_file='././logs.log')\n",
    "        self.client = MongoClient(connection_string)\n",
    "        self.db = self.client.SECRawData\n",
    "        self.tickerdata = self.db.TickerData\n",
    "        self.tickerfilings = self.db.TickerFilings\n",
    "        self.sicdb = self.db.SICList\n",
    "        self.factsdb = self.db.Facts\n",
    "        try:\n",
    "            self.tickerdata.create_indexes(\n",
    "                [IndexModel([('cik', ASCENDING)], unique=True)])\n",
    "        except OperationFailure as e:\n",
    "            self.scrape_logger.error(e)\n",
    "\n",
    "        try:\n",
    "            self.tickerfilings.create_indexes([IndexModel(\n",
    "                [('accessionNumber', ASCENDING)], unique=True), IndexModel([('form', ASCENDING)])])\n",
    "        except OperationFailure as e:\n",
    "            self.scrape_logger.error(e)\n",
    "\n",
    "        try:\n",
    "            self.factsdb.create_indexes(\n",
    "                [IndexModel([('factId', ASCENDING)], unique=True)])\n",
    "\n",
    "        except OperationFailure as e:\n",
    "            self.scrape_logger.error(e)\n",
    "\n",
    "    @property\n",
    "    def get_server_info(self):\n",
    "        return self.client.server_info()\n",
    "\n",
    "    @property\n",
    "    def get_collection_names(self):\n",
    "        return self.db.list_collection_names()\n",
    "\n",
    "    @property\n",
    "    def get_tickerdata_index_information(self):\n",
    "        return self.tickerdata.index_information()\n",
    "\n",
    "    @property\n",
    "    def get_tickerfilings_index_information(self):\n",
    "        return self.tickerfilings.index_information()\n",
    "\n",
    "    def get_tickerdata(self, cik: str = None, ticker: str = None):\n",
    "        if cik is not None:\n",
    "            return self.tickerdata.find_one({'cik': cik})\n",
    "        elif ticker is not None:\n",
    "            return self.tickerdata.find_one({'tickers': ticker.upper()})\n",
    "        else:\n",
    "            raise Exception('Please provide either a CIK or ticker.')\n",
    "\n",
    "    def insert_submission(self, submission: dict):\n",
    "        \"\"\"Insert submissions into SEC database. CIK is the primary key.\n",
    "\n",
    "        Args:\n",
    "            ticker (TickerData): TickerData object\n",
    "\n",
    "        Returns:\n",
    "            str: empty string if successful\n",
    "            str: ticker's cik if failed\n",
    "        \"\"\"\n",
    "        submission['lastUpdated'] = dt.datetime.now()\n",
    "        try:\n",
    "            self.tickerdata.update_one({'cik': submission['cik']}, {\n",
    "                                       '$set': submission}, upsert=True)\n",
    "            self.scrape_logger.info(\n",
    "                f'Inserted submissions for {submission[\"cik\"]} into SEC database.')\n",
    "\n",
    "        except Exception as e:\n",
    "            self.scrape_logger.error(\n",
    "                f'Failed to insert submissions for {submission[\"cik\"]} into SEC database. Error: {e}')\n",
    "            return submission['cik']\n",
    "        return None\n",
    "\n",
    "    def insert_filings(self, cik: str, filings: list):\n",
    "        \"\"\"Insert filings into SEC database. Each submission has many filings. Accession number is the primary key.\n",
    "\n",
    "        Args:\n",
    "            ticker (TickerData): TickerData object\n",
    "\n",
    "        Returns:\n",
    "            str: empty string if successful\n",
    "            str: ticker's cik if failed\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for doc in filings:\n",
    "                doc['lastUpdated'] = dt.datetime.now()\n",
    "\n",
    "            update_requests = [UpdateOne({'accessionNumber': doc['accessionNumber']}, {\n",
    "                                         '$set': doc}, upsert=True) for doc in filings]\n",
    "\n",
    "            self.tickerfilings.bulk_write(update_requests)\n",
    "            self.scrape_logger.info(\n",
    "                f'Sucessfully updated filings for {cik}...')\n",
    "\n",
    "        except Exception as e:\n",
    "            self.scrape_logger.error(\n",
    "                f'Failed to insert filings for {cik}...{e}')\n",
    "            return cik\n",
    "        return None\n",
    "\n",
    "    def insert_facts(self, accession: str, facts: list):\n",
    "        \"\"\"Insert facts into SEC database. Each filing has many facts.\n",
    "\n",
    "        Args:\n",
    "            facts (list): A list containing facts for a single filing\n",
    "\n",
    "        Returns:\n",
    "            str: empty string if successful\n",
    "            str: ticker's cik if failed\n",
    "        \"\"\"\n",
    "        try:\n",
    "            for doc in facts:\n",
    "                doc['lastUpdated'] = dt.datetime.now()\n",
    "\n",
    "            fact_update_requests = [UpdateOne({'factId': fact['factId']}, {\n",
    "                                              '$set': fact}, upsert=True) for fact in facts]\n",
    "\n",
    "            self.factsdb.bulk_write(fact_update_requests)\n",
    "            self.scrape_logger.info(f'Updated facts for {accession}...')\n",
    "\n",
    "        except Exception as e:\n",
    "            self.scrape_logger.error(\n",
    "                f'Failed to insert facts for {accession}...{e}')\n",
    "            return accession\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "sec = SECData()\n",
    "mongo = SECDatabase(os.getenv('mongodb_sec'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to insert submission, filings, and facts for each filing into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec = SECData()\n",
    "sic_dict = sec.get_sic_list()\n",
    "mongo = SECDatabase(connection_string=os.getenv('mongodb_sec'))\n",
    "\n",
    "failed_submissions = []\n",
    "failed_filings = []\n",
    "failed_facts = []\n",
    "\n",
    "with trange(len(sec.cik_list['ticker'][:50]), desc='Instantiating ticker...',) as t:\n",
    "    for item in t:\n",
    "        ticker = sec.cik_list['ticker'].iloc[item] # Get ticker from cik_list\n",
    "        t.set_postfix(ticker=ticker, cik=sec.cik_list['cik_str'].iloc[item])\n",
    "\n",
    "        # Initialize and instantiate TickerData object\n",
    "        try:\n",
    "            symbol = TickerData(ticker=ticker)\n",
    "            cik = symbol.cik # get cik of ticker\n",
    "            symbol.submissions['lastUpdated'] = dt.datetime.now()\n",
    "            symbol.submissions['office'] = mongo.sicdb.find_one({'_id': symbol.submissions['sic']})['Office']\n",
    "            sec.scrape_logger.info(f'{t}')\n",
    "            sec.scrape_logger.info(f'\\nInstantiated {symbol}...')\n",
    "        except Exception as e:\n",
    "            sec.scrape_logger.info(f'{t}')\n",
    "            sec.scrape_logger.error(f'Failed to instantiate {ticker} with cik {cik}...{e}')\n",
    "            continue\n",
    "\n",
    "        filings = symbol.submissions.pop('filings')\n",
    "        # print(filings)\n",
    "        # Insert submissions to TickerData collection\n",
    "        inserted_submission = mongo.insert_submission(submission=symbol._submissions)\n",
    "        if inserted_submission is not None:\n",
    "            failed_submissions.append(inserted_submission)\n",
    "\n",
    "        # Insert filings to TickerFilings collection\n",
    "        inserted_filing = mongo.insert_filings(cik=cik, filings=filings)\n",
    "        if inserted_filing is not None:\n",
    "            failed_filings.append(inserted_filing)\n",
    "\n",
    "        # # Insert facts to Facts collection\n",
    "        # for doc in filings:\n",
    "        #     doc['lastUpdated'] = dt.datetime.now()\n",
    "\n",
    "        #     if doc['form'] == '10-Q' or doc['form'] == '10-K':\n",
    "        #         try:\n",
    "        #             facts = symbol.get_facts_for_each_filing(doc)\n",
    "        #             inserted_facts = mongo.insert_facts(accession=doc['accessionNumber'], facts=facts)\n",
    "        #             if inserted_facts is not None:\n",
    "        #                 failed_facts.append(inserted_facts)\n",
    "        #         except Exception as e:\n",
    "        #             sec.scrape_logger.error(f'TickerData().get_facts_for_each_filing() function failed for {doc[\"accessionNumber\"]}...{e}')\n",
    "        #             failed_facts.append(doc['accessionNumber'])\n",
    "            \n",
    "        sec.scrape_logger.info(f'Successfully updated {ticker}({cik})...\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in libraries\n",
    "from dataclasses import dataclass\n",
    "import datetime as dt\n",
    "import re\n",
    "\n",
    "# Third party libraries\n",
    "from bs4.element import Tag\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Context:\n",
    "    context_tag: Tag\n",
    "\n",
    "    @property\n",
    "    def contextId(self) -> str:\n",
    "        \"\"\"Get contextId\n",
    "\n",
    "        Returns:\n",
    "            str: contextId\n",
    "        \"\"\"\n",
    "        return self.context_tag.attrs.get('id')\n",
    "\n",
    "    @property\n",
    "    def entity(self) -> str | None:\n",
    "        \"\"\"Get entity\n",
    "\n",
    "        Returns:\n",
    "            str: entity\n",
    "        \"\"\"\n",
    "        return self.context_tag.find(\"entity\").text.split()[\n",
    "            0] if self.context_tag.find(\"entity\") is not None else None\n",
    "\n",
    "    @property\n",
    "    def startDate(self) -> dt.datetime | None:\n",
    "        \"\"\"Get start date\n",
    "\n",
    "        Returns:\n",
    "            dt.datetime: start date\n",
    "        \"\"\"\n",
    "        start = self.context_tag.find(\"startdate\").text if self.context_tag.find(\n",
    "            \"startdate\") is not None else None\n",
    "        return dt.datetime.strptime(start, '%Y-%m-%d') if start is not None else None\n",
    "\n",
    "    @property\n",
    "    def endDate(self) -> dt.datetime | None:\n",
    "        \"\"\"Get end date\n",
    "\n",
    "        Returns:\n",
    "            dt.datetime: end date\n",
    "        \"\"\"\n",
    "        end = self.context_tag.find(\"enddate\").text if self.context_tag.find(\n",
    "            \"enddate\") is not None else None\n",
    "        return dt.datetime.strptime(end, '%Y-%m-%d') if end is not None else None\n",
    "\n",
    "    @property\n",
    "    def instant(self):\n",
    "        \"\"\"Get instant date\n",
    "\n",
    "        Returns:\n",
    "            dt.datetime: instant date\n",
    "        \"\"\"\n",
    "        instant = self.context_tag.find(\"instant\").text if self.context_tag.find(\n",
    "            \"instant\") is not None else None\n",
    "        return dt.datetime.strptime(instant, '%Y-%m-%d') if instant is not None else None\n",
    "\n",
    "    @property\n",
    "    def segment(self) -> dict | None:\n",
    "        \"\"\"Get segments and tags classifying the segment and store in dict\n",
    "\n",
    "        Returns:\n",
    "            dict: dict containing segment and tags classifying the segment\n",
    "        \"\"\"\n",
    "        segment = self.context_tag.find(\"segment\")\n",
    "\n",
    "        if segment is None:\n",
    "            return None\n",
    "\n",
    "        segment_dict = {}\n",
    "\n",
    "        segment_breakdown = segment.find_all(re.compile('^xbrldi:.*'))\n",
    "\n",
    "        for i in segment_breakdown:\n",
    "            segment_dict[i.attrs.get('dimension')] = i.text\n",
    "\n",
    "        return segment_dict\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert context to dict\n",
    "\n",
    "        Returns:\n",
    "            dict: dict containing context information\n",
    "        \"\"\"\n",
    "        return dict(contextId=self.contextId, entity=self.entity, segment=self.segment, startDate=self.startDate, endDate=self.endDate, instant=self.instant)\n",
    "\n",
    "    def get_segment_length(self) -> int:\n",
    "        \"\"\"Get length of segment\n",
    "\n",
    "        Returns:\n",
    "            int: length of segment\n",
    "        \"\"\"\n",
    "        segment = self.context_tag.find(\"segment\")\n",
    "\n",
    "        if segment is None:\n",
    "            return 0\n",
    "\n",
    "        return len(segment)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Context(contextId={self.contextId}, entity={self.entity}, segment={self.segment}, startDate={self.startDate}, endDate={self.endDate}, instant={self.instant})'\n",
    "\n",
    "    def __repr_html__(self):\n",
    "        return f\"\"\"\n",
    "        <div style=\"border: 1px solid #ccc; padding: 10px; margin: 10px;\">\n",
    "            <h3>Context</h3>\n",
    "            <p><strong>contextId:</strong> {self.contextId}</p>\n",
    "            <p><strong>entity:</strong> {self.entity}</p>\n",
    "            <p><strong>segment:</strong> {self.segment}</p>\n",
    "            <p><strong>startDate:</strong> {self.startDate}</p>\n",
    "            <p><strong>endDate:</strong> {self.endDate}</p>\n",
    "            <p><strong>instant:</strong> {self.instant}</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'''contextId={self.contextId}\n",
    "entity={self.entity}\n",
    "segment={self.segment}\n",
    "startDate={self.startDate}\n",
    "endDate={self.endDate}\n",
    "instant={self.instant}'''\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LinkLabels:\n",
    "    label_tag: Tag\n",
    "\n",
    "    @property\n",
    "    def linkLabelId(self) -> str | None:\n",
    "        \"\"\"Get labelId\n",
    "\n",
    "        Returns:\n",
    "            str: labelId\n",
    "        \"\"\"\n",
    "        return self.label_tag.attrs.get('id')\n",
    "\n",
    "    @property\n",
    "    def xlinkLabel(self) -> str | None:\n",
    "        \"\"\"Get linkLabel\n",
    "\n",
    "        Returns:\n",
    "            str: linkLabel\n",
    "        \"\"\"\n",
    "        return self.label_tag.attrs.get('xlink:label')\n",
    "\n",
    "    @property\n",
    "    def xlinkRole(self) -> str | None:\n",
    "        \"\"\"Get linkRole\n",
    "\n",
    "        Returns:\n",
    "            str: linkRole\n",
    "        \"\"\"\n",
    "        return self.label_tag.attrs.get('xlink:role')\n",
    "\n",
    "    @property\n",
    "    def xlinkType(self) -> str | None:\n",
    "        \"\"\"Get linkType\n",
    "\n",
    "        Returns:\n",
    "            str: linkType\n",
    "        \"\"\"\n",
    "        return self.label_tag.attrs.get('xlink:type')\n",
    "\n",
    "    @property\n",
    "    def xlmnsXml(self) -> str | None:\n",
    "        \"\"\"Get xlmnsXml\n",
    "\n",
    "        Returns:\n",
    "            str: xlmnsXml\n",
    "        \"\"\"\n",
    "        return self.label_tag.attrs.get('xmlns:xml')\n",
    "\n",
    "    @property\n",
    "    def xlmLang(self) -> str | None:\n",
    "        \"\"\"Get xlmLang\n",
    "\n",
    "        Returns:\n",
    "            str: xlmLang\n",
    "        \"\"\"\n",
    "        return self.label_tag.attrs.get('xml:lang')\n",
    "\n",
    "    @property\n",
    "    def labelName(self) -> str | None:\n",
    "        \"\"\"Get labelName\n",
    "\n",
    "        Returns:\n",
    "            str: labelName\n",
    "        \"\"\"\n",
    "        return self.label_tag.text if self.label_tag.text is not None else None\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert linkLabels to dict\n",
    "\n",
    "        Returns:\n",
    "            dict: dict containing linkLabels information\n",
    "        \"\"\"\n",
    "        return dict(linkRole=self.linkRole, linkLabel=self.linkLabel, linkbase=self.linkbase)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'LinkLabels(linkRole={self.linkRole}, linkLabel={self.linkLabel}, linkbase={self.linkbase})'\n",
    "\n",
    "    def __repr_html__(self):\n",
    "        return f\"\"\"\n",
    "        <div style=\"border: 1px solid #ccc; padding: 10px; margin: 10px;\">\n",
    "            <h3>LinkLabels</h3>\n",
    "            <p><strong>linkRole:</strong> {self.linkRole}</p>\n",
    "            <p><strong>linkLabel:</strong> {self.linkLabel}</p>\n",
    "            <p><strong>linkbase:</strong> {self.linkbase}</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'''linkRole={self.linkRole}\n",
    "linkLabel={self.linkLabel}\n",
    "linkBase={self.linkbase}'''\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Facts:\n",
    "    fact_tag: Tag\n",
    "\n",
    "    @property\n",
    "    def factName(self):\n",
    "        \"\"\"Get factName\n",
    "\n",
    "        Returns:\n",
    "            str: factName\n",
    "        \"\"\"\n",
    "        return self.fact_tag.name\n",
    "\n",
    "    @property\n",
    "    def factId(self):\n",
    "        \"\"\"Get factId\n",
    "\n",
    "        Returns:\n",
    "            str: factId\n",
    "        \"\"\"\n",
    "        return self.fact_tag.attrs.get('id')\n",
    "\n",
    "    @property\n",
    "    def contextRef(self):\n",
    "        \"\"\"Get contextRef\n",
    "\n",
    "        Returns:\n",
    "            str: contextRef\n",
    "        \"\"\"\n",
    "        return self.fact_tag.attrs.get('contextref')\n",
    "\n",
    "    @property\n",
    "    def unitRef(self):\n",
    "        \"\"\"Get unitRef\n",
    "\n",
    "        Returns:\n",
    "            str: unitRef\n",
    "        \"\"\"\n",
    "        return self.fact_tag.attrs.get('unitref')\n",
    "\n",
    "    @property\n",
    "    def decimals(self):\n",
    "        \"\"\"Get decimals\n",
    "\n",
    "        Returns:\n",
    "            str: decimals\n",
    "        \"\"\"\n",
    "        return self.fact_tag.attrs.get('decimals')\n",
    "\n",
    "    @property\n",
    "    def factValue(self):\n",
    "        \"\"\"Get factValue\n",
    "\n",
    "        Returns:\n",
    "            str: factValue\n",
    "        \"\"\"\n",
    "        return self.fact_tag.text\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert facts to dict\n",
    "\n",
    "        Returns:\n",
    "            dict: dict containing facts information\n",
    "        \"\"\"\n",
    "        return dict(factName=self.factName, factId=self.factId, contextRef=self.contextRef, unitRef=self.unitRef, decimals=self.decimals, factValue=self.factValue)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Facts(factName={self.factName}, factId={self.factId}, contextRef={self.contextRef}, unitRef={self.unitRef}, decimals={self.decimals}, factValue={self.factValue})'\n",
    "\n",
    "    def __repr_html__(self):\n",
    "        return f\"\"\"\n",
    "        <div style=\"border: 1px solid #ccc; padding: 10px; margin: 10px;\">\n",
    "            <h3>Facts</h3>\n",
    "            <p><strong>factName:</strong> {self.factName}</p>\n",
    "            <p><strong>factId:</strong> {self.factId}</p>\n",
    "            <p><strong>contextRef:</strong> {self.contextRef}</p>\n",
    "            <p><strong>unitRef:</strong> {self.unitRef}</p>\n",
    "            <p><strong>decimals:</strong> {self.decimals}</p>\n",
    "            <p><strong>factValue:</strong> {self.factValue}</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'''factName={self.factName}\n",
    "factId={self.factId}\n",
    "contextRef={self.contextRef}\n",
    "unitRef={self.unitRef}\n",
    "decimals={self.decimals}\n",
    "factValue={self.factValue}'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather labels, definitions, and calculations xml data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:178: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:178: SyntaxWarning: invalid escape sequence '\\.'\n",
      "/var/folders/hg/wck4x05567dcx5xstz11b4qm0000gn/T/ipykernel_23862/4020566942.py:178: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  '[^0-9\\.\\-]|(^\\d+\\-\\d+\\-\\d+$)')) & (merged_facts['value'] != \"\")].copy()\n"
     ]
    }
   ],
   "source": [
    "def get_filing_facts(ticker: TickerData, filings_to_scrape: list, verbose=False):\n",
    "    \"\"\"\n",
    "    Scrape facts, context, labels, definitions, calculations, metalinks from filings_to_scrape\n",
    "\n",
    "    ### Parameters\n",
    "    ----------\n",
    "    ticker : TickerData\n",
    "        TickerData object\n",
    "    filings_to_scrape : list\n",
    "        list of filings dict to scrape\n",
    "\n",
    "    ### Returns\n",
    "    -------\n",
    "    all_labels : pd.DataFrame\n",
    "        all labels scraped\n",
    "    all_calc : pd.DataFrame\n",
    "        all calculations scraped\n",
    "    all_defn : pd.DataFrame\n",
    "        all definitions scraped\n",
    "    all_context : pd.DataFrame\n",
    "        all contexts scraped\n",
    "    all_facts : pd.DataFrame\n",
    "        all facts scraped\n",
    "    all_metalinks : pd.DataFrame    \n",
    "        all metalinks scraped\n",
    "    all_merged_facts : pd.DataFrame\n",
    "        all merged facts scraped\n",
    "    failed_folders : list\n",
    "        list of failed folders\n",
    "    \"\"\"\n",
    "    all_labels = pd.DataFrame()\n",
    "    all_calc = pd.DataFrame()\n",
    "    all_defn = pd.DataFrame()\n",
    "    all_context = pd.DataFrame()\n",
    "    all_facts = pd.DataFrame()\n",
    "    all_metalinks = pd.DataFrame()\n",
    "    all_merged_facts = pd.DataFrame()\n",
    "    failed_folders = []\n",
    "\n",
    "    for file in filings_to_scrape:\n",
    "        if (file.get('form') != '10-Q' or file.get('form') != '10-K') and file.get('filingDate') < dt.datetime(2009, 1, 1):\n",
    "            continue\n",
    "\n",
    "        accessionNumber = file.get('accessionNumber')\n",
    "        folder_url = file.get('folder_url')\n",
    "        file_url = file.get('file_url')\n",
    "        ticker.scrape_logger.info(\n",
    "            file.get('filingDate').strftime('%Y-%m-%d') + ': ' + folder_url)\n",
    "\n",
    "        soup = ticker.get_file_data(file_url=file_url)\n",
    "\n",
    "        try:  # Scrape facts\n",
    "            facts_list = []\n",
    "            facts = ticker.search_facts(soup=soup)\n",
    "            for fact_tag in facts:\n",
    "                facts_list.append(Facts(soup=fact_tag).to_dict())\n",
    "            facts_df = pd.DataFrame(facts_list)\n",
    "            facts_df['accessionNumber'] = accessionNumber\n",
    "            all_facts = pd.concat([all_facts, facts_df], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            ticker.scrape_logger.error(\n",
    "                f'Failed to scrape facts for {folder_url}...{e}')\n",
    "            failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                                  error=f'Failed to scrape facts for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "            pass\n",
    "\n",
    "        if len(facts_list) == 0:\n",
    "            ticker.scrape_logger.info(\n",
    "                f'No facts found for {ticker.ticker}({ticker.cik})-{folder_url}...\\n')\n",
    "            continue\n",
    "\n",
    "        try:  # Scrape context\n",
    "            context_list = []\n",
    "            contexts = ticker.search_context(soup=soup)\n",
    "            for tag in contexts:\n",
    "                context_list.append(Context(soup=tag).to_dict())\n",
    "            context_df = pd.DataFrame(context_list)\n",
    "            context_df['accessionNumber'] = accessionNumber\n",
    "            all_context = pd.concat(\n",
    "                [all_context, context_df], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            ticker.scrape_logger.error(\n",
    "                f'Failed to scrape context for {folder_url}...{e}')\n",
    "            failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                                  error=f'Failed to scrape context for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "            pass\n",
    "\n",
    "        index_df = ticker.get_filing_folder_index(folder_url=folder_url)\n",
    "\n",
    "        try:  # Scrape metalinks\n",
    "            metalinks = ticker.get_metalinks(\n",
    "                folder_url=folder_url + '/MetaLinks.json')\n",
    "            metalinks['accessionNumber'] = accessionNumber\n",
    "            all_metalinks = pd.concat(\n",
    "                [all_metalinks, metalinks], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            ticker.scrape_logger.error(\n",
    "                f'Failed to scrape metalinks for {folder_url}...{e}')\n",
    "            failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                                  error=f'Failed to scrape metalinks for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "            pass\n",
    "\n",
    "        try:  # Scrape labels\n",
    "            labels = ticker.get_elements(folder_url=folder_url, index_df=index_df,\n",
    "                                         scrape_file_extension='_lab').query(\"`xlink:type` == 'resource'\")\n",
    "            labels['xlink:role'] = labels['xlink:role'].str.split(\n",
    "                '/').apply(lambda x: x[-1])\n",
    "            labels['xlink:label'] = labels['xlink:label'].str\\\n",
    "                .replace('(lab_)|(_en-US)', '', regex=True).str\\\n",
    "                .split('_')\\\n",
    "                .apply(lambda x: ':'.join(x[:2]))\\\n",
    "                .str.lower()\n",
    "            labels['accessionNumber'] = accessionNumber\n",
    "            all_labels = pd.concat([all_labels, labels], ignore_index=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            ticker.scrape_logger.error(\n",
    "                f'Failed to scrape labels for {folder_url}...{e}')\n",
    "            failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                                  error=f'Failed to scrape labels for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "            pass\n",
    "\n",
    "        try:  # Scrape calculations\n",
    "            calc = ticker.get_elements(folder_url=folder_url, index_df=index_df,\n",
    "                                       scrape_file_extension='_cal').query(\"`xlink:type` == 'arc'\")\n",
    "            calc['accessionNumber'] = accessionNumber\n",
    "            all_calc = pd.concat([all_calc, calc], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            ticker.scrape_logger.error(\n",
    "                f'Failed to scrape calc for {folder_url}...{e}')\n",
    "            failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                                  error=f'Failed to scrape calc for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "            pass\n",
    "\n",
    "        try:  # Scrape definitions\n",
    "            defn = ticker.get_elements(folder_url=folder_url, index_df=index_df,\n",
    "                                       scrape_file_extension='_def').query(\"`xlink:type` == 'arc'\")\n",
    "            defn['accessionNumber'] = accessionNumber\n",
    "            all_defn = pd.concat([all_defn, defn], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            ticker.scrape_logger.error(\n",
    "                f'Failed to scrape defn for {folder_url}...{e}')\n",
    "            failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                                  error=f'Failed to scrape defn for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "            pass\n",
    "\n",
    "        ticker.scrape_logger.info(\n",
    "            f'Merging facts with context and labels. Current facts length: {len(facts_list)}...')\n",
    "        try:\n",
    "            merged_facts = facts_df.merge(context_df, how='left', left_on='contextRef', right_on='contextId')\\\n",
    "                .merge(labels.query(\"`xlink:role` == 'label'\"), how='left', left_on='factName', right_on='xlink:label')\n",
    "            merged_facts = merged_facts.drop(\n",
    "                ['accessionNumber_x', 'accessionNumber_y'], axis=1)\n",
    "            ticker.scrape_logger.info(\n",
    "                f'Successfully merged facts with context and labels. Merged facts length: {len(merged_facts)}...')\n",
    "        except Exception as e:\n",
    "            ticker.scrape_logger.error(\n",
    "                f'Failed to merge facts with context and labels for {folder_url}...{e}')\n",
    "            failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                                  error=f'Failed to merge facts with context and labels for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "            pass\n",
    "\n",
    "        all_merged_facts = pd.concat(\n",
    "            [all_merged_facts, merged_facts], ignore_index=True)\n",
    "        ticker.scrape_logger.info(\n",
    "            f'Successfully scraped {ticker.ticker}({ticker.cik})-{folder_url}...\\n')\n",
    "        if verbose:\n",
    "            st.success(\n",
    "                ticker.ticker + ' ' + file.get('filingDate').strftime('%Y-%m-%d'))\n",
    "    all_merged_facts = all_merged_facts.loc[~all_merged_facts['labelText'].isnull(), [\n",
    "        'labelText', 'segment', 'startDate', 'endDate', 'instant', 'value', 'unitRef']]\n",
    "\n",
    "    return all_labels, all_calc, all_defn, all_context, all_facts, all_metalinks, all_merged_facts, failed_folders\n",
    "\n",
    "\n",
    "def clean_values_in_facts(merged_facts: pd.DataFrame):\n",
    "    df = merged_facts.loc[(~merged_facts['value'].str.contains(\n",
    "        '[^0-9\\.\\-]|(^\\d+\\-\\d+\\-\\d+$)')) & (merged_facts['value'] != \"\")].copy()\n",
    "    df['value'] = df['value'].astype(float)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_values_in_segment(merged_facts: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Segment column of merged facts is cleaned to remove \"ticker:\" and \"us-gaap:\" prepend, and to split camel case into separate words (e.g. \"us-gaap:RevenuesBeforeTax\" becomes \"Revenues Before Tax\"). \n",
    "\n",
    "    Args:\n",
    "        merged_facts (pd.DataFrame): merged facts data frame from get_filing_facts.\n",
    "\n",
    "    Returns:\n",
    "        merged_facts (pd.DataFrame): merged facts data frame with segment column cleaned\n",
    "    \"\"\"\n",
    "    prepends = [i[0] for i in merged_facts.loc[(merged_facts['segment'].str.contains(':')) & (\n",
    "        ~merged_facts['segment'].isna())]['segment'].str.extract(r'(.*:)').drop_duplicates().values]\n",
    "    pattern = '|'.join(prepends)\n",
    "\n",
    "    merged_facts['segment'] = merged_facts['segment']\\\n",
    "        .str.replace(pat=pattern, repl='', regex=True)\\\n",
    "        .str.replace(pat=r'([A-Z])', repl=r' \\1', regex=True).str.strip()\n",
    "    # .apply(lambda x: x[-1] if isinstance(x, list) else x)\\\n",
    "\n",
    "    return merged_facts\n",
    "\n",
    "\n",
    "def split_facts_into_start_instant(merged_facts: pd.DataFrame):\n",
    "    \"\"\"Splits facts into start/end and instant\n",
    "\n",
    "    Args:\n",
    "        merged_facts (pd.DataFrame): merged facts data frame from get_filing_facts\n",
    "\n",
    "    Returns:\n",
    "        merged_facts: merged facts data frame without duplicates on the columns labelText, segment, startDate, endDate, instant, value\n",
    "        start_end: start/end facts data frame where startDate and endDate are not null\n",
    "        instant: instant facts data frame where instant is not null\n",
    "    \"\"\"\n",
    "    merged_facts.drop_duplicates(subset=[\n",
    "        'labelText', 'segment', 'startDate', 'endDate', 'instant', 'value'], keep='last', inplace=True)\n",
    "\n",
    "    start_end = merged_facts.dropna(axis=0, subset=['startDate', 'endDate'])[['labelText', 'segment', 'unitRef',\n",
    "                                                                              'startDate', 'endDate', 'value']].sort_values(by=['labelText', 'segment', 'startDate', 'endDate',])\n",
    "    instant = merged_facts.dropna(axis=0, subset=['instant'])[\n",
    "        ['labelText', 'segment', 'unitRef', 'instant', 'value']].sort_values(by=['labelText', 'segment', 'instant',])\n",
    "\n",
    "    return merged_facts, start_end, instant\n",
    "\n",
    "\n",
    "def get_monthly_period(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['period'] = pd.to_timedelta(\n",
    "        df['endDate'] - df['startDate']).dt.days / 30.25\n",
    "    df['period'] = df['period'].round(0)\n",
    "    df['Months Ended'] = np.select(\n",
    "        [\n",
    "            df['period'] == 3,\n",
    "            df['period'] == 6,\n",
    "            df['period'] == 9,\n",
    "            df['period'] == 12,\n",
    "        ],\n",
    "        [\n",
    "            \"Three Months Ended\",\n",
    "            \"Six Months Ended\",\n",
    "            \"Nine Months Ended\",\n",
    "            \"Twelve Months Ended\",\n",
    "        ],\n",
    "        default=None\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 13:08:18,675 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/files/company_tickers.json\n",
      "2024-01-14 13:08:19,708 - sec-scraper - INFO - Request successful at URL: https://data.sec.gov/submissions/CIK0001318605.json\n",
      "2024-01-14 13:08:19,842 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001318605/index.json\n",
      "2024-01-14 13:08:19,848 - sec-scraper - INFO - Making http request for TSLA filings...\n",
      "2024-01-14 13:08:19,848 - sec-scraper - INFO - Additional filings found for TSLA...\n",
      "2024-01-14 13:08:20,151 - sec-scraper - INFO - Request successful at URL: https://data.sec.gov/submissions/CIK0001318605-submissions-001.json\n",
      "2024-01-14 13:08:20,423 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001318605/000162828023034847/0001628280-23-034847.txt\n",
      "2024-01-14 13:08:21,433 - sec-scraper - INFO - Parsed file data from https://www.sec.gov/Archives/edgar/data/0001318605/000162828023034847/0001628280-23-034847.txt successfully.\n",
      "2024-01-14 13:08:21,435 - sec-scraper - INFO - 2023-10-23: https://www.sec.gov/Archives/edgar/data/0001318605/000162828023034847\n"
     ]
    }
   ],
   "source": [
    "sec = SECData()\n",
    "ticker = TickerData(ticker='TSLA')\n",
    "file = ticker.filings.loc[ticker.filings['form'] == '10-Q'].iloc[0]\n",
    "accessionNumber = file.get('accessionNumber')\n",
    "folder_url = file.get('folder_url')\n",
    "file_url = file.get('file_url')\n",
    "soup = ticker.get_file_data(file_url=file_url)\n",
    "\n",
    "\n",
    "ticker.scrape_logger.info(\n",
    "    file.get('filingDate').strftime('%Y-%m-%d') + ': ' + folder_url)\n",
    "\n",
    "start_date = dt.datetime(2022, 1, 1) # after XBRL implementation\n",
    "\n",
    "query = {\n",
    "    'cik': ticker.cik,\n",
    "    'form': {'$in': ['10-K']},\n",
    "    'filingDate': {'$gte': start_date},\n",
    "}\n",
    "\n",
    "filings_to_scrape = [i for i in mongo.tickerfilings.find(query).sort('filingDate', 1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Get Elements vs search tags for getting labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 00:06:29,675 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001318605/000119312513435480/index.json\n"
     ]
    }
   ],
   "source": [
    "index_df = ticker.get_filing_folder_index(folder_url=folder_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 00:06:31,818 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001318605/000119312513435480/tsla-20130930_lab.xml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1776 entries, 0 to 1775\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   xlink:type     1776 non-null   object\n",
      " 1   xlink:href     419 non-null    object\n",
      " 2   roleURI        1 non-null      object\n",
      " 3   labelText      1776 non-null   object\n",
      " 4   xlink:role     939 non-null    object\n",
      " 5   xlink:label    1356 non-null   object\n",
      " 6   xml:lang       938 non-null    object\n",
      " 7   id             938 non-null    object\n",
      " 8   xlink:arcrole  418 non-null    object\n",
      " 9   xlink:from     418 non-null    object\n",
      " 10  xlink:to       418 non-null    object\n",
      "dtypes: object(11)\n",
      "memory usage: 152.8+ KB\n"
     ]
    }
   ],
   "source": [
    "ticker.get_elements(folder_url=folder_url, \n",
    "                    index_df=index_df,\n",
    "                    scrape_file_extension='_lab').info()\n",
    "                        # .query(\"`xlink:type` == 'resource'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1774 entries, 0 to 1773\n",
      "Data columns (total 10 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   xlink:type     1774 non-null   object\n",
      " 1   xlink:href     418 non-null    object\n",
      " 2   xlink:label    1356 non-null   object\n",
      " 3   labelText      1774 non-null   object\n",
      " 4   xlink:role     938 non-null    object\n",
      " 5   xml:lang       938 non-null    object\n",
      " 6   id             938 non-null    object\n",
      " 7   xlink:arcrole  418 non-null    object\n",
      " 8   xlink:from     418 non-null    object\n",
      " 9   xlink:to       418 non-null    object\n",
      "dtypes: object(10)\n",
      "memory usage: 138.7+ KB\n"
     ]
    }
   ],
   "source": [
    "labels_list = []\n",
    "for i in ticker.search_tags(soup=soup, pattern='^label.*'):\n",
    "    for x in i.find_all():\n",
    "        label_dict = dict(**x.attrs, labelText=x.text)\n",
    "        labels_list.append(label_dict)\n",
    "\n",
    "pd.DataFrame(labels_list).info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test new context dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list = []\n",
    "contexts_soup = ticker.search_context(soup=soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contextId</th>\n",
       "      <th>entity</th>\n",
       "      <th>segment</th>\n",
       "      <th>startDate</th>\n",
       "      <th>endDate</th>\n",
       "      <th>instant</th>\n",
       "      <th>accessionNumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eol_PE665410--1310-Q0007_STD_92_20130930_0_928...</td>\n",
       "      <td>0001318605</td>\n",
       "      <td>{'us-gaap:AntidilutiveSecuritiesExcludedFromCo...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0001193125-13-435480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eol_PE665410--1310-Q0007_STD_92_20130930_0_928...</td>\n",
       "      <td>0001318605</td>\n",
       "      <td>{'us-gaap:AntidilutiveSecuritiesExcludedFromCo...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0001193125-13-435480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eol_PE665410--1310-Q0007_STD_92_20130930_0_928...</td>\n",
       "      <td>0001318605</td>\n",
       "      <td>{'us-gaap:AntidilutiveSecuritiesExcludedFromCo...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0001193125-13-435480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eol_PE665410--1310-Q0007_STD_92_20130930_0_928...</td>\n",
       "      <td>0001318605</td>\n",
       "      <td>{'us-gaap:AntidilutiveSecuritiesExcludedFromCo...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0001193125-13-435480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eol_PE665410--1310-Q0007_STD_92_20130930_0_928...</td>\n",
       "      <td>0001318605</td>\n",
       "      <td>{'us-gaap:TypeOfArrangementAxis': 'tsla:Daimle...</td>\n",
       "      <td>2013-07-01</td>\n",
       "      <td>2013-09-30</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0001193125-13-435480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>eol_PE665410--1310-Q0007_STD_0_20120630_0</td>\n",
       "      <td>0001318605</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>0001193125-13-435480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>eol_PE665410--1310-Q0007_STD_0_20100531_0</td>\n",
       "      <td>0001318605</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2010-05-31</td>\n",
       "      <td>0001193125-13-435480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>eol_PE665410--1310-Q0007_STD_0_20131031_0</td>\n",
       "      <td>0001318605</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2013-10-31</td>\n",
       "      <td>0001193125-13-435480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>eol_PE665410--1310-Q0007_STD_0_20121031_0_9251...</td>\n",
       "      <td>0001318605</td>\n",
       "      <td>{'us-gaap:TitleOfIndividualAxis': 'us-gaap:Chi...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2012-10-31</td>\n",
       "      <td>0001193125-13-435480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>eol_PE665410--1310-Q0007_STD_0_20121031_0</td>\n",
       "      <td>0001318605</td>\n",
       "      <td>None</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2012-10-31</td>\n",
       "      <td>0001193125-13-435480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             contextId      entity  \\\n",
       "0    eol_PE665410--1310-Q0007_STD_92_20130930_0_928...  0001318605   \n",
       "1    eol_PE665410--1310-Q0007_STD_92_20130930_0_928...  0001318605   \n",
       "2    eol_PE665410--1310-Q0007_STD_92_20130930_0_928...  0001318605   \n",
       "3    eol_PE665410--1310-Q0007_STD_92_20130930_0_928...  0001318605   \n",
       "4    eol_PE665410--1310-Q0007_STD_92_20130930_0_928...  0001318605   \n",
       "..                                                 ...         ...   \n",
       "148          eol_PE665410--1310-Q0007_STD_0_20120630_0  0001318605   \n",
       "149          eol_PE665410--1310-Q0007_STD_0_20100531_0  0001318605   \n",
       "150          eol_PE665410--1310-Q0007_STD_0_20131031_0  0001318605   \n",
       "151  eol_PE665410--1310-Q0007_STD_0_20121031_0_9251...  0001318605   \n",
       "152          eol_PE665410--1310-Q0007_STD_0_20121031_0  0001318605   \n",
       "\n",
       "                                               segment  startDate    endDate  \\\n",
       "0    {'us-gaap:AntidilutiveSecuritiesExcludedFromCo... 2013-07-01 2013-09-30   \n",
       "1    {'us-gaap:AntidilutiveSecuritiesExcludedFromCo... 2013-07-01 2013-09-30   \n",
       "2    {'us-gaap:AntidilutiveSecuritiesExcludedFromCo... 2013-07-01 2013-09-30   \n",
       "3    {'us-gaap:AntidilutiveSecuritiesExcludedFromCo... 2013-07-01 2013-09-30   \n",
       "4    {'us-gaap:TypeOfArrangementAxis': 'tsla:Daimle... 2013-07-01 2013-09-30   \n",
       "..                                                 ...        ...        ...   \n",
       "148                                               None        NaT        NaT   \n",
       "149                                               None        NaT        NaT   \n",
       "150                                               None        NaT        NaT   \n",
       "151  {'us-gaap:TitleOfIndividualAxis': 'us-gaap:Chi...        NaT        NaT   \n",
       "152                                               None        NaT        NaT   \n",
       "\n",
       "       instant       accessionNumber  \n",
       "0          NaT  0001193125-13-435480  \n",
       "1          NaT  0001193125-13-435480  \n",
       "2          NaT  0001193125-13-435480  \n",
       "3          NaT  0001193125-13-435480  \n",
       "4          NaT  0001193125-13-435480  \n",
       "..         ...                   ...  \n",
       "148 2012-06-30  0001193125-13-435480  \n",
       "149 2010-05-31  0001193125-13-435480  \n",
       "150 2013-10-31  0001193125-13-435480  \n",
       "151 2012-10-31  0001193125-13-435480  \n",
       "152 2012-10-31  0001193125-13-435480  \n",
       "\n",
       "[153 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: get explicit member from segment to categorize the segment by the axis it belongs to\n",
    "        # the axis is standardized by US GAAP standards\n",
    "        # e.g. ProductorService Axis = srt:ProductOrServiceAxis\n",
    "        # e.g. StatementGeographicalAxis Axis = srt:StatementGeographicalAxis\n",
    "        # use scraped labels to get the axis name (terselabel) like 'Product and Service' or 'Geographical'\n",
    "        # use the axis name to categorize the segment\n",
    "\n",
    "all_context = pd.DataFrame()\n",
    "\n",
    "for tag in contexts_soup:\n",
    "    context_list.append(Context(context_tag=tag).to_dict())\n",
    "context_df = pd.DataFrame(context_list)\n",
    "context_df['accessionNumber'] = accessionNumber\n",
    "all_context = pd.concat(\n",
    "    [all_context, context_df], ignore_index=True)\n",
    "all_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test new facts dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>factName</th>\n",
       "      <th>factId</th>\n",
       "      <th>contextRef</th>\n",
       "      <th>unitRef</th>\n",
       "      <th>decimals</th>\n",
       "      <th>factValue</th>\n",
       "      <th>accessionNumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>us-gaap:commonstocksharesissued</td>\n",
       "      <td>id_3062907_303E2F50-B45F-4B70-B597-B91B864719B...</td>\n",
       "      <td>eol_PE665410--1310-Q0007_STD_0_20121031_0</td>\n",
       "      <td>shares</td>\n",
       "      <td>INF</td>\n",
       "      <td>7964601</td>\n",
       "      <td>0001193125-13-435480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>us-gaap:commonstocksharesissued</td>\n",
       "      <td>id_3062907_303E2F50-B45F-4B70-B597-B91B864719B...</td>\n",
       "      <td>eol_PE665410--1310-Q0007_STD_0_20121031_0_9251...</td>\n",
       "      <td>shares</td>\n",
       "      <td>INF</td>\n",
       "      <td>35398</td>\n",
       "      <td>0001193125-13-435480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>us-gaap:accrualforenvironmentallosscontingencies</td>\n",
       "      <td>id_3062907_774CF1BF-D65E-4EEB-884A-9AF000865E3...</td>\n",
       "      <td>eol_PE665410--1310-Q0007_STD_0_20100531_0</td>\n",
       "      <td>iso4217_USD</td>\n",
       "      <td>-5</td>\n",
       "      <td>5300000</td>\n",
       "      <td>0001193125-13-435480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>us-gaap:standardproductwarrantyaccrual</td>\n",
       "      <td>id_3062907_8FEDD537-22DD-42E4-9E8A-AD7F8552A0F...</td>\n",
       "      <td>eol_PE665410--1310-Q0007_STD_0_20120630_0</td>\n",
       "      <td>iso4217_USD</td>\n",
       "      <td>-3</td>\n",
       "      <td>5723000</td>\n",
       "      <td>0001193125-13-435480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>us-gaap:otherliabilitiesfairvaluedisclosure</td>\n",
       "      <td>id_3062907_96E9CB2B-1275-4B2D-826A-F9DF4C0B5D0...</td>\n",
       "      <td>eol_PE665410--1310-Q0007_STD_0_20120630_0</td>\n",
       "      <td>iso4217_USD</td>\n",
       "      <td>-3</td>\n",
       "      <td>8529000</td>\n",
       "      <td>0001193125-13-435480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>us-gaap:salesrevenueservicesnet</td>\n",
       "      <td>id_3062907_154F9647-2540-4CC3-8926-C1E0F30D320...</td>\n",
       "      <td>eol_PE665410--1310-Q0007_STD_92_20130930_0_928...</td>\n",
       "      <td>iso4217_USD</td>\n",
       "      <td>-5</td>\n",
       "      <td>1200000</td>\n",
       "      <td>0001193125-13-435480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>us-gaap:antidilutivesecuritiesexcludedfromcomp...</td>\n",
       "      <td>id_3062907_7116ABEA-A50B-4974-AA1F-EB39A58827A...</td>\n",
       "      <td>eol_PE665410--1310-Q0007_STD_92_20130930_0_928...</td>\n",
       "      <td>shares</td>\n",
       "      <td>INF</td>\n",
       "      <td>8870</td>\n",
       "      <td>0001193125-13-435480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>us-gaap:antidilutivesecuritiesexcludedfromcomp...</td>\n",
       "      <td>id_3062907_7116ABEA-A50B-4974-AA1F-EB39A58827A...</td>\n",
       "      <td>eol_PE665410--1310-Q0007_STD_92_20130930_0_928...</td>\n",
       "      <td>shares</td>\n",
       "      <td>INF</td>\n",
       "      <td>852987</td>\n",
       "      <td>0001193125-13-435480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>us-gaap:antidilutivesecuritiesexcludedfromcomp...</td>\n",
       "      <td>id_3062907_7116ABEA-A50B-4974-AA1F-EB39A58827A...</td>\n",
       "      <td>eol_PE665410--1310-Q0007_STD_92_20130930_0_928...</td>\n",
       "      <td>shares</td>\n",
       "      <td>INF</td>\n",
       "      <td>16401</td>\n",
       "      <td>0001193125-13-435480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>us-gaap:antidilutivesecuritiesexcludedfromcomp...</td>\n",
       "      <td>id_3062907_7116ABEA-A50B-4974-AA1F-EB39A58827A...</td>\n",
       "      <td>eol_PE665410--1310-Q0007_STD_92_20130930_0_928...</td>\n",
       "      <td>shares</td>\n",
       "      <td>INF</td>\n",
       "      <td>14390740</td>\n",
       "      <td>0001193125-13-435480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>501 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              factName  \\\n",
       "0                      us-gaap:commonstocksharesissued   \n",
       "1                      us-gaap:commonstocksharesissued   \n",
       "2     us-gaap:accrualforenvironmentallosscontingencies   \n",
       "3               us-gaap:standardproductwarrantyaccrual   \n",
       "4          us-gaap:otherliabilitiesfairvaluedisclosure   \n",
       "..                                                 ...   \n",
       "496                    us-gaap:salesrevenueservicesnet   \n",
       "497  us-gaap:antidilutivesecuritiesexcludedfromcomp...   \n",
       "498  us-gaap:antidilutivesecuritiesexcludedfromcomp...   \n",
       "499  us-gaap:antidilutivesecuritiesexcludedfromcomp...   \n",
       "500  us-gaap:antidilutivesecuritiesexcludedfromcomp...   \n",
       "\n",
       "                                                factId  \\\n",
       "0    id_3062907_303E2F50-B45F-4B70-B597-B91B864719B...   \n",
       "1    id_3062907_303E2F50-B45F-4B70-B597-B91B864719B...   \n",
       "2    id_3062907_774CF1BF-D65E-4EEB-884A-9AF000865E3...   \n",
       "3    id_3062907_8FEDD537-22DD-42E4-9E8A-AD7F8552A0F...   \n",
       "4    id_3062907_96E9CB2B-1275-4B2D-826A-F9DF4C0B5D0...   \n",
       "..                                                 ...   \n",
       "496  id_3062907_154F9647-2540-4CC3-8926-C1E0F30D320...   \n",
       "497  id_3062907_7116ABEA-A50B-4974-AA1F-EB39A58827A...   \n",
       "498  id_3062907_7116ABEA-A50B-4974-AA1F-EB39A58827A...   \n",
       "499  id_3062907_7116ABEA-A50B-4974-AA1F-EB39A58827A...   \n",
       "500  id_3062907_7116ABEA-A50B-4974-AA1F-EB39A58827A...   \n",
       "\n",
       "                                            contextRef      unitRef decimals  \\\n",
       "0            eol_PE665410--1310-Q0007_STD_0_20121031_0       shares      INF   \n",
       "1    eol_PE665410--1310-Q0007_STD_0_20121031_0_9251...       shares      INF   \n",
       "2            eol_PE665410--1310-Q0007_STD_0_20100531_0  iso4217_USD       -5   \n",
       "3            eol_PE665410--1310-Q0007_STD_0_20120630_0  iso4217_USD       -3   \n",
       "4            eol_PE665410--1310-Q0007_STD_0_20120630_0  iso4217_USD       -3   \n",
       "..                                                 ...          ...      ...   \n",
       "496  eol_PE665410--1310-Q0007_STD_92_20130930_0_928...  iso4217_USD       -5   \n",
       "497  eol_PE665410--1310-Q0007_STD_92_20130930_0_928...       shares      INF   \n",
       "498  eol_PE665410--1310-Q0007_STD_92_20130930_0_928...       shares      INF   \n",
       "499  eol_PE665410--1310-Q0007_STD_92_20130930_0_928...       shares      INF   \n",
       "500  eol_PE665410--1310-Q0007_STD_92_20130930_0_928...       shares      INF   \n",
       "\n",
       "    factValue       accessionNumber  \n",
       "0     7964601  0001193125-13-435480  \n",
       "1       35398  0001193125-13-435480  \n",
       "2     5300000  0001193125-13-435480  \n",
       "3     5723000  0001193125-13-435480  \n",
       "4     8529000  0001193125-13-435480  \n",
       "..        ...                   ...  \n",
       "496   1200000  0001193125-13-435480  \n",
       "497      8870  0001193125-13-435480  \n",
       "498    852987  0001193125-13-435480  \n",
       "499     16401  0001193125-13-435480  \n",
       "500  14390740  0001193125-13-435480  \n",
       "\n",
       "[501 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_facts = pd.DataFrame()\n",
    "facts_list = []\n",
    "facts = ticker.search_facts(soup=soup)\n",
    "for fact_tag in facts:\n",
    "    facts_list.append(Facts(fact_tag=fact_tag).to_dict())\n",
    "facts_df = pd.DataFrame(facts_list)\n",
    "facts_df['accessionNumber'] = accessionNumber\n",
    "all_facts = pd.concat([all_facts, facts_df], ignore_index=True)\n",
    "all_facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test get filing Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-13 19:39:44,120 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001318605/000162828023034847/index.json\n",
      "2024-01-13 19:39:44,126 - sec-scraper - ERROR - Failed to scrape metalinks for https://www.sec.gov/Archives/edgar/data/0001318605/000162828023034847...TickerData.get_metalinks() got an unexpected keyword argument 'folder_url'\n",
      "2024-01-13 19:39:44,673 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001318605/000162828023034847/tsla-20230930_lab.xml\n",
      "2024-01-13 19:39:45,023 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001318605/000162828023034847/tsla-20230930_cal.xml\n",
      "2024-01-13 19:39:45,457 - sec-scraper - INFO - Request successful at URL: https://www.sec.gov/Archives/edgar/data/0001318605/000162828023034847/tsla-20230930_def.xml\n",
      "2024-01-13 19:39:45,482 - sec-scraper - INFO - Merging facts with context and labels. Current facts length: 997...\n",
      "2024-01-13 19:39:45,489 - sec-scraper - INFO - Successfully merged facts with context and labels. Merged facts length: 997...\n",
      "2024-01-13 19:39:45,489 - sec-scraper - INFO - Successfully scraped TSLA(0001318605)-https://www.sec.gov/Archives/edgar/data/0001318605/000162828023034847...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_labels = pd.DataFrame()\n",
    "all_calc = pd.DataFrame()\n",
    "all_defn = pd.DataFrame()\n",
    "all_context = pd.DataFrame()\n",
    "all_facts = pd.DataFrame()\n",
    "all_metalinks = pd.DataFrame()\n",
    "all_merged_facts = pd.DataFrame()\n",
    "failed_folders = []\n",
    "\n",
    "try:  # Scrape facts\n",
    "    facts_list = []\n",
    "    facts = ticker.search_facts(soup=soup)\n",
    "    for fact_tag in facts:\n",
    "        facts_list.append(Facts(fact_tag=fact_tag).to_dict())\n",
    "    facts_df = pd.DataFrame(facts_list)\n",
    "    facts_df['accessionNumber'] = accessionNumber\n",
    "    all_facts = pd.concat([all_facts, facts_df], ignore_index=True)\n",
    "except Exception as e:\n",
    "    ticker.scrape_logger.error(\n",
    "        f'Failed to scrape facts for {folder_url}...{e}')\n",
    "    failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                            error=f'Failed to scrape facts for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "    pass\n",
    "\n",
    "try:  # Scrape context\n",
    "    context_list = []\n",
    "    contexts = ticker.search_context(soup=soup)\n",
    "    for tag in contexts:\n",
    "        context_list.append(Context(context_tag=tag).to_dict())\n",
    "    context_df = pd.DataFrame(context_list)\n",
    "    context_df['accessionNumber'] = accessionNumber\n",
    "    all_context = pd.concat(\n",
    "        [all_context, context_df], ignore_index=True)\n",
    "except Exception as e:\n",
    "    ticker.scrape_logger.error(\n",
    "        f'Failed to scrape context for {folder_url}...{e}')\n",
    "    failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                            error=f'Failed to scrape context for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "    pass\n",
    "\n",
    "index_df = ticker.get_filing_folder_index(folder_url=folder_url)\n",
    "\n",
    "try:  # Scrape metalinks\n",
    "    metalinks = ticker.get_metalinks(\n",
    "        folder_url=folder_url + '/MetaLinks.json')\n",
    "    metalinks['accessionNumber'] = accessionNumber\n",
    "    all_metalinks = pd.concat(\n",
    "        [all_metalinks, metalinks], ignore_index=True)\n",
    "except Exception as e:\n",
    "    ticker.scrape_logger.error(\n",
    "        f'Failed to scrape metalinks for {folder_url}...{e}')\n",
    "    failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                            error=f'Failed to scrape metalinks for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "    pass\n",
    "\n",
    "try:  # Scrape labels\n",
    "    labels = ticker.get_elements(folder_url=folder_url, index_df=index_df,\n",
    "                                    scrape_file_extension='_lab').query(\"`xlink:type` == 'resource'\")\n",
    "    labels['xlink:role'] = labels['xlink:role'].str.split(\n",
    "        '/').apply(lambda x: x[-1])\n",
    "    labels['xlink:label'] = labels['xlink:label'].str\\\n",
    "        .replace('(lab_)|(_en-US)', '', regex=True).str\\\n",
    "        .split('_')\\\n",
    "        .apply(lambda x: ':'.join(x[:2]))\\\n",
    "        .str.lower()\n",
    "    labels['accessionNumber'] = accessionNumber\n",
    "    all_labels = pd.concat([all_labels, labels], ignore_index=True)\n",
    "\n",
    "except Exception as e:\n",
    "    ticker.scrape_logger.error(\n",
    "        f'Failed to scrape labels for {folder_url}...{e}')\n",
    "    failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                            error=f'Failed to scrape labels for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "    pass\n",
    "\n",
    "try:  # Scrape calculations\n",
    "    calc = ticker.get_elements(folder_url=folder_url, index_df=index_df,\n",
    "                                scrape_file_extension='_cal').query(\"`xlink:type` == 'arc'\")\n",
    "    calc['accessionNumber'] = accessionNumber\n",
    "    all_calc = pd.concat([all_calc, calc], ignore_index=True)\n",
    "except Exception as e:\n",
    "    ticker.scrape_logger.error(\n",
    "        f'Failed to scrape calc for {folder_url}...{e}')\n",
    "    failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                            error=f'Failed to scrape calc for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "    pass\n",
    "\n",
    "try:  # Scrape definitions\n",
    "    defn = ticker.get_elements(folder_url=folder_url, index_df=index_df,\n",
    "                                scrape_file_extension='_def').query(\"`xlink:type` == 'arc'\")\n",
    "    defn['accessionNumber'] = accessionNumber\n",
    "    all_defn = pd.concat([all_defn, defn], ignore_index=True)\n",
    "except Exception as e:\n",
    "    ticker.scrape_logger.error(\n",
    "        f'Failed to scrape defn for {folder_url}...{e}')\n",
    "    failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                            error=f'Failed to scrape defn for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "    pass\n",
    "\n",
    "ticker.scrape_logger.info(\n",
    "    f'Merging facts with context and labels. Current facts length: {len(facts_list)}...')\n",
    "try:\n",
    "    merged_facts = facts_df.merge(context_df, how='left', left_on='contextRef', right_on='contextId')\\\n",
    "        .merge(labels.query(\"`xlink:role` == 'label'\"), how='left', left_on='factName', right_on='xlink:label')\n",
    "    merged_facts = merged_facts.drop(\n",
    "        ['accessionNumber_x', 'accessionNumber_y'], axis=1)\n",
    "    ticker.scrape_logger.info(\n",
    "        f'Successfully merged facts with context and labels. Merged facts length: {len(merged_facts)}...')\n",
    "except Exception as e:\n",
    "    ticker.scrape_logger.error(\n",
    "        f'Failed to merge facts with context and labels for {folder_url}...{e}')\n",
    "    failed_folders.append(dict(folder_url=folder_url, accessionNumber=accessionNumber,\n",
    "                            error=f'Failed to merge facts with context and labels for {folder_url}...{e}', filingDate=file.get('filingDate')))\n",
    "    pass\n",
    "\n",
    "all_merged_facts = pd.concat(\n",
    "    [all_merged_facts, merged_facts], ignore_index=True)\n",
    "ticker.scrape_logger.info(\n",
    "    f'Successfully scraped {ticker.ticker}({ticker.cik})-{folder_url}...\\n')\n",
    "\n",
    "all_merged_facts = all_merged_facts.loc[~all_merged_facts['labelText'].isnull(), [\n",
    "'labelText', 'segment', 'startDate', 'endDate', 'instant', 'factValue', 'unitRef']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all_labels, all_calc, all_defn to xlsx on different sheets\n",
    "with pd.ExcelWriter(f'././data/{ticker.ticker}_all_data.xlsx') as writer:\n",
    "    all_facts.to_excel(writer, sheet_name='facts', index=False)\n",
    "    all_context.to_excel(writer, sheet_name='context', index=False)\n",
    "    all_labels.to_excel(writer, sheet_name='labels', index=False)\n",
    "    all_merged_facts.to_excel(writer, sheet_name='merged_facts', index=False)\n",
    "    all_calc.to_excel(writer, sheet_name='calc', index=False)\n",
    "    all_defn.to_excel(writer, sheet_name='defn', index=False)\n",
    "    all_metalinks.to_excel(writer, sheet_name='metalinks', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df = clean_values_in_facts(all_merged_facts)\n",
    "# final_df = clean_values_in_segment(final_df, ticker.ticker)\n",
    "# final_df, start_end, instant = split_facts_into_start_instant(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbrl_us_gaap = 'http://xbrl.fasb.org/us-gaap/2024/elts/us-gaap-2024.xsd'\n",
    "xbrl_srt = 'http://xbrl.fasb.org/srt/2024/elts/srt-std-2024.xsd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in ticker.us_gaap_tags['id'].str.split('_') if len(i) > 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse using GPT (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = symbol.search_context(soup)[0]\n",
    "data = {\n",
    "    'id': context['id'],\n",
    "    'entity': {\n",
    "        'identifier': {\n",
    "            'scheme': context.find('identifier')['scheme'],\n",
    "            'value': context.find('identifier').text\n",
    "        }\n",
    "    },\n",
    "    'period': {\n",
    "        'startDate': context.find('startdate').text,\n",
    "        'endDate': context.find('enddate').text\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import XMLOutputParser\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessage,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import json\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "parser = XMLOutputParser(tags=['id', 'entity', 'period'])\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assistant that parses XML files for a company's financial statements from the SEC Edgar database.\"\n",
    "                \"The XML content will be provided by the user.\"\n",
    "                \"You will parse the output and return it in the json format.\"\n",
    "                \"{format_instructions}\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\"{xml}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "context_list = []\n",
    "total_cost = 0\n",
    "total_tokens = 0\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "with trange(len(contexts[:]), desc='Scraping contexts...') as t:\n",
    "    for i in t:\n",
    "        with get_openai_callback() as cb:\n",
    "            t.set_postfix(context=contexts[i].attrs.get('id'))\n",
    "            output = llm(template.format_messages(format_instructions=parser.get_format_instructions(), xml=contexts[i]))\n",
    "            total_cost += cb.total_cost\n",
    "            total_tokens += cb.total_tokens\n",
    "            context_list.append(json.loads(output.content))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance-dashboard-kernel",
   "language": "python",
   "name": "finance-dashboard-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
